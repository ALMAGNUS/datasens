{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSens E1_v2 ‚Äî 03_ingest_sources\n",
    "\n",
    "- Objectifs: Collecte r√©elle des **5 types de sources** avec stockage hybride (PostgreSQL + MinIO)\n",
    "- Pr√©requis: 01_setup_env + 02_schema_create ex√©cut√©s\n",
    "- Sortie: Donn√©es collect√©es + visualisations + tables r√©elles √† chaque √©tape\n",
    "- Guide: docs/GUIDE_TECHNIQUE_E1.md\n",
    "\n",
    "> **E1_v2** : Collecte r√©elle fonctionnelle (18 tables PostgreSQL)\n",
    "> - Source 1 : Kaggle Dataset (split 50/50 PostgreSQL/MinIO)\n",
    "> - Source 2 : API OpenWeatherMap (m√©t√©o 4 villes)\n",
    "> - Source 3 : Flux RSS Multi-Sources (Franceinfo + 20 Minutes + Le Monde)\n",
    "> - Source 4 : NewsAPI (optionnel si cl√© API disponible)\n",
    "> - Source 5 : GDELT Big Data (√©chantillon France)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notes:\n",
    "> - Lecture d‚Äôun flux RSS (Franceinfo) via `feedparser`.\n",
    "> - Construction d‚Äôun DataFrame normalis√©: `titre`, `texte`, `date_publication`, `langue`.\n",
    "> - Sauvegarde du brut en CSV (tra√ßabilit√©) et insertion en base.\n",
    "> - `get_source_id` assure l‚Äôexistence de la source; `flux` mat√©rialise la collecte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RSS: 20 articles ins√©r√©s\n"
     ]
    }
   ],
   "source": [
    "# DataSens E1_v2 - 03_ingest_sources\n",
    "# üì• Collecte r√©elle des 5 types de sources avec visualisations\n",
    "\n",
    "import datetime as dt\n",
    "import hashlib\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import feedparser\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "from minio import Minio\n",
    "from sqlalchemy import create_engine, text\n",
    "from tqdm import tqdm\n",
    "\n",
    "# R√©cup√©rer les variables du notebook 01\n",
    "if 'PROJECT_ROOT' not in globals():\n",
    "    current = Path.cwd()\n",
    "    PROJECT_ROOT = None\n",
    "    while current != current.parent:\n",
    "        if (current / \"notebooks\").exists() and (current / \"docs\").exists():\n",
    "            PROJECT_ROOT = current\n",
    "            break\n",
    "        current = current.parent\n",
    "    else:\n",
    "        PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "if 'RAW_DIR' not in globals():\n",
    "    RAW_DIR = PROJECT_ROOT / 'data' / 'raw'\n",
    "\n",
    "if 'PG_URL' not in globals():\n",
    "    PG_URL = os.getenv(\"DATASENS_PG_URL\", \"postgresql+psycopg2://postgres:postgres@localhost:5433/postgres\")\n",
    "\n",
    "if 'MINIO_ENDPOINT' not in globals():\n",
    "    MINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT\", \"http://localhost:9002\")\n",
    "    MINIO_ACCESS_KEY = os.getenv(\"MINIO_ACCESS_KEY\", \"admin\")\n",
    "    MINIO_SECRET_KEY = os.getenv(\"MINIO_SECRET_KEY\", \"admin123\")\n",
    "    MINIO_BUCKET = os.getenv(\"MINIO_BUCKET\", \"datasens-raw\")\n",
    "\n",
    "if 'ts' not in globals():\n",
    "    def ts() -> str:\n",
    "        return dt.datetime.now(tz=dt.UTC).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "\n",
    "if 'sha256_hash' not in globals():\n",
    "    def sha256_hash(s: str) -> str:\n",
    "        return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "# Connexions\n",
    "engine = create_engine(PG_URL, future=True)\n",
    "\n",
    "try:\n",
    "    minio_client = Minio(\n",
    "        MINIO_ENDPOINT.replace(\"http://\", \"\").replace(\"https://\", \"\"),\n",
    "        access_key=MINIO_ACCESS_KEY,\n",
    "        secret_key=MINIO_SECRET_KEY,\n",
    "        secure=False\n",
    "    )\n",
    "    if not minio_client.bucket_exists(MINIO_BUCKET):\n",
    "        minio_client.make_bucket(MINIO_BUCKET)\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è MinIO: {e}\")\n",
    "    minio_client = None\n",
    "\n",
    "print(\"‚úÖ Connexions pr√™tes (PostgreSQL + MinIO)\")\n",
    "print(\"=\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Utilitaires : Fonctions helpers pour la collecte\n",
    "\n",
    "Fonctions r√©utilisables pour :\n",
    "- **minio_upload()** : Upload fichier vers MinIO (DataLake)\n",
    "- **get_source_id()** : R√©cup√©rer ou cr√©er une source\n",
    "- **create_flux()** : Cr√©er un flux de collecte avec tra√ßabilit√©\n",
    "- **ensure_territoire()** : Cr√©er ou r√©cup√©rer un territoire\n",
    "- **insert_documents()** : Insertion batch avec gestion des doublons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è Fonctions utilitaires pour la collecte\n",
    "\n",
    "def minio_upload(local_path: Path, minio_path: str) -> str:\n",
    "    \"\"\"Upload un fichier vers MinIO et retourne l'URI\"\"\"\n",
    "    if minio_client is None:\n",
    "        return f\"local://{local_path}\"\n",
    "    try:\n",
    "        minio_client.fput_object(MINIO_BUCKET, minio_path, str(local_path))\n",
    "        return f\"s3://{MINIO_BUCKET}/{minio_path}\"\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Erreur MinIO upload: {e}\")\n",
    "        return f\"local://{local_path}\"\n",
    "\n",
    "def get_source_id(conn, nom: str) -> int:\n",
    "    \"\"\"R√©cup√®re l'ID d'une source ou la cr√©e si absente\"\"\"\n",
    "    result = conn.execute(text(\"SELECT id_source FROM source WHERE nom = :nom\"), {\"nom\": nom}).scalar()\n",
    "    if result:\n",
    "        return result\n",
    "    # Cr√©er la source (trouver type_donnee 'API' par d√©faut)\n",
    "    tid = conn.execute(text(\"SELECT id_type_donnee FROM type_donnee WHERE libelle = 'API' LIMIT 1\")).scalar()\n",
    "    if not tid:\n",
    "        tid = conn.execute(text(\"INSERT INTO type_donnee(libelle) VALUES ('API') RETURNING id_type_donnee\")).scalar()\n",
    "    return conn.execute(text(\"\"\"\n",
    "        INSERT INTO source(id_type_donnee, nom, url, fiabilite) \n",
    "        VALUES (:tid, :nom, '', 0.8) RETURNING id_source\n",
    "    \"\"\"), {\"tid\": tid, \"nom\": nom}).scalar()\n",
    "\n",
    "def create_flux(conn, source_nom: str, format_type: str = \"csv\", manifest_uri: str = None) -> int:\n",
    "    \"\"\"Cr√©e un flux de collecte et retourne son ID\"\"\"\n",
    "    sid = get_source_id(conn, source_nom)\n",
    "    return conn.execute(text(\"\"\"\n",
    "        INSERT INTO flux(id_source, format, manifest_uri, date_collecte)\n",
    "        VALUES (:sid, :format, :manifest, NOW()) RETURNING id_flux\n",
    "    \"\"\"), {\"sid\": sid, \"format\": format_type, \"manifest\": manifest_uri}).scalar()\n",
    "\n",
    "def ensure_territoire(conn, ville: str, code_insee: str = None, lat: float = None, lon: float = None) -> int:\n",
    "    \"\"\"Cr√©e ou r√©cup√®re un territoire\"\"\"\n",
    "    result = conn.execute(text(\"SELECT id_territoire FROM territoire WHERE ville = :ville\"), {\"ville\": ville}).scalar()\n",
    "    if result:\n",
    "        return result\n",
    "    return conn.execute(text(\"\"\"\n",
    "        INSERT INTO territoire(ville, code_insee, lat, lon) \n",
    "        VALUES (:ville, :code, :lat, :lon) RETURNING id_territoire\n",
    "    \"\"\"), {\"ville\": ville, \"code\": code_insee, \"lat\": lat, \"lon\": lon}).scalar()\n",
    "\n",
    "def insert_documents(conn, df: pd.DataFrame, flux_id: int):\n",
    "    \"\"\"Insertion batch de documents avec gestion des doublons\"\"\"\n",
    "    inserted = 0\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            conn.execute(text(\"\"\"\n",
    "                INSERT INTO document(id_flux, titre, texte, langue, date_publication, hash_fingerprint)\n",
    "                VALUES(:fid, :titre, :texte, :langue, :date, :hash)\n",
    "                ON CONFLICT (hash_fingerprint) DO NOTHING\n",
    "            \"\"\"), {\n",
    "                \"fid\": flux_id,\n",
    "                \"titre\": row.get(\"titre\", \"\"),\n",
    "                \"texte\": row.get(\"texte\", \"\"),\n",
    "                \"langue\": row.get(\"langue\", \"fr\"),\n",
    "                \"date\": row.get(\"date_publication\"),\n",
    "                \"hash\": row.get(\"hash_fingerprint\", \"\")\n",
    "            })\n",
    "            inserted += 1\n",
    "        except Exception as e:\n",
    "            pass  # Doublon ou erreur silencieuse\n",
    "    return inserted\n",
    "\n",
    "print(\"‚úÖ Fonctions utilitaires charg√©es\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì∞ Source 1 : Flux RSS Multi-Sources (Presse fran√ßaise)\n",
    "\n",
    "Collecte d'articles depuis 3 flux RSS fran√ßais :\n",
    "- **Franceinfo** : Service public, actualit√©s g√©n√©rales\n",
    "- **20 Minutes** : Presse gratuite, grand public  \n",
    "- **Le Monde** : Presse de r√©f√©rence\n",
    "\n",
    "**Process** : Parsing RSS ‚Üí DataFrame ‚Üí D√©duplication SHA256 ‚Üí PostgreSQL + MinIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì∞ Source 1 : Flux RSS Multi-Sources\n",
    "print(\"üì∞ SOURCE 1 : Flux RSS Multi-Sources\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "RSS_SOURCES = {\n",
    "    \"Franceinfo\": \"https://www.francetvinfo.fr/titres.rss\",\n",
    "    \"20 Minutes\": \"https://www.20minutes.fr/feeds/rss-une.xml\",\n",
    "    \"Le Monde\": \"https://www.lemonde.fr/rss/une.xml\"\n",
    "}\n",
    "\n",
    "all_rss_items = []\n",
    "\n",
    "for source_name, rss_url in RSS_SOURCES.items():\n",
    "    print(f\"\\nüì° Source : {source_name}\")\n",
    "    try:\n",
    "        feed = feedparser.parse(rss_url)\n",
    "        if len(feed.entries) == 0:\n",
    "            print(\"   ‚ö†Ô∏è Aucun article\")\n",
    "            continue\n",
    "        \n",
    "        source_items = []\n",
    "        for e in feed.entries[:30]:  # Max 30 par source\n",
    "            titre = e.get(\"title\", \"\").strip()\n",
    "            texte = (e.get(\"summary\", \"\") or e.get(\"description\", \"\") or \"\").strip()\n",
    "            if titre and texte:\n",
    "                source_items.append({\n",
    "                    \"titre\": titre,\n",
    "                    \"texte\": texte,\n",
    "                    \"date_publication\": pd.to_datetime(e.get(\"published\", \"\"), errors=\"coerce\"),\n",
    "                    \"langue\": \"fr\",\n",
    "                    \"source_media\": source_name,\n",
    "                    \"url\": e.get(\"link\", \"\")\n",
    "                })\n",
    "        all_rss_items.extend(source_items)\n",
    "        print(f\"   ‚úÖ {len(source_items)} articles collect√©s\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Erreur : {str(e)[:80]}\")\n",
    "    time.sleep(1)\n",
    "\n",
    "# Consolidation\n",
    "df_rss = pd.DataFrame(all_rss_items)\n",
    "if len(df_rss) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è Aucun article RSS collect√©\")\n",
    "else:\n",
    "    print(f\"\\nüìä Total brut : {len(df_rss)} articles\")\n",
    "    \n",
    "    # D√©duplication\n",
    "    df_rss[\"hash_fingerprint\"] = df_rss.apply(\n",
    "        lambda row: sha256_hash(row[\"titre\"] + \" \" + row[\"texte\"]), axis=1\n",
    "    )\n",
    "    nb_avant = len(df_rss)\n",
    "    df_rss = df_rss.drop_duplicates(subset=[\"hash_fingerprint\"])\n",
    "    nb_apres = len(df_rss)\n",
    "    print(f\"üßπ D√©duplication : {nb_avant} ‚Üí {nb_apres} articles uniques\")\n",
    "    \n",
    "    # Sauvegarde locale + MinIO\n",
    "    local = RAW_DIR / \"rss\" / f\"rss_multi_{ts()}.csv\"\n",
    "    local.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_rss.to_csv(local, index=False)\n",
    "    minio_uri = minio_upload(local, f\"rss/{local.name}\")\n",
    "    \n",
    "    # Insertion PostgreSQL\n",
    "    with engine.begin() as conn:\n",
    "        flux_id = create_flux(conn, \"Flux RSS Multi-Sources (Franceinfo + 20 Minutes + Le Monde)\", \"rss\", minio_uri)\n",
    "        inserted = insert_documents(conn, df_rss[[\"titre\", \"texte\", \"langue\", \"date_publication\", \"hash_fingerprint\"]], flux_id)\n",
    "    \n",
    "    print(f\"\\n‚úÖ RSS : {inserted} articles ins√©r√©s en base + MinIO\")\n",
    "    print(f\"‚òÅÔ∏è MinIO : {minio_uri}\")\n",
    "    \n",
    "    # üìä Visualisations\n",
    "    print(\"\\nüìä R√©partition par source m√©diatique :\")\n",
    "    lang_counts = df_rss['source_media'].value_counts()\n",
    "    display(pd.DataFrame({\"Source\": lang_counts.index, \"Nombre\": lang_counts.values}))\n",
    "    \n",
    "    if len(lang_counts) > 0:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        bars = plt.bar(lang_counts.index, lang_counts.values, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "        for bar, value in zip(bars, lang_counts.values):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                    str(value), ha='center', va='bottom', fontweight='bold')\n",
    "        plt.title(\"üìä R√©partition des articles RSS par source\", fontsize=12, fontweight='bold')\n",
    "        plt.ylabel(\"Nombre d'articles\", fontsize=11)\n",
    "        plt.xticks(rotation=15, ha='right')\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # üìã Table de donn√©es r√©elles\n",
    "    print(\"\\nüìã Table 'document' - Articles RSS ins√©r√©s (aper√ßu 10 premiers) :\")\n",
    "    df_docs = pd.read_sql_query(\"\"\"\n",
    "        SELECT d.id_doc, d.titre, d.langue, d.date_publication, s.nom AS source\n",
    "        FROM document d\n",
    "        JOIN flux f ON d.id_flux = f.id_flux\n",
    "        JOIN source s ON f.id_source = s.id_source\n",
    "        WHERE s.nom LIKE '%RSS%'\n",
    "        ORDER BY d.id_doc DESC\n",
    "        LIMIT 10\n",
    "    \"\"\", engine)\n",
    "    display(df_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üå¶Ô∏è Source 2 : API OpenWeatherMap (M√©t√©o en temps r√©el)\n",
    "\n",
    "Collecte de donn√©es m√©t√©o pour 4 villes fran√ßaises :\n",
    "- **Paris, Lyon, Marseille, Toulouse**\n",
    "\n",
    "**Donn√©es** : Temp√©rature, humidit√©, vent, pression, type m√©t√©o\n",
    "\n",
    "**Stockage** : PostgreSQL (table `meteo` + `territoire`) + MinIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üå¶Ô∏è Source 2 : API OpenWeatherMap\n",
    "print(\"\\nüå¶Ô∏è SOURCE 2 : API OpenWeatherMap\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "OWM_CITIES = [\"Paris,FR\", \"Lyon,FR\", \"Marseille,FR\", \"Toulouse,FR\"]\n",
    "OWM_API_KEY = os.getenv(\"OWM_API_KEY\")\n",
    "\n",
    "if not OWM_API_KEY:\n",
    "    print(\"‚ö†Ô∏è OWM_API_KEY manquante - Source 2 ignor√©e\")\n",
    "else:\n",
    "    rows = []\n",
    "    for city in tqdm(OWM_CITIES, desc=\"OWM\"):\n",
    "        try:\n",
    "            r = requests.get(\n",
    "                \"https://api.openweathermap.org/data/2.5/weather\",\n",
    "                params={\"q\": city, \"appid\": OWM_API_KEY, \"units\": \"metric\", \"lang\": \"fr\"},\n",
    "                timeout=10\n",
    "            )\n",
    "            if r.status_code == 200:\n",
    "                j = r.json()\n",
    "                rows.append({\n",
    "                    \"ville\": j[\"name\"],\n",
    "                    \"lat\": j[\"coord\"][\"lat\"],\n",
    "                    \"lon\": j[\"coord\"][\"lon\"],\n",
    "                    \"date_obs\": pd.to_datetime(j[\"dt\"], unit=\"s\"),\n",
    "                    \"temperature\": j[\"main\"][\"temp\"],\n",
    "                    \"humidite\": j[\"main\"][\"humidity\"],\n",
    "                    \"vent_kmh\": (j.get(\"wind\", {}).get(\"speed\") or 0) * 3.6,\n",
    "                    \"pression\": j.get(\"main\", {}).get(\"pressure\"),\n",
    "                    \"meteo_type\": j[\"weather\"][0][\"main\"] if j.get(\"weather\") else None\n",
    "                })\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Erreur {city}: {str(e)[:60]}\")\n",
    "    \n",
    "    if rows:\n",
    "        df_owm = pd.DataFrame(rows)\n",
    "        local = RAW_DIR / \"api\" / \"owm\" / f\"owm_{ts()}.csv\"\n",
    "        local.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df_owm.to_csv(local, index=False)\n",
    "        minio_uri = minio_upload(local, f\"api/owm/{local.name}\")\n",
    "        \n",
    "        # Insertion PostgreSQL\n",
    "        with engine.begin() as conn:\n",
    "            flux_id = create_flux(conn, \"OpenWeatherMap\", \"json\", minio_uri)\n",
    "            for _, r in df_owm.iterrows():\n",
    "                tid = ensure_territoire(conn, r[\"ville\"], lat=r[\"lat\"], lon=r[\"lon\"])\n",
    "                conn.execute(text(\"\"\"\n",
    "                    INSERT INTO meteo(id_territoire, date_obs, temperature, humidite, vent_kmh, pression, meteo_type)\n",
    "                    VALUES(:t, :d, :T, :H, :V, :P, :MT)\n",
    "                \"\"\"), {\n",
    "                    \"t\": tid, \"d\": r[\"date_obs\"], \"T\": r[\"temperature\"],\n",
    "                    \"H\": r[\"humidite\"], \"V\": r[\"vent_kmh\"], \"P\": r[\"pression\"], \"MT\": r[\"meteo_type\"]\n",
    "                })\n",
    "        \n",
    "        print(f\"\\n‚úÖ OWM : {len(df_owm)} relev√©s ins√©r√©s en base + MinIO\")\n",
    "        print(f\"‚òÅÔ∏è MinIO : {minio_uri}\")\n",
    "        \n",
    "        # üìä Visualisations\n",
    "        print(\"\\nüìä R√©partition des relev√©s par ville :\")\n",
    "        display(df_owm[[\"ville\", \"temperature\", \"humidite\", \"meteo_type\"]])\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        bars = plt.bar(df_owm[\"ville\"], df_owm[\"temperature\"], color='#FF6B6B')\n",
    "        for bar, value in zip(bars, df_owm[\"temperature\"]):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                    f\"{value:.1f}¬∞C\", ha='center', va='bottom', fontweight='bold')\n",
    "        plt.title(\"üå°Ô∏è Temp√©rature par ville\", fontsize=12, fontweight='bold')\n",
    "        plt.ylabel(\"Temp√©rature (¬∞C)\", fontsize=11)\n",
    "        plt.xticks(rotation=15)\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        bars = plt.bar(df_owm[\"ville\"], df_owm[\"humidite\"], color='#4ECDC4')\n",
    "        for bar, value in zip(bars, df_owm[\"humidite\"]):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                    f\"{value}%\", ha='center', va='bottom', fontweight='bold')\n",
    "        plt.title(\"üíß Humidit√© par ville\", fontsize=12, fontweight='bold')\n",
    "        plt.ylabel(\"Humidit√© (%)\", fontsize=11)\n",
    "        plt.xticks(rotation=15)\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # üìã Tables de donn√©es r√©elles\n",
    "        print(\"\\nüìã Table 'meteo' - Relev√©s ins√©r√©s :\")\n",
    "        df_meteo = pd.read_sql_query(\"\"\"\n",
    "            SELECT m.id_meteo, t.ville, m.date_obs, m.temperature, m.humidite, m.meteo_type\n",
    "            FROM meteo m\n",
    "            JOIN territoire t ON m.id_territoire = t.id_territoire\n",
    "            ORDER BY m.id_meteo DESC\n",
    "            LIMIT 10\n",
    "        \"\"\", engine)\n",
    "        display(df_meteo)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Aucun relev√© m√©t√©o collect√©\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì∞ Source 3 : NewsAPI (Actualit√©s - Optionnel)\n",
    "\n",
    "Collecte d'articles via l'API NewsAPI si la cl√© est configur√©e.\n",
    "\n",
    "**Quota gratuit** : 1000 requ√™tes/jour (peut √™tre √©puis√©)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì∞ Source 3 : NewsAPI (Optionnel)\n",
    "print(\"\\nüì∞ SOURCE 3 : NewsAPI (Optionnel)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "NEWSAPI_KEY = os.getenv(\"NEWSAPI_KEY\")\n",
    "\n",
    "if not NEWSAPI_KEY:\n",
    "    print(\"‚ö†Ô∏è NEWSAPI_KEY manquante - Source 3 ignor√©e\")\n",
    "else:\n",
    "    NEWS_CATEGORIES = [\"general\", \"technology\", \"health\", \"business\"]\n",
    "    all_articles = []\n",
    "    \n",
    "    for category in NEWS_CATEGORIES:\n",
    "        try:\n",
    "            r = requests.get(\n",
    "                \"https://newsapi.org/v2/top-headlines\",\n",
    "                params={\"apiKey\": NEWSAPI_KEY, \"country\": \"fr\", \"category\": category, \"pageSize\": 20},\n",
    "                timeout=10\n",
    "            )\n",
    "            if r.status_code == 200:\n",
    "                data = r.json()\n",
    "                articles = data.get(\"articles\", [])\n",
    "                for art in articles:\n",
    "                    all_articles.append({\n",
    "                        \"titre\": (art.get(\"title\") or \"\").strip(),\n",
    "                        \"texte\": (art.get(\"description\") or art.get(\"content\") or \"\").strip(),\n",
    "                        \"date_publication\": pd.to_datetime(art.get(\"publishedAt\"), errors=\"coerce\"),\n",
    "                        \"langue\": \"fr\",\n",
    "                        \"categorie\": category\n",
    "                    })\n",
    "            elif r.status_code in [426, 429]:\n",
    "                print(f\"   ‚ö†Ô∏è Quota √©puis√© pour {category}\")\n",
    "                break\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Erreur {category}: {str(e)[:60]}\")\n",
    "    \n",
    "    if all_articles:\n",
    "        df_news = pd.DataFrame(all_articles)\n",
    "        df_news = df_news[df_news[\"texte\"].str.len() > 20].copy()\n",
    "        df_news[\"hash_fingerprint\"] = df_news.apply(\n",
    "            lambda row: sha256_hash(row[\"titre\"] + \" \" + row[\"texte\"]), axis=1\n",
    "        )\n",
    "        df_news = df_news.drop_duplicates(subset=[\"hash_fingerprint\"])\n",
    "        \n",
    "        local = RAW_DIR / \"api\" / \"newsapi\" / f\"newsapi_{ts()}.csv\"\n",
    "        local.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df_news.to_csv(local, index=False)\n",
    "        minio_uri = minio_upload(local, f\"api/newsapi/{local.name}\")\n",
    "        \n",
    "        with engine.begin() as conn:\n",
    "            flux_id = create_flux(conn, \"NewsAPI\", \"json\", minio_uri)\n",
    "            inserted = insert_documents(conn, df_news[[\"titre\", \"texte\", \"langue\", \"date_publication\", \"hash_fingerprint\"]], flux_id)\n",
    "        \n",
    "        print(f\"\\n‚úÖ NewsAPI : {inserted} articles ins√©r√©s\")\n",
    "        \n",
    "        # üìä Visualisation\n",
    "        if len(df_news) > 0:\n",
    "            cat_counts = df_news['categorie'].value_counts()\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.pie(cat_counts.values, labels=cat_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "            plt.title(\"üìä R√©partition NewsAPI par cat√©gorie\", fontsize=12, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # üìã Table de donn√©es\n",
    "        print(\"\\nüìã Table 'document' - Articles NewsAPI (aper√ßu 5 premiers) :\")\n",
    "        df_newsapi = pd.read_sql_query(\"\"\"\n",
    "            SELECT d.id_doc, d.titre, d.date_publication\n",
    "            FROM document d\n",
    "            JOIN flux f ON d.id_flux = f.id_flux\n",
    "            JOIN source s ON f.id_source = s.id_source\n",
    "            WHERE s.nom = 'NewsAPI'\n",
    "            ORDER BY d.id_doc DESC\n",
    "            LIMIT 5\n",
    "        \"\"\", engine)\n",
    "        display(df_newsapi)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Aucun article NewsAPI r√©cup√©r√© (quota √©puis√© ou cl√© invalide)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Bilan de la collecte E1_v2\n",
    "\n",
    "R√©capitulatif de toutes les sources collect√©es avec statistiques globales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Bilan global de la collecte\n",
    "print(\"\\nüìä BILAN GLOBAL DE LA COLLECTE E1_v2\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Statistiques par source\n",
    "with engine.connect() as conn:\n",
    "    stats = pd.read_sql_query(\"\"\"\n",
    "        SELECT \n",
    "            s.nom AS source,\n",
    "            COUNT(DISTINCT f.id_flux) AS nb_flux,\n",
    "            COUNT(DISTINCT d.id_doc) AS nb_documents,\n",
    "            td.libelle AS type_donnee\n",
    "        FROM source s\n",
    "        LEFT JOIN flux f ON s.id_source = f.id_source\n",
    "        LEFT JOIN document d ON f.id_flux = d.id_flux\n",
    "        LEFT JOIN type_donnee td ON s.id_type_donnee = td.id_type_donnee\n",
    "        GROUP BY s.nom, td.libelle\n",
    "        ORDER BY nb_documents DESC\n",
    "    \"\"\", conn)\n",
    "\n",
    "print(\"\\nüìà Statistiques par source :\")\n",
    "display(stats)\n",
    "\n",
    "# Total documents\n",
    "total_docs = stats['nb_documents'].sum()\n",
    "print(f\"\\nüìä Total documents collect√©s : {total_docs}\")\n",
    "\n",
    "# Graphique global\n",
    "if len(stats) > 0:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(stats[\"source\"], stats[\"nb_documents\"], color=plt.cm.Set3(range(len(stats))))\n",
    "    for bar, value in zip(bars, stats[\"nb_documents\"]):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                str(int(value)), ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "    plt.title(\"üìä Nombre de documents collect√©s par source\", fontsize=14, fontweight='bold')\n",
    "    plt.ylabel(\"Nombre de documents\", fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Vue compl√®te : tous les documents avec contexte\n",
    "print(\"\\nüìã Vue compl√®te - Tous les documents avec contexte (50 premiers) :\")\n",
    "df_all_docs = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        d.id_doc,\n",
    "        d.titre,\n",
    "        LEFT(d.texte, 100) AS texte_apercu,\n",
    "        d.langue,\n",
    "        d.date_publication,\n",
    "        s.nom AS source,\n",
    "        f.date_collecte,\n",
    "        f.format\n",
    "    FROM document d\n",
    "    JOIN flux f ON d.id_flux = f.id_flux\n",
    "    JOIN source s ON f.id_source = s.id_source\n",
    "    ORDER BY d.id_doc DESC\n",
    "    LIMIT 50\n",
    "\"\"\", engine)\n",
    "display(df_all_docs)\n",
    "\n",
    "# Statistiques par type de donn√©e\n",
    "print(\"\\nüìä R√©partition par type de donn√©e :\")\n",
    "df_types = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        td.libelle AS type_donnee,\n",
    "        COUNT(DISTINCT s.id_source) AS nb_sources,\n",
    "        COUNT(DISTINCT d.id_doc) AS nb_documents\n",
    "    FROM type_donnee td\n",
    "    LEFT JOIN source s ON td.id_type_donnee = s.id_type_donnee\n",
    "    LEFT JOIN flux f ON s.id_source = f.id_source\n",
    "    LEFT JOIN document d ON f.id_flux = d.id_flux\n",
    "    GROUP BY td.libelle\n",
    "    ORDER BY nb_documents DESC\n",
    "\"\"\", engine)\n",
    "display(df_types)\n",
    "\n",
    "if len(df_types) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(df_types[\"type_donnee\"], df_types[\"nb_documents\"], color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "    for bar, value in zip(bars, df_types[\"nb_documents\"]):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                str(int(value)), ha='center', va='bottom', fontweight='bold')\n",
    "    plt.title(\"üìä R√©partition des documents par type de donn√©e\", fontsize=12, fontweight='bold')\n",
    "    plt.ylabel(\"Nombre de documents\", fontsize=11)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Collecte E1_v2 termin√©e : {total_docs} documents collect√©s et stock√©s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
