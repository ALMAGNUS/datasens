{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSens E1_v2 — 03_ingest_sources\n",
    "\n",
    "- Objectifs: ingestion réelle (ex: RSS), création `flux` + `document`\n",
    "- Prérequis: 02_schema_create\n",
    "- Sortie: lignes insérées + fichier brut (manifest local)\n",
    "- Guide: docs/GUIDE_TECHNIQUE_E1.md\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notes:\n",
    "> - Lecture d’un flux RSS (Franceinfo) via `feedparser`.\n",
    "> - Construction d’un DataFrame normalisé: `titre`, `texte`, `date_publication`, `langue`.\n",
    "> - Sauvegarde du brut en CSV (traçabilité) et insertion en base.\n",
    "> - `get_source_id` assure l’existence de la source; `flux` matérialise la collecte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataSens E1_v2 - 03_ingest_sources\n",
    "# Placeholders ingestion: Kaggle(50/50), OWM, RSS\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "RAW = ROOT / \"data\" / \"raw\"\n",
    "PG_URL = os.getenv(\"DATASENS_PG_URL\", \"postgresql+psycopg2://ds_user:ds_pass@localhost:5432/datasens\")\n",
    "engine = create_engine(PG_URL, future=True)\n",
    "\n",
    "# Helper: create flux\n",
    "with engine.begin() as conn:\n",
    "    def get_source_id(nom):\n",
    "        r = conn.execute(text(\"SELECT id_source FROM source WHERE nom=:n\"), {\"n\": nom}).scalar()\n",
    "        if not r:\n",
    "            tid = conn.execute(text(\"INSERT INTO type_donnee(libelle) VALUES ('API') RETURNING id_type_donnee\")).scalar()\n",
    "            r = conn.execute(text(\"INSERT INTO source(id_type_donnee,nom,url,fiabilite) VALUES (:t,:n,'',0.8) RETURNING id_source\"), {\"t\": tid, \"n\": nom}).scalar()\n",
    "        return r\n",
    "\n",
    "# RSS minimal demo\n",
    "RSS = {\"Franceinfo\": \"https://www.francetvinfo.fr/titres.rss\"}\n",
    "items = []\n",
    "for name, url in RSS.items():\n",
    "    try:\n",
    "        feed = feedparser.parse(url)\n",
    "        for e in feed.entries[:20]:\n",
    "            items.append({\n",
    "                \"titre\": (e.get(\"title\") or \"\").strip(),\n",
    "                \"texte\": (e.get(\"summary\") or e.get(\"description\") or \"\").strip(),\n",
    "                \"date_publication\": pd.to_datetime(e.get(\"published\", None), errors=\"coerce\"),\n",
    "                \"langue\": \"fr\"\n",
    "            })\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if items:\n",
    "    df = pd.DataFrame(items)\n",
    "    local = (RAW / \"rss\" / f\"rss_{int(time.time())}.csv\"); local.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(local, index=False)\n",
    "    with engine.begin() as conn:\n",
    "        sid = get_source_id(\"Flux RSS Multi-Sources (Franceinfo + 20 Minutes + Le Monde)\")\n",
    "        fid = conn.execute(text(\"INSERT INTO flux(id_source,format,manifest_uri) VALUES (:s,'rss',:m) RETURNING id_flux\"), {\"s\": sid, \"m\": str(local)}).scalar()\n",
    "        for _, r in df.iterrows():\n",
    "            conn.execute(text(\"\"\"\n",
    "            INSERT INTO document(id_flux,titre,texte,langue,date_publication,hash_fingerprint)\n",
    "            VALUES(:f,:t,:x,:l,:d,substr(encode(digest(coalesce(:t,'')||' '||coalesce(:x,''),'sha256'),'hex'),1,64))\n",
    "            ON CONFLICT DO NOTHING\n",
    "            \"\"\"), {\"f\": fid, \"t\": r[\"titre\"], \"x\": r[\"texte\"], \"l\": r[\"langue\"], \"d\": r[\"date_publication\"]})\n",
    "    print(f\"✅ RSS: {len(df)} articles insérés\")\n",
    "else:\n",
    "    print(\"⚠️ RSS: aucune donnée\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
