{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSens E1_v2 â€” 03_ingest_sources\n",
    "\n",
    "- Objectifs: Collecte rÃ©elle des **5 types de sources** avec stockage hybride (PostgreSQL + MinIO)\n",
    "- PrÃ©requis: 01_setup_env + 02_schema_create exÃ©cutÃ©s\n",
    "- Sortie: DonnÃ©es collectÃ©es + visualisations + tables rÃ©elles Ã  chaque Ã©tape\n",
    "- Guide: docs/GUIDE_TECHNIQUE_E1.md\n",
    "\n",
    "> **E1_v2** : Collecte rÃ©elle fonctionnelle (18 tables PostgreSQL)\n",
    "> - Source 1 : Kaggle Dataset (split 50/50 PostgreSQL/MinIO)\n",
    "> - Source 2 : API OpenWeatherMap (mÃ©tÃ©o 4 villes)\n",
    "> - Source 3 : Flux RSS Multi-Sources (Franceinfo + 20 Minutes + Le Monde)\n",
    "> - Source 4 : NewsAPI (optionnel si clÃ© API disponible)\n",
    "> - Source 5 : GDELT Big Data (Ã©chantillon France)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notes:\n",
    "> - Lecture dâ€™un flux RSS (Franceinfo) via `feedparser`.\n",
    "> - Construction dâ€™un DataFrame normalisÃ©: `titre`, `texte`, `date_publication`, `langue`.\n",
    "> - Sauvegarde du brut en CSV (traÃ§abilitÃ©) et insertion en base.\n",
    "> - `get_source_id` assure lâ€™existence de la source; `flux` matÃ©rialise la collecte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… RSS: 20 articles insÃ©rÃ©s\n"
     ]
    }
   ],
   "source": [
    "# DataSens E1_v2 - 03_ingest_sources\n",
    "# ðŸ“¥ Collecte rÃ©elle des 5 types de sources avec visualisations\n",
    "\n",
    "import datetime as dt\n",
    "import hashlib\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import feedparser\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "from minio import Minio\n",
    "from sqlalchemy import create_engine, text\n",
    "from tqdm import tqdm\n",
    "\n",
    "# RÃ©cupÃ©rer les variables du notebook 01\n",
    "if 'PROJECT_ROOT' not in globals():\n",
    "    current = Path.cwd()\n",
    "    PROJECT_ROOT = None\n",
    "    while current != current.parent:\n",
    "        if (current / \"notebooks\").exists() and (current / \"docs\").exists():\n",
    "            PROJECT_ROOT = current\n",
    "            break\n",
    "        current = current.parent\n",
    "    else:\n",
    "        PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "if 'RAW_DIR' not in globals():\n",
    "    RAW_DIR = PROJECT_ROOT / 'data' / 'raw'\n",
    "\n",
    "if 'PG_URL' not in globals():\n",
    "    PG_URL = os.getenv(\"DATASENS_PG_URL\", \"postgresql+psycopg2://postgres:postgres@localhost:5433/postgres\")\n",
    "\n",
    "if 'MINIO_ENDPOINT' not in globals():\n",
    "    MINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT\", \"http://localhost:9002\")\n",
    "    MINIO_ACCESS_KEY = os.getenv(\"MINIO_ACCESS_KEY\", \"admin\")\n",
    "    MINIO_SECRET_KEY = os.getenv(\"MINIO_SECRET_KEY\", \"admin123\")\n",
    "    MINIO_BUCKET = os.getenv(\"MINIO_BUCKET\", \"datasens-raw\")\n",
    "\n",
    "if 'ts' not in globals():\n",
    "    def ts() -> str:\n",
    "        return dt.datetime.now(tz=dt.UTC).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "\n",
    "if 'sha256_hash' not in globals():\n",
    "    def sha256_hash(s: str) -> str:\n",
    "        return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "# Connexions\n",
    "engine = create_engine(PG_URL, future=True)\n",
    "\n",
    "try:\n",
    "    minio_client = Minio(\n",
    "        MINIO_ENDPOINT.replace(\"http://\", \"\").replace(\"https://\", \"\"),\n",
    "        access_key=MINIO_ACCESS_KEY,\n",
    "        secret_key=MINIO_SECRET_KEY,\n",
    "        secure=False\n",
    "    )\n",
    "    if not minio_client.bucket_exists(MINIO_BUCKET):\n",
    "        minio_client.make_bucket(MINIO_BUCKET)\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ MinIO: {e}\")\n",
    "    minio_client = None\n",
    "\n",
    "print(\"âœ… Connexions prÃªtes (PostgreSQL + MinIO)\")\n",
    "print(\"=\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Utilitaires : Fonctions helpers pour la collecte\n",
    "\n",
    "Fonctions rÃ©utilisables pour :\n",
    "- **minio_upload()** : Upload fichier vers MinIO (DataLake)\n",
    "- **get_source_id()** : RÃ©cupÃ©rer ou crÃ©er une source\n",
    "- **create_flux()** : CrÃ©er un flux de collecte avec traÃ§abilitÃ©\n",
    "- **ensure_territoire()** : CrÃ©er ou rÃ©cupÃ©rer un territoire\n",
    "- **insert_documents()** : Insertion batch avec gestion des doublons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ› ï¸ Fonctions utilitaires pour la collecte\n",
    "\n",
    "def minio_upload(local_path: Path, minio_path: str) -> str:\n",
    "    \"\"\"Upload un fichier vers MinIO et retourne l'URI\"\"\"\n",
    "    if minio_client is None:\n",
    "        return f\"local://{local_path}\"\n",
    "    try:\n",
    "        minio_client.fput_object(MINIO_BUCKET, minio_path, str(local_path))\n",
    "        return f\"s3://{MINIO_BUCKET}/{minio_path}\"\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ Erreur MinIO upload: {e}\")\n",
    "        return f\"local://{local_path}\"\n",
    "\n",
    "def get_source_id(conn, nom: str) -> int:\n",
    "    \"\"\"RÃ©cupÃ¨re l'ID d'une source ou la crÃ©e si absente\"\"\"\n",
    "    result = conn.execute(text(\"SELECT id_source FROM source WHERE nom = :nom\"), {\"nom\": nom}).scalar()\n",
    "    if result:\n",
    "        return result\n",
    "    # CrÃ©er la source (trouver type_donnee 'API' par dÃ©faut)\n",
    "    tid = conn.execute(text(\"SELECT id_type_donnee FROM type_donnee WHERE libelle = 'API' LIMIT 1\")).scalar()\n",
    "    if not tid:\n",
    "        tid = conn.execute(text(\"INSERT INTO type_donnee(libelle) VALUES ('API') RETURNING id_type_donnee\")).scalar()\n",
    "    return conn.execute(text(\"\"\"\n",
    "        INSERT INTO source(id_type_donnee, nom, url, fiabilite) \n",
    "        VALUES (:tid, :nom, '', 0.8) RETURNING id_source\n",
    "    \"\"\"), {\"tid\": tid, \"nom\": nom}).scalar()\n",
    "\n",
    "def create_flux(conn, source_nom: str, format_type: str = \"csv\", manifest_uri: str = None) -> int:\n",
    "    \"\"\"CrÃ©e un flux de collecte et retourne son ID\"\"\"\n",
    "    sid = get_source_id(conn, source_nom)\n",
    "    return conn.execute(text(\"\"\"\n",
    "        INSERT INTO flux(id_source, format, manifest_uri, date_collecte)\n",
    "        VALUES (:sid, :format, :manifest, NOW()) RETURNING id_flux\n",
    "    \"\"\"), {\"sid\": sid, \"format\": format_type, \"manifest\": manifest_uri}).scalar()\n",
    "\n",
    "def ensure_territoire(conn, ville: str, code_insee: str = None, lat: float = None, lon: float = None) -> int:\n",
    "    \"\"\"CrÃ©e ou rÃ©cupÃ¨re un territoire\"\"\"\n",
    "    result = conn.execute(text(\"SELECT id_territoire FROM territoire WHERE ville = :ville\"), {\"ville\": ville}).scalar()\n",
    "    if result:\n",
    "        return result\n",
    "    return conn.execute(text(\"\"\"\n",
    "        INSERT INTO territoire(ville, code_insee, lat, lon) \n",
    "        VALUES (:ville, :code, :lat, :lon) RETURNING id_territoire\n",
    "    \"\"\"), {\"ville\": ville, \"code\": code_insee, \"lat\": lat, \"lon\": lon}).scalar()\n",
    "\n",
    "def insert_documents(conn, df: pd.DataFrame, flux_id: int):\n",
    "    \"\"\"Insertion batch de documents avec gestion des doublons\"\"\"\n",
    "    inserted = 0\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            conn.execute(text(\"\"\"\n",
    "                INSERT INTO document(id_flux, titre, texte, langue, date_publication, hash_fingerprint)\n",
    "                VALUES(:fid, :titre, :texte, :langue, :date, :hash)\n",
    "                ON CONFLICT (hash_fingerprint) DO NOTHING\n",
    "            \"\"\"), {\n",
    "                \"fid\": flux_id,\n",
    "                \"titre\": row.get(\"titre\", \"\"),\n",
    "                \"texte\": row.get(\"texte\", \"\"),\n",
    "                \"langue\": row.get(\"langue\", \"fr\"),\n",
    "                \"date\": row.get(\"date_publication\"),\n",
    "                \"hash\": row.get(\"hash_fingerprint\", \"\")\n",
    "            })\n",
    "            inserted += 1\n",
    "        except Exception as e:\n",
    "            pass  # Doublon ou erreur silencieuse\n",
    "    return inserted\n",
    "\n",
    "print(\"âœ… Fonctions utilitaires chargÃ©es\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“° Source 1 : Flux RSS Multi-Sources (Presse franÃ§aise)\n",
    "\n",
    "Collecte d'articles depuis 3 flux RSS franÃ§ais :\n",
    "- **Franceinfo** : Service public, actualitÃ©s gÃ©nÃ©rales\n",
    "- **20 Minutes** : Presse gratuite, grand public  \n",
    "- **Le Monde** : Presse de rÃ©fÃ©rence\n",
    "\n",
    "**Process** : Parsing RSS â†’ DataFrame â†’ DÃ©duplication SHA256 â†’ PostgreSQL + MinIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“° Source 1 : Flux RSS Multi-Sources\n",
    "print(\"ðŸ“° SOURCE 1 : Flux RSS Multi-Sources\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "RSS_SOURCES = {\n",
    "    \"Franceinfo\": \"https://www.francetvinfo.fr/titres.rss\",\n",
    "    \"20 Minutes\": \"https://www.20minutes.fr/feeds/rss-une.xml\",\n",
    "    \"Le Monde\": \"https://www.lemonde.fr/rss/une.xml\"\n",
    "}\n",
    "\n",
    "all_rss_items = []\n",
    "\n",
    "for source_name, rss_url in RSS_SOURCES.items():\n",
    "    print(f\"\\nðŸ“¡ Source : {source_name}\")\n",
    "    try:\n",
    "        feed = feedparser.parse(rss_url)\n",
    "        if len(feed.entries) == 0:\n",
    "            print(\"   âš ï¸ Aucun article\")\n",
    "            continue\n",
    "        \n",
    "        source_items = []\n",
    "        for e in feed.entries[:30]:  # Max 30 par source\n",
    "            titre = e.get(\"title\", \"\").strip()\n",
    "            texte = (e.get(\"summary\", \"\") or e.get(\"description\", \"\") or \"\").strip()\n",
    "            if titre and texte:\n",
    "                source_items.append({\n",
    "                    \"titre\": titre,\n",
    "                    \"texte\": texte,\n",
    "                    \"date_publication\": pd.to_datetime(e.get(\"published\", \"\"), errors=\"coerce\"),\n",
    "                    \"langue\": \"fr\",\n",
    "                    \"source_media\": source_name,\n",
    "                    \"url\": e.get(\"link\", \"\")\n",
    "                })\n",
    "        all_rss_items.extend(source_items)\n",
    "        print(f\"   âœ… {len(source_items)} articles collectÃ©s\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Erreur : {str(e)[:80]}\")\n",
    "    time.sleep(1)\n",
    "\n",
    "# Consolidation\n",
    "df_rss = pd.DataFrame(all_rss_items)\n",
    "if len(df_rss) == 0:\n",
    "    print(\"\\nâš ï¸ Aucun article RSS collectÃ©\")\n",
    "else:\n",
    "    print(f\"\\nðŸ“Š Total brut : {len(df_rss)} articles\")\n",
    "    \n",
    "    # DÃ©duplication\n",
    "    df_rss[\"hash_fingerprint\"] = df_rss.apply(\n",
    "        lambda row: sha256_hash(row[\"titre\"] + \" \" + row[\"texte\"]), axis=1\n",
    "    )\n",
    "    nb_avant = len(df_rss)\n",
    "    df_rss = df_rss.drop_duplicates(subset=[\"hash_fingerprint\"])\n",
    "    nb_apres = len(df_rss)\n",
    "    print(f\"ðŸ§¹ DÃ©duplication : {nb_avant} â†’ {nb_apres} articles uniques\")\n",
    "    \n",
    "    # Sauvegarde locale + MinIO\n",
    "    local = RAW_DIR / \"rss\" / f\"rss_multi_{ts()}.csv\"\n",
    "    local.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_rss.to_csv(local, index=False)\n",
    "    minio_uri = minio_upload(local, f\"rss/{local.name}\")\n",
    "    \n",
    "    # Insertion PostgreSQL\n",
    "    with engine.begin() as conn:\n",
    "        flux_id = create_flux(conn, \"Flux RSS Multi-Sources (Franceinfo + 20 Minutes + Le Monde)\", \"rss\", minio_uri)\n",
    "        inserted = insert_documents(conn, df_rss[[\"titre\", \"texte\", \"langue\", \"date_publication\", \"hash_fingerprint\"]], flux_id)\n",
    "    \n",
    "    print(f\"\\nâœ… RSS : {inserted} articles insÃ©rÃ©s en base + MinIO\")\n",
    "    print(f\"â˜ï¸ MinIO : {minio_uri}\")\n",
    "    \n",
    "    # ðŸ“Š Visualisations\n",
    "    print(\"\\nðŸ“Š RÃ©partition par source mÃ©diatique :\")\n",
    "    lang_counts = df_rss['source_media'].value_counts()\n",
    "    display(pd.DataFrame({\"Source\": lang_counts.index, \"Nombre\": lang_counts.values}))\n",
    "    \n",
    "    if len(lang_counts) > 0:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        bars = plt.bar(lang_counts.index, lang_counts.values, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "        for bar, value in zip(bars, lang_counts.values):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                    str(value), ha='center', va='bottom', fontweight='bold')\n",
    "        plt.title(\"ðŸ“Š RÃ©partition des articles RSS par source\", fontsize=12, fontweight='bold')\n",
    "        plt.ylabel(\"Nombre d'articles\", fontsize=11)\n",
    "        plt.xticks(rotation=15, ha='right')\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # ðŸ“‹ Table de donnÃ©es rÃ©elles\n",
    "    print(\"\\nðŸ“‹ Table 'document' - Articles RSS insÃ©rÃ©s (aperÃ§u 10 premiers) :\")\n",
    "    df_docs = pd.read_sql_query(\"\"\"\n",
    "        SELECT d.id_doc, d.titre, d.langue, d.date_publication, s.nom AS source\n",
    "        FROM document d\n",
    "        JOIN flux f ON d.id_flux = f.id_flux\n",
    "        JOIN source s ON f.id_source = s.id_source\n",
    "        WHERE s.nom LIKE '%RSS%'\n",
    "        ORDER BY d.id_doc DESC\n",
    "        LIMIT 10\n",
    "    \"\"\", engine)\n",
    "    display(df_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒ¦ï¸ Source 2 : API OpenWeatherMap (MÃ©tÃ©o en temps rÃ©el)\n",
    "\n",
    "Collecte de donnÃ©es mÃ©tÃ©o pour 4 villes franÃ§aises :\n",
    "- **Paris, Lyon, Marseille, Toulouse**\n",
    "\n",
    "**DonnÃ©es** : TempÃ©rature, humiditÃ©, vent, pression, type mÃ©tÃ©o\n",
    "\n",
    "**Stockage** : PostgreSQL (table `meteo` + `territoire`) + MinIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŒ¦ï¸ Source 2 : API OpenWeatherMap\n",
    "print(\"\\nðŸŒ¦ï¸ SOURCE 2 : API OpenWeatherMap\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "OWM_CITIES = [\"Paris,FR\", \"Lyon,FR\", \"Marseille,FR\", \"Toulouse,FR\"]\n",
    "OWM_API_KEY = os.getenv(\"OWM_API_KEY\")\n",
    "\n",
    "if not OWM_API_KEY:\n",
    "    print(\"âš ï¸ OWM_API_KEY manquante - Source 2 ignorÃ©e\")\n",
    "else:\n",
    "    rows = []\n",
    "    for city in tqdm(OWM_CITIES, desc=\"OWM\"):\n",
    "        try:\n",
    "            r = requests.get(\n",
    "                \"https://api.openweathermap.org/data/2.5/weather\",\n",
    "                params={\"q\": city, \"appid\": OWM_API_KEY, \"units\": \"metric\", \"lang\": \"fr\"},\n",
    "                timeout=10\n",
    "            )\n",
    "            if r.status_code == 200:\n",
    "                j = r.json()\n",
    "                rows.append({\n",
    "                    \"ville\": j[\"name\"],\n",
    "                    \"lat\": j[\"coord\"][\"lat\"],\n",
    "                    \"lon\": j[\"coord\"][\"lon\"],\n",
    "                    \"date_obs\": pd.to_datetime(j[\"dt\"], unit=\"s\"),\n",
    "                    \"temperature\": j[\"main\"][\"temp\"],\n",
    "                    \"humidite\": j[\"main\"][\"humidity\"],\n",
    "                    \"vent_kmh\": (j.get(\"wind\", {}).get(\"speed\") or 0) * 3.6,\n",
    "                    \"pression\": j.get(\"main\", {}).get(\"pressure\"),\n",
    "                    \"meteo_type\": j[\"weather\"][0][\"main\"] if j.get(\"weather\") else None\n",
    "                })\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ Erreur {city}: {str(e)[:60]}\")\n",
    "    \n",
    "    if rows:\n",
    "        df_owm = pd.DataFrame(rows)\n",
    "        local = RAW_DIR / \"api\" / \"owm\" / f\"owm_{ts()}.csv\"\n",
    "        local.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df_owm.to_csv(local, index=False)\n",
    "        minio_uri = minio_upload(local, f\"api/owm/{local.name}\")\n",
    "        \n",
    "        # Insertion PostgreSQL\n",
    "        with engine.begin() as conn:\n",
    "            flux_id = create_flux(conn, \"OpenWeatherMap\", \"json\", minio_uri)\n",
    "            for _, r in df_owm.iterrows():\n",
    "                tid = ensure_territoire(conn, r[\"ville\"], lat=r[\"lat\"], lon=r[\"lon\"])\n",
    "                conn.execute(text(\"\"\"\n",
    "                    INSERT INTO meteo(id_territoire, date_obs, temperature, humidite, vent_kmh, pression, meteo_type)\n",
    "                    VALUES(:t, :d, :T, :H, :V, :P, :MT)\n",
    "                \"\"\"), {\n",
    "                    \"t\": tid, \"d\": r[\"date_obs\"], \"T\": r[\"temperature\"],\n",
    "                    \"H\": r[\"humidite\"], \"V\": r[\"vent_kmh\"], \"P\": r[\"pression\"], \"MT\": r[\"meteo_type\"]\n",
    "                })\n",
    "        \n",
    "        print(f\"\\nâœ… OWM : {len(df_owm)} relevÃ©s insÃ©rÃ©s en base + MinIO\")\n",
    "        print(f\"â˜ï¸ MinIO : {minio_uri}\")\n",
    "        \n",
    "        # ðŸ“Š Visualisations\n",
    "        print(\"\\nðŸ“Š RÃ©partition des relevÃ©s par ville :\")\n",
    "        display(df_owm[[\"ville\", \"temperature\", \"humidite\", \"meteo_type\"]])\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        bars = plt.bar(df_owm[\"ville\"], df_owm[\"temperature\"], color='#FF6B6B')\n",
    "        for bar, value in zip(bars, df_owm[\"temperature\"]):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                    f\"{value:.1f}Â°C\", ha='center', va='bottom', fontweight='bold')\n",
    "        plt.title(\"ðŸŒ¡ï¸ TempÃ©rature par ville\", fontsize=12, fontweight='bold')\n",
    "        plt.ylabel(\"TempÃ©rature (Â°C)\", fontsize=11)\n",
    "        plt.xticks(rotation=15)\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        bars = plt.bar(df_owm[\"ville\"], df_owm[\"humidite\"], color='#4ECDC4')\n",
    "        for bar, value in zip(bars, df_owm[\"humidite\"]):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                    f\"{value}%\", ha='center', va='bottom', fontweight='bold')\n",
    "        plt.title(\"ðŸ’§ HumiditÃ© par ville\", fontsize=12, fontweight='bold')\n",
    "        plt.ylabel(\"HumiditÃ© (%)\", fontsize=11)\n",
    "        plt.xticks(rotation=15)\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # ðŸ“‹ Tables de donnÃ©es rÃ©elles\n",
    "        print(\"\\nðŸ“‹ Table 'meteo' - RelevÃ©s insÃ©rÃ©s :\")\n",
    "        df_meteo = pd.read_sql_query(\"\"\"\n",
    "            SELECT m.id_meteo, t.ville, m.date_obs, m.temperature, m.humidite, m.meteo_type\n",
    "            FROM meteo m\n",
    "            JOIN territoire t ON m.id_territoire = t.id_territoire\n",
    "            ORDER BY m.id_meteo DESC\n",
    "            LIMIT 10\n",
    "        \"\"\", engine)\n",
    "        display(df_meteo)\n",
    "    else:\n",
    "        print(\"âš ï¸ Aucun relevÃ© mÃ©tÃ©o collectÃ©\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“° Source 3 : NewsAPI (ActualitÃ©s - Optionnel)\n",
    "\n",
    "Collecte d'articles via l'API NewsAPI si la clÃ© est configurÃ©e.\n",
    "\n",
    "**Quota gratuit** : 1000 requÃªtes/jour (peut Ãªtre Ã©puisÃ©)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“° Source 3 : NewsAPI (Optionnel)\n",
    "print(\"\\nðŸ“° SOURCE 3 : NewsAPI (Optionnel)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "NEWSAPI_KEY = os.getenv(\"NEWSAPI_KEY\")\n",
    "\n",
    "if not NEWSAPI_KEY:\n",
    "    print(\"âš ï¸ NEWSAPI_KEY manquante - Source 3 ignorÃ©e\")\n",
    "else:\n",
    "    NEWS_CATEGORIES = [\"general\", \"technology\", \"health\", \"business\"]\n",
    "    all_articles = []\n",
    "    \n",
    "    for category in NEWS_CATEGORIES:\n",
    "        try:\n",
    "            r = requests.get(\n",
    "                \"https://newsapi.org/v2/top-headlines\",\n",
    "                params={\"apiKey\": NEWSAPI_KEY, \"country\": \"fr\", \"category\": category, \"pageSize\": 20},\n",
    "                timeout=10\n",
    "            )\n",
    "            if r.status_code == 200:\n",
    "                data = r.json()\n",
    "                articles = data.get(\"articles\", [])\n",
    "                for art in articles:\n",
    "                    all_articles.append({\n",
    "                        \"titre\": (art.get(\"title\") or \"\").strip(),\n",
    "                        \"texte\": (art.get(\"description\") or art.get(\"content\") or \"\").strip(),\n",
    "                        \"date_publication\": pd.to_datetime(art.get(\"publishedAt\"), errors=\"coerce\"),\n",
    "                        \"langue\": \"fr\",\n",
    "                        \"categorie\": category\n",
    "                    })\n",
    "            elif r.status_code in [426, 429]:\n",
    "                print(f\"   âš ï¸ Quota Ã©puisÃ© pour {category}\")\n",
    "                break\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ Erreur {category}: {str(e)[:60]}\")\n",
    "    \n",
    "    if all_articles:\n",
    "        df_news = pd.DataFrame(all_articles)\n",
    "        df_news = df_news[df_news[\"texte\"].str.len() > 20].copy()\n",
    "        df_news[\"hash_fingerprint\"] = df_news.apply(\n",
    "            lambda row: sha256_hash(row[\"titre\"] + \" \" + row[\"texte\"]), axis=1\n",
    "        )\n",
    "        df_news = df_news.drop_duplicates(subset=[\"hash_fingerprint\"])\n",
    "        \n",
    "        local = RAW_DIR / \"api\" / \"newsapi\" / f\"newsapi_{ts()}.csv\"\n",
    "        local.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df_news.to_csv(local, index=False)\n",
    "        minio_uri = minio_upload(local, f\"api/newsapi/{local.name}\")\n",
    "        \n",
    "        with engine.begin() as conn:\n",
    "            flux_id = create_flux(conn, \"NewsAPI\", \"json\", minio_uri)\n",
    "            inserted = insert_documents(conn, df_news[[\"titre\", \"texte\", \"langue\", \"date_publication\", \"hash_fingerprint\"]], flux_id)\n",
    "        \n",
    "        print(f\"\\nâœ… NewsAPI : {inserted} articles insÃ©rÃ©s\")\n",
    "        \n",
    "        # ðŸ“Š Visualisation\n",
    "        if len(df_news) > 0:\n",
    "            cat_counts = df_news['categorie'].value_counts()\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.pie(cat_counts.values, labels=cat_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "            plt.title(\"ðŸ“Š RÃ©partition NewsAPI par catÃ©gorie\", fontsize=12, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # ðŸ“‹ Table de donnÃ©es\n",
    "        print(\"\\nðŸ“‹ Table 'document' - Articles NewsAPI (aperÃ§u 5 premiers) :\")\n",
    "        df_newsapi = pd.read_sql_query(\"\"\"\n",
    "            SELECT d.id_doc, d.titre, d.date_publication\n",
    "            FROM document d\n",
    "            JOIN flux f ON d.id_flux = f.id_flux\n",
    "            JOIN source s ON f.id_source = s.id_source\n",
    "            WHERE s.nom = 'NewsAPI'\n",
    "            ORDER BY d.id_doc DESC\n",
    "            LIMIT 5\n",
    "        \"\"\", engine)\n",
    "        display(df_newsapi)\n",
    "    else:\n",
    "        print(\"âš ï¸ Aucun article NewsAPI rÃ©cupÃ©rÃ© (quota Ã©puisÃ© ou clÃ© invalide)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Bilan de la collecte E1_v2\n",
    "\n",
    "RÃ©capitulatif de toutes les sources collectÃ©es avec statistiques globales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Bilan global de la collecte\n",
    "print(\"\\nðŸ“Š BILAN GLOBAL DE LA COLLECTE E1_v2\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Statistiques par source\n",
    "with engine.connect() as conn:\n",
    "    stats = pd.read_sql_query(\"\"\"\n",
    "        SELECT \n",
    "            s.nom AS source,\n",
    "            COUNT(DISTINCT f.id_flux) AS nb_flux,\n",
    "            COUNT(DISTINCT d.id_doc) AS nb_documents,\n",
    "            td.libelle AS type_donnee\n",
    "        FROM source s\n",
    "        LEFT JOIN flux f ON s.id_source = f.id_source\n",
    "        LEFT JOIN document d ON f.id_flux = d.id_flux\n",
    "        LEFT JOIN type_donnee td ON s.id_type_donnee = td.id_type_donnee\n",
    "        GROUP BY s.nom, td.libelle\n",
    "        ORDER BY nb_documents DESC\n",
    "    \"\"\", conn)\n",
    "\n",
    "print(\"\\nðŸ“ˆ Statistiques par source :\")\n",
    "display(stats)\n",
    "\n",
    "# Total documents\n",
    "total_docs = stats['nb_documents'].sum()\n",
    "print(f\"\\nðŸ“Š Total documents collectÃ©s : {total_docs}\")\n",
    "\n",
    "# Graphique global\n",
    "if len(stats) > 0:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(stats[\"source\"], stats[\"nb_documents\"], color=plt.cm.Set3(range(len(stats))))\n",
    "    for bar, value in zip(bars, stats[\"nb_documents\"]):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                str(int(value)), ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "    plt.title(\"ðŸ“Š Nombre de documents collectÃ©s par source\", fontsize=14, fontweight='bold')\n",
    "    plt.ylabel(\"Nombre de documents\", fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Vue complÃ¨te : tous les documents avec contexte\n",
    "print(\"\\nðŸ“‹ Vue complÃ¨te - Tous les documents avec contexte (50 premiers) :\")\n",
    "df_all_docs = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        d.id_doc,\n",
    "        d.titre,\n",
    "        LEFT(d.texte, 100) AS texte_apercu,\n",
    "        d.langue,\n",
    "        d.date_publication,\n",
    "        s.nom AS source,\n",
    "        f.date_collecte,\n",
    "        f.format\n",
    "    FROM document d\n",
    "    JOIN flux f ON d.id_flux = f.id_flux\n",
    "    JOIN source s ON f.id_source = s.id_source\n",
    "    ORDER BY d.id_doc DESC\n",
    "    LIMIT 50\n",
    "\"\"\", engine)\n",
    "display(df_all_docs)\n",
    "\n",
    "# Statistiques par type de donnÃ©e\n",
    "print(\"\\nðŸ“Š RÃ©partition par type de donnÃ©e :\")\n",
    "df_types = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        td.libelle AS type_donnee,\n",
    "        COUNT(DISTINCT s.id_source) AS nb_sources,\n",
    "        COUNT(DISTINCT d.id_doc) AS nb_documents\n",
    "    FROM type_donnee td\n",
    "    LEFT JOIN source s ON td.id_type_donnee = s.id_type_donnee\n",
    "    LEFT JOIN flux f ON s.id_source = f.id_source\n",
    "    LEFT JOIN document d ON f.id_flux = d.id_flux\n",
    "    GROUP BY td.libelle\n",
    "    ORDER BY nb_documents DESC\n",
    "\"\"\", engine)\n",
    "display(df_types)\n",
    "\n",
    "if len(df_types) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(df_types[\"type_donnee\"], df_types[\"nb_documents\"], color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "    for bar, value in zip(bars, df_types[\"nb_documents\"]):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                str(int(value)), ha='center', va='bottom', fontweight='bold')\n",
    "    plt.title(\"ðŸ“Š RÃ©partition des documents par type de donnÃ©e\", fontsize=12, fontweight='bold')\n",
    "    plt.ylabel(\"Nombre de documents\", fontsize=11)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\nâœ… Collecte E1_v2 terminÃ©e : {total_docs} documents collectÃ©s et stockÃ©s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
