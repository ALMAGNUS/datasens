{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataSens logging setup (marker:datasens_logging)\n",
    "import logging, os\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler('logs/datasens.log', encoding='utf-8')\n",
    "    ]\n",
    ")\n",
    "logging.info('D√©marrage')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≠ Chapitre 2 : OpenWeatherMap\n",
    "\n",
    "**Contexte narratif** : Collecte de donn√©es depuis OpenWeatherMap\n",
    "\n",
    "**Avant cette collecte** :\n",
    "- Sources pr√©c√©dentes : 1 source(s) d√©j√† collect√©e(s)\n",
    "- Documents en base : [V√©rification en cours...]\n",
    "\n",
    "**Objectif de cette √©tape** :\n",
    "- Collecter de nouvelles donn√©es depuis OpenWeatherMap\n",
    "- Enrichir notre dataset avec cette source\n",
    "- Progression du pipeline vers le dataset final\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üé≠ STORYTELLING : PR√âPARATION COLLECTE 2 - OpenWeatherMap\n",
    "# ============================================================\n",
    "# Cette section raconte l'histoire de la collecte avant de l'effectuer\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"üé≠ CHAPITRE 2 : COLLECTE OpenWeatherMap\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# V√©rifier l'√©tat actuel avant cette collecte\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        # Statistiques avant cette source\n",
    "        nb_sources_avant = conn.execute(text(\"SELECT COUNT(*) FROM source\")).scalar() or 0\n",
    "        nb_docs_avant = conn.execute(text(\"SELECT COUNT(*) FROM document\")).scalar() or 0\n",
    "        nb_flux_avant = conn.execute(text(\"SELECT COUNT(*) FROM flux\")).scalar() or 0\n",
    "        \n",
    "        print(f\"\\nüìä √âTAT ACTUEL DU PIPELINE (avant OpenWeatherMap):\")\n",
    "        print(f\"   ‚Ä¢ Sources configur√©es : {nb_sources_avant}\")\n",
    "        print(f\"   ‚Ä¢ Documents collect√©s : {nb_docs_avant:,}\")\n",
    "        print(f\"   ‚Ä¢ Flux de collecte : {nb_flux_avant}\")\n",
    "        \n",
    "        # Visualisation √©tat actuel\n",
    "        if nb_docs_avant > 0:\n",
    "            # Graphique progression\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            \n",
    "            categories = ['Sources', 'Flux', 'Documents']\n",
    "            valeurs = [nb_sources_avant, nb_flux_avant, nb_docs_avant]\n",
    "            colors = ['#FF6B6B', '#FECA57', '#4ECDC4']\n",
    "            \n",
    "            bars = ax.bar(categories, valeurs, color=colors)\n",
    "            for bar, val in zip(bars, valeurs):\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(valeurs) * 0.02,\n",
    "                       f\"{int(val):,}\", ha='center', va='bottom', fontweight='bold')\n",
    "            \n",
    "            ax.set_title(f\"üìä √âtat du pipeline AVANT collecte OpenWeatherMap\", fontsize=12, fontweight='bold')\n",
    "            ax.set_ylabel(\"Volume\", fontsize=11)\n",
    "            ax.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"\\nüí° Prochaine √©tape : Collecte OpenWeatherMap pour enrichir le dataset...\")\n",
    "        else:\n",
    "            print(f\"\\nüí° D√©marrage : Premi√®re collecte avec OpenWeatherMap...\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"\\nüí° Pr√™t pour collecte OpenWeatherMap...\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(f\"‚û°Ô∏è Lancement de la collecte OpenWeatherMap...\")\n",
    "print(\"-\"*80 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≠ Chapitre 2 : OpenWeatherMap\n",
    "\n",
    "**Contexte narratif** : Collecte de donn√©es depuis OpenWeatherMap\n",
    "\n",
    "**Avant cette collecte** :\n",
    "- Sources pr√©c√©dentes : 1 source(s) d√©j√† collect√©e(s)\n",
    "- Documents en base : [V√©rification en cours...]\n",
    "\n",
    "**Objectif de cette √©tape** :\n",
    "- Collecter de nouvelles donn√©es depuis OpenWeatherMap\n",
    "- Enrichir notre dataset avec cette source\n",
    "- Progression du pipeline vers le dataset final\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSens E1_v2 ‚Äî 03_ingest_sources\n",
    "\n",
    "- Objectifs: Collecte r√©elle des **5 types de sources** avec stockage hybride (PostgreSQL + MinIO)\n",
    "- Pr√©requis: 01_setup_env + 02_schema_create ex√©cut√©s\n",
    "- Sortie: Donn√©es collect√©es + visualisations + tables r√©elles √† chaque √©tape\n",
    "- Guide: docs/GUIDE_TECHNIQUE_E1.md\n",
    "\n",
    "> **E1_v2** : Collecte r√©elle fonctionnelle (18 tables PostgreSQL)\n",
    "> - Source 1 : Kaggle Dataset (split 50/50 PostgreSQL/MinIO)\n",
    "> - Source 2 : API OpenWeatherMap (m√©t√©o 4 villes)\n",
    "> - Source 3 : Flux RSS Multi-Sources (Franceinfo + 20 Minutes + Le Monde)\n",
    "> - Source 4 : NewsAPI (optionnel si cl√© API disponible)\n",
    "> - Source 5 : GDELT Big Data (√©chantillon France)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üé¨ DASHBOARD NARRATIF - O√ô SOMMES-NOUS ?\n",
    "# ============================================================\n",
    "# Ce dashboard vous guide √† travers le pipeline DataSens E1\n",
    "# Il montre la progression et l'√©tat actuel des donn√©es\n",
    "# ============================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üé¨ FIL D'ARIANE VISUEL - PIPELINE DATASENS E1\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Cr√©er figure dashboard\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 6)\n",
    "ax.axis('off')\n",
    "\n",
    "# √âtapes du pipeline\n",
    "etapes = [\n",
    "    {\"nom\": \"üì• COLLECTE\", \"status\": \"‚úÖ\", \"desc\": \"Sources brutes\"},\n",
    "    {\"nom\": \"‚òÅÔ∏è DATALAKE\", \"status\": \"‚úÖ\", \"desc\": \"MinIO Raw\"},\n",
    "    {\"nom\": \"üßπ NETTOYAGE\", \"status\": \"üîÑ\", \"desc\": \"D√©duplication\"},\n",
    "    {\"nom\": \"üíæ ETL\", \"status\": \"‚è≥\", \"desc\": \"PostgreSQL\"},\n",
    "    {\"nom\": \"üìä ANNOTATION\", \"status\": \"‚è≥\", \"desc\": \"Enrichissement\"},\n",
    "    {\"nom\": \"üì¶ EXPORT\", \"status\": \"‚è≥\", \"desc\": \"Dataset IA\"}\n",
    "]\n",
    "\n",
    "# Couleurs selon statut\n",
    "colors = {\n",
    "    \"‚úÖ\": \"#4ECDC4\",\n",
    "    \"üîÑ\": \"#FECA57\", \n",
    "    \"‚è≥\": \"#E8E8E8\"\n",
    "}\n",
    "\n",
    "# Dessiner timeline\n",
    "y_pos = 4\n",
    "x_start = 1\n",
    "x_spacing = 1.4\n",
    "\n",
    "for i, etape in enumerate(etapes):\n",
    "    x_pos = x_start + i * x_spacing\n",
    "    \n",
    "    # Cercle √©tape\n",
    "    circle = plt.Circle((x_pos, y_pos), 0.25, color=colors[etape[\"status\"]], zorder=3)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(x_pos, y_pos, etape[\"status\"], ha='center', va='center', fontsize=14, fontweight='bold', zorder=4)\n",
    "    \n",
    "    # Nom √©tape\n",
    "    ax.text(x_pos, y_pos - 0.6, etape[\"nom\"], ha='center', va='top', fontsize=11, fontweight='bold')\n",
    "    ax.text(x_pos, y_pos - 0.85, etape[\"desc\"], ha='center', va='top', fontsize=9, style='italic')\n",
    "    \n",
    "    # Fl√®che vers prochaine √©tape\n",
    "    if i < len(etapes) - 1:\n",
    "        ax.arrow(x_pos + 0.3, y_pos, x_spacing - 0.6, 0, \n",
    "                head_width=0.1, head_length=0.15, fc='gray', ec='gray', zorder=2)\n",
    "\n",
    "# Titre narratif\n",
    "ax.text(5, 5.5, \"üéØ PROGRESSION DU PIPELINE E1\", ha='center', va='center', \n",
    "        fontsize=16, fontweight='bold', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# L√©gende\n",
    "legend_elements = [\n",
    "    mpatches.Patch(facecolor='#4ECDC4', label='Termin√©'),\n",
    "    mpatches.Patch(facecolor='#FECA57', label='En cours'),\n",
    "    mpatches.Patch(facecolor='#E8E8E8', label='√Ä venir')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper left', fontsize=10)\n",
    "\n",
    "# Statistiques rapides (si disponibles)\n",
    "stats_text = \"\\nüìä SNAPSHOT ACTUEL :\\n\"\n",
    "try:\n",
    "    # Essayer de charger des stats si base disponible\n",
    "    stats_text += \"   ‚Ä¢ Pipeline en cours d'ex√©cution...\\n\"\n",
    "except:\n",
    "    stats_text += \"   ‚Ä¢ D√©marrage du pipeline...\\n\"\n",
    "\n",
    "ax.text(5, 1.5, stats_text, ha='center', va='center', fontsize=10,\n",
    "        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n",
    "\n",
    "plt.title(\"üé¨ FIL D'ARIANE VISUEL - Accompagnement narratif du jury\", \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Le fil d'Ariane vous guide √©tape par √©tape √† travers le pipeline\")\n",
    "print(\"   Chaque visualisation s'inscrit dans cette progression narrative\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notes:\n",
    "> - Lecture d‚Äôun flux RSS (Franceinfo) via `feedparser`.\n",
    "> - Construction d‚Äôun DataFrame normalis√©: `titre`, `texte`, `date_publication`, `langue`.\n",
    "> - Sauvegarde du brut en CSV (tra√ßabilit√©) et insertion en base.\n",
    "> - `get_source_id` assure l‚Äôexistence de la source; `flux` mat√©rialise la collecte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RSS: 20 articles ins√©r√©s\n"
     ]
    }
   ],
   "source": [
    "# DataSens E1_v2 - 03_ingest_sources\n",
    "# üì• Collecte r√©elle des 5 types de sources avec visualisations\n",
    "\n",
    "import datetime as dt\n",
    "import hashlib\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import feedparser\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "from minio import Minio\n",
    "from sqlalchemy import create_engine, text\n",
    "from tqdm import tqdm\n",
    "\n",
    "# R√©cup√©rer les variables du notebook 01\n",
    "if 'PROJECT_ROOT' not in globals():\n",
    "    current = Path.cwd()\n",
    "    PROJECT_ROOT = None\n",
    "    while current != current.parent:\n",
    "        if (current / \"notebooks\").exists() and (current / \"docs\").exists():\n",
    "            PROJECT_ROOT = current\n",
    "            break\n",
    "        current = current.parent\n",
    "    else:\n",
    "        PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "if 'RAW_DIR' not in globals():\n",
    "    RAW_DIR = PROJECT_ROOT / 'data' / 'raw'\n",
    "\n",
    "if 'PG_URL' not in globals():\n",
    "    PG_URL = os.getenv(\"DATASENS_PG_URL\", \"postgresql+psycopg2://postgres:postgres@localhost:5433/postgres\")\n",
    "\n",
    "if 'MINIO_ENDPOINT' not in globals():\n",
    "    MINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT\", \"http://localhost:9002\")\n",
    "    MINIO_ACCESS_KEY = os.getenv(\"MINIO_ACCESS_KEY\", \"admin\")\n",
    "    MINIO_SECRET_KEY = os.getenv(\"MINIO_SECRET_KEY\", \"admin123\")\n",
    "    MINIO_BUCKET = os.getenv(\"MINIO_BUCKET\", \"datasens-raw\")\n",
    "\n",
    "if 'ts' not in globals():\n",
    "    def ts() -> str:\n",
    "        return dt.datetime.now(tz=dt.UTC).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "\n",
    "if 'sha256_hash' not in globals():\n",
    "    def sha256_hash(s: str) -> str:\n",
    "        return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "# Connexions\n",
    "engine = create_engine(PG_URL, future=True)\n",
    "\n",
    "try:\n",
    "    minio_client = Minio(\n",
    "        MINIO_ENDPOINT.replace(\"http://\", \"\").replace(\"https://\", \"\"),\n",
    "        access_key=MINIO_ACCESS_KEY,\n",
    "        secret_key=MINIO_SECRET_KEY,\n",
    "        secure=False\n",
    "    )\n",
    "    if not minio_client.bucket_exists(MINIO_BUCKET):\n",
    "        minio_client.make_bucket(MINIO_BUCKET)\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è MinIO: {e}\")\n",
    "    minio_client = None\n",
    "\n",
    "print(\"‚úÖ Connexions pr√™tes (PostgreSQL + MinIO)\")\n",
    "print(\"=\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Utilitaires : Fonctions helpers pour la collecte\n",
    "\n",
    "Fonctions r√©utilisables pour :\n",
    "- **minio_upload()** : Upload fichier vers MinIO (DataLake)\n",
    "- **get_source_id()** : R√©cup√©rer ou cr√©er une source\n",
    "- **create_flux()** : Cr√©er un flux de collecte avec tra√ßabilit√©\n",
    "- **ensure_territoire()** : Cr√©er ou r√©cup√©rer un territoire\n",
    "- **insert_documents()** : Insertion batch avec gestion des doublons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è Fonctions utilitaires pour la collecte\n",
    "\n",
    "def minio_upload(local_path: Path, minio_path: str) -> str:\n",
    "    \"\"\"Upload un fichier vers MinIO et retourne l'URI\"\"\"\n",
    "    if minio_client is None:\n",
    "        return f\"local://{local_path}\"\n",
    "    try:\n",
    "        minio_client.fput_object(MINIO_BUCKET, minio_path, str(local_path))\n",
    "        return f\"s3://{MINIO_BUCKET}/{minio_path}\"\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Erreur MinIO upload: {e}\")\n",
    "        return f\"local://{local_path}\"\n",
    "\n",
    "def get_source_id(conn, nom: str) -> int:\n",
    "    \"\"\"R√©cup√®re l'ID d'une source ou la cr√©e si absente\"\"\"\n",
    "    result = conn.execute(text(\"SELECT id_source FROM source WHERE nom = :nom\"), {\"nom\": nom}).scalar()\n",
    "    if result:\n",
    "        return result\n",
    "    # Cr√©er la source (trouver type_donnee 'API' par d√©faut)\n",
    "    tid = conn.execute(text(\"SELECT id_type_donnee FROM type_donnee WHERE libelle = 'API' LIMIT 1\")).scalar()\n",
    "    if not tid:\n",
    "        tid = conn.execute(text(\"INSERT INTO type_donnee(libelle) VALUES ('API') RETURNING id_type_donnee\")).scalar()\n",
    "    return conn.execute(text(\"\"\"\n",
    "        INSERT INTO source(id_type_donnee, nom, url, fiabilite) \n",
    "        VALUES (:tid, :nom, '', 0.8) RETURNING id_source\n",
    "    \"\"\"), {\"tid\": tid, \"nom\": nom}).scalar()\n",
    "\n",
    "def create_flux(conn, source_nom: str, format_type: str = \"csv\", manifest_uri: str = None) -> int:\n",
    "    \"\"\"Cr√©e un flux de collecte et retourne son ID\"\"\"\n",
    "    sid = get_source_id(conn, source_nom)\n",
    "    return conn.execute(text(\"\"\"\n",
    "        INSERT INTO flux(id_source, format, manifest_uri, date_collecte)\n",
    "        VALUES (:sid, :format, :manifest, NOW()) RETURNING id_flux\n",
    "    \"\"\"), {\"sid\": sid, \"format\": format_type, \"manifest\": manifest_uri}).scalar()\n",
    "\n",
    "def ensure_territoire(conn, ville: str, code_insee: str = None, lat: float = None, lon: float = None) -> int:\n",
    "    \"\"\"Cr√©e ou r√©cup√®re un territoire\"\"\"\n",
    "    result = conn.execute(text(\"SELECT id_territoire FROM territoire WHERE ville = :ville\"), {\"ville\": ville}).scalar()\n",
    "    if result:\n",
    "        return result\n",
    "    return conn.execute(text(\"\"\"\n",
    "        INSERT INTO territoire(ville, code_insee, lat, lon) \n",
    "        VALUES (:ville, :code, :lat, :lon) RETURNING id_territoire\n",
    "    \"\"\"), {\"ville\": ville, \"code\": code_insee, \"lat\": lat, \"lon\": lon}).scalar()\n",
    "\n",
    "def insert_documents(conn, df: pd.DataFrame, flux_id: int):\n",
    "    \"\"\"Insertion batch de documents avec gestion des doublons\"\"\"\n",
    "    inserted = 0\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            conn.execute(text(\"\"\"\n",
    "                INSERT INTO document(id_flux, titre, texte, langue, date_publication, hash_fingerprint)\n",
    "                VALUES(:fid, :titre, :texte, :langue, :date, :hash)\n",
    "                ON CONFLICT (hash_fingerprint) DO NOTHING\n",
    "            \"\"\"), {\n",
    "                \"fid\": flux_id,\n",
    "                \"titre\": row.get(\"titre\", \"\"),\n",
    "                \"texte\": row.get(\"texte\", \"\"),\n",
    "                \"langue\": row.get(\"langue\", \"fr\"),\n",
    "                \"date\": row.get(\"date_publication\"),\n",
    "                \"hash\": row.get(\"hash_fingerprint\", \"\")\n",
    "            })\n",
    "            inserted += 1\n",
    "        except Exception as e:\n",
    "            pass  # Doublon ou erreur silencieuse\n",
    "    return inserted\n",
    "\n",
    "# =====================================================\n",
    "# FONCTIONS UTILITAIRES DE S√âCURIT√â\n",
    "# =====================================================\n",
    "def assert_valid_identifier(name: str) -> None:\n",
    "    \"\"\"\n",
    "    Valide qu'un identifiant SQL (nom de table, colonne) est s√ªr.\n",
    "    L√®ve une ValueError si l'identifiant contient des caract√®res non autoris√©s.\n",
    "    \"\"\"\n",
    "    if not isinstance(name, str):\n",
    "        raise ValueError(\"L'identifiant doit √™tre une cha√Æne de caract√®res.\")\n",
    "    # Autorise lettres, chiffres, underscores, et points (pour sch√©mas.tables)\n",
    "    if not name.replace('_', '').replace('.', '').isalnum():\n",
    "        raise ValueError(f\"Identifiant SQL invalide : {name}. Seuls les caract√®res alphanum√©riques, underscores et points sont autoris√©s.\")\n",
    "\n",
    "def load_whitelist_tables(conn, schema: str = 'public') -> set[str]:\n",
    "    \"\"\"\n",
    "    Charge une liste blanche des noms de tables valides depuis information_schema.\n",
    "    Retourne un set des noms de tables pour validation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = conn.execute(text(f\"\"\"\n",
    "            SELECT table_name FROM information_schema.tables\n",
    "            WHERE table_schema = :schema_name\n",
    "        \"\"\"), {\"schema_name\": schema}).fetchall()\n",
    "        return {row[0] for row in result}\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur lors du chargement de la whitelist des tables: {e}\")\n",
    "        return set()  # Retourne un set vide en cas d'erreur\n",
    "\n",
    "print(\"‚úÖ Fonctions utilitaires charg√©es\")\n",
    "print(\"‚úÖ Fonctions de s√©curit√© (assert_valid_identifier, load_whitelist_tables) charg√©es.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≠ Chapitre 1 : RSS Multi-Sources\n",
    "\n",
    "**Contexte narratif** : Collecte de donn√©es depuis RSS Multi-Sources\n",
    "\n",
    "**Avant cette collecte** :\n",
    "- Sources pr√©c√©dentes : 0 source(s) d√©j√† collect√©e(s)\n",
    "- Documents en base : [V√©rification en cours...]\n",
    "\n",
    "**Objectif de cette √©tape** :\n",
    "- Collecter de nouvelles donn√©es depuis RSS Multi-Sources\n",
    "- Enrichir notre dataset avec cette source\n",
    "- Progression du pipeline vers le dataset final\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üé≠ STORYTELLING : PR√âPARATION COLLECTE 1 - RSS Multi-Sources\n",
    "# ============================================================\n",
    "# Cette section raconte l'histoire de la collecte avant de l'effectuer\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"üé≠ CHAPITRE 1 : COLLECTE RSS Multi-Sources\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# V√©rifier l'√©tat actuel avant cette collecte\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        # Statistiques avant cette source\n",
    "        nb_sources_avant = conn.execute(text(\"SELECT COUNT(*) FROM source\")).scalar() or 0\n",
    "        nb_docs_avant = conn.execute(text(\"SELECT COUNT(*) FROM document\")).scalar() or 0\n",
    "        nb_flux_avant = conn.execute(text(\"SELECT COUNT(*) FROM flux\")).scalar() or 0\n",
    "        \n",
    "        print(f\"\\nüìä √âTAT ACTUEL DU PIPELINE (avant RSS Multi-Sources):\")\n",
    "        print(f\"   ‚Ä¢ Sources configur√©es : {nb_sources_avant}\")\n",
    "        print(f\"   ‚Ä¢ Documents collect√©s : {nb_docs_avant:,}\")\n",
    "        print(f\"   ‚Ä¢ Flux de collecte : {nb_flux_avant}\")\n",
    "        \n",
    "        # Visualisation √©tat actuel\n",
    "        if nb_docs_avant > 0:\n",
    "            # Graphique progression\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            \n",
    "            categories = ['Sources', 'Flux', 'Documents']\n",
    "            valeurs = [nb_sources_avant, nb_flux_avant, nb_docs_avant]\n",
    "            colors = ['#FF6B6B', '#FECA57', '#4ECDC4']\n",
    "            \n",
    "            bars = ax.bar(categories, valeurs, color=colors)\n",
    "            for bar, val in zip(bars, valeurs):\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(valeurs) * 0.02,\n",
    "                       f\"{int(val):,}\", ha='center', va='bottom', fontweight='bold')\n",
    "            \n",
    "            ax.set_title(f\"üìä √âtat du pipeline AVANT collecte RSS Multi-Sources\", fontsize=12, fontweight='bold')\n",
    "            ax.set_ylabel(\"Volume\", fontsize=11)\n",
    "            ax.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"\\nüí° Prochaine √©tape : Collecte RSS Multi-Sources pour enrichir le dataset...\")\n",
    "        else:\n",
    "            print(f\"\\nüí° D√©marrage : Premi√®re collecte avec RSS Multi-Sources...\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"\\nüí° Pr√™t pour collecte RSS Multi-Sources...\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(f\"‚û°Ô∏è Lancement de la collecte RSS Multi-Sources...\")\n",
    "print(\"-\"*80 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≠ Chapitre 1 : RSS Multi-Sources\n",
    "\n",
    "**Contexte narratif** : Collecte de donn√©es depuis RSS Multi-Sources\n",
    "\n",
    "**Avant cette collecte** :\n",
    "- Sources pr√©c√©dentes : 0 source(s) d√©j√† collect√©e(s)\n",
    "- Documents en base : [V√©rification en cours...]\n",
    "\n",
    "**Objectif de cette √©tape** :\n",
    "- Collecter de nouvelles donn√©es depuis RSS Multi-Sources\n",
    "- Enrichir notre dataset avec cette source\n",
    "- Progression du pipeline vers le dataset final\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì∞ Source 1 : Flux RSS Multi-Sources (Presse fran√ßaise)\n",
    "\n",
    "Collecte d'articles depuis 3 flux RSS fran√ßais :\n",
    "- **Franceinfo** : Service public, actualit√©s g√©n√©rales\n",
    "- **20 Minutes** : Presse gratuite, grand public  \n",
    "- **Le Monde** : Presse de r√©f√©rence\n",
    "\n",
    "**Process** : Parsing RSS ‚Üí DataFrame ‚Üí D√©duplication SHA256 ‚Üí PostgreSQL + MinIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì∞ Source 1 : Flux RSS Multi-Sources\n",
    "print(\"üì∞ SOURCE 1 : Flux RSS Multi-Sources\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "RSS_SOURCES = {\n",
    "    \"Franceinfo\": \"https://www.francetvinfo.fr/titres.rss\",\n",
    "    \"20 Minutes\": \"https://www.20minutes.fr/feeds/rss-une.xml\",\n",
    "    \"Le Monde\": \"https://www.lemonde.fr/rss/une.xml\"\n",
    "}\n",
    "\n",
    "all_rss_items = []\n",
    "\n",
    "for source_name, rss_url in RSS_SOURCES.items():\n",
    "    print(f\"\\nüì° Source : {source_name}\")\n",
    "    try:\n",
    "        feed = feedparser.parse(rss_url)\n",
    "        if len(feed.entries) == 0:\n",
    "            print(\"   ‚ö†Ô∏è Aucun article\")\n",
    "            continue\n",
    "        \n",
    "        source_items = []\n",
    "        for e in feed.entries[:30]:  # Max 30 par source\n",
    "            titre = e.get(\"title\", \"\").strip()\n",
    "            texte = (e.get(\"summary\", \"\") or e.get(\"description\", \"\") or \"\").strip()\n",
    "            if titre and texte:\n",
    "                source_items.append({\n",
    "                    \"titre\": titre,\n",
    "                    \"texte\": texte,\n",
    "                    \"date_publication\": pd.to_datetime(e.get(\"published\", \"\"), errors=\"coerce\"),\n",
    "                    \"langue\": \"fr\",\n",
    "                    \"source_media\": source_name,\n",
    "                    \"url\": e.get(\"link\", \"\")\n",
    "                })\n",
    "        all_rss_items.extend(source_items)\n",
    "        print(f\"   ‚úÖ {len(source_items)} articles collect√©s\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Erreur : {str(e)[:80]}\")\n",
    "    time.sleep(1)\n",
    "\n",
    "# Consolidation\n",
    "df_rss = pd.DataFrame(all_rss_items)\n",
    "if len(df_rss) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è Aucun article RSS collect√©\")\n",
    "else:\n",
    "    print(f\"\\nüìä Total brut : {len(df_rss)} articles\")\n",
    "    \n",
    "    # D√©duplication\n",
    "    df_rss[\"hash_fingerprint\"] = df_rss.apply(\n",
    "        lambda row: sha256_hash(row[\"titre\"] + \" \" + row[\"texte\"]), axis=1\n",
    "    )\n",
    "    nb_avant = len(df_rss)\n",
    "    df_rss = df_rss.drop_duplicates(subset=[\"hash_fingerprint\"])\n",
    "    nb_apres = len(df_rss)\n",
    "    print(f\"üßπ D√©duplication : {nb_avant} ‚Üí {nb_apres} articles uniques\")\n",
    "    \n",
    "    # Sauvegarde locale + MinIO\n",
    "    local = RAW_DIR / \"rss\" / f\"rss_multi_{ts()}.csv\"\n",
    "    local.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_rss.to_csv(local, index=False)\n",
    "    minio_uri = minio_upload(local, f\"rss/{local.name}\")\n",
    "    \n",
    "    # Insertion PostgreSQL\n",
    "    with engine.begin() as conn:\n",
    "        flux_id = create_flux(conn, \"Flux RSS Multi-Sources (Franceinfo + 20 Minutes + Le Monde)\", \"rss\", minio_uri)\n",
    "        inserted = insert_documents(conn, df_rss[[\"titre\", \"texte\", \"langue\", \"date_publication\", \"hash_fingerprint\"]], flux_id)\n",
    "    \n",
    "    print(f\"\\n‚úÖ RSS : {inserted} articles ins√©r√©s en base + MinIO\")\n",
    "    print(f\"‚òÅÔ∏è MinIO : {minio_uri}\")\n",
    "    \n",
    "    # üìä Visualisations\n",
    "    print(\"\\nüìä R√©partition par source m√©diatique :\")\n",
    "    lang_counts = df_rss['source_media'].value_counts()\n",
    "    display(pd.DataFrame({\"Source\": lang_counts.index, \"Nombre\": lang_counts.values}))\n",
    "    \n",
    "    if len(lang_counts) > 0:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        bars = plt.bar(lang_counts.index, lang_counts.values, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "        for bar, value in zip(bars, lang_counts.values):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                    str(value), ha='center', va='bottom', fontweight='bold')\n",
    "        plt.title(\"üìä R√©partition des articles RSS par source\", fontsize=12, fontweight='bold')\n",
    "        plt.ylabel(\"Nombre d'articles\", fontsize=11)\n",
    "        plt.xticks(rotation=15, ha='right')\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # üìã Table de donn√©es r√©elles\n",
    "    print(\"\\nüìã Table 'document' - Articles RSS ins√©r√©s (aper√ßu 10 premiers) :\")\n",
    "    df_docs = pd.read_sql_query(\"\"\"\n",
    "        SELECT d.id_doc, d.titre, d.langue, d.date_publication, s.nom AS source\n",
    "        FROM document d\n",
    "        JOIN flux f ON d.id_flux = f.id_flux\n",
    "        JOIN source s ON f.id_source = s.id_source\n",
    "        WHERE s.nom LIKE '%RSS%'\n",
    "        ORDER BY d.id_doc DESC\n",
    "        LIMIT 10\n",
    "    \"\"\", engine)\n",
    "    display(df_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≠ Chapitre 2 : OpenWeatherMap\n",
    "\n",
    "**Contexte narratif** : Collecte de donn√©es depuis OpenWeatherMap\n",
    "\n",
    "**Avant cette collecte** :\n",
    "- Sources pr√©c√©dentes : 1 source(s) d√©j√† collect√©e(s)\n",
    "- Documents en base : [V√©rification en cours...]\n",
    "\n",
    "**Objectif de cette √©tape** :\n",
    "- Collecter de nouvelles donn√©es depuis OpenWeatherMap\n",
    "- Enrichir notre dataset avec cette source\n",
    "- Progression du pipeline vers le dataset final\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≠ Chapitre 2 : OpenWeatherMap\n",
    "\n",
    "**Contexte narratif** : Collecte de donn√©es depuis OpenWeatherMap\n",
    "\n",
    "**Avant cette collecte** :\n",
    "- Sources pr√©c√©dentes : 1 source(s) d√©j√† collect√©e(s)\n",
    "- Documents en base : [V√©rification en cours...]\n",
    "\n",
    "**Objectif de cette √©tape** :\n",
    "- Collecter de nouvelles donn√©es depuis OpenWeatherMap\n",
    "- Enrichir notre dataset avec cette source\n",
    "- Progression du pipeline vers le dataset final\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üå¶Ô∏è Source 2 : API OpenWeatherMap (M√©t√©o en temps r√©el)\n",
    "\n",
    "Collecte de donn√©es m√©t√©o pour 4 villes fran√ßaises :\n",
    "- **Paris, Lyon, Marseille, Toulouse**\n",
    "\n",
    "**Donn√©es** : Temp√©rature, humidit√©, vent, pression, type m√©t√©o\n",
    "\n",
    "**Stockage** : PostgreSQL (table `meteo` + `territoire`) + MinIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üå¶Ô∏è Source 2 : API OpenWeatherMap\n",
    "print(\"\\nüå¶Ô∏è SOURCE 2 : API OpenWeatherMap\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "OWM_CITIES = [\"Paris,FR\", \"Lyon,FR\", \"Marseille,FR\", \"Toulouse,FR\"]\n",
    "OWM_API_KEY = os.getenv(\"OWM_API_KEY\")\n",
    "\n",
    "if not OWM_API_KEY:\n",
    "    print(\"‚ö†Ô∏è OWM_API_KEY manquante - Source 2 ignor√©e\")\n",
    "else:\n",
    "    rows = []\n",
    "    for city in tqdm(OWM_CITIES, desc=\"OWM\"):\n",
    "        try:\n",
    "            r = requests.get(\n",
    "                \"https://api.openweathermap.org/data/2.5/weather\",\n",
    "                params={\"q\": city, \"appid\": OWM_API_KEY, \"units\": \"metric\", \"lang\": \"fr\"},\n",
    "                timeout=10\n",
    "            )\n",
    "            if r.status_code == 200:\n",
    "                j = r.json()\n",
    "                rows.append({\n",
    "                    \"ville\": j[\"name\"],\n",
    "                    \"lat\": j[\"coord\"][\"lat\"],\n",
    "                    \"lon\": j[\"coord\"][\"lon\"],\n",
    "                    \"date_obs\": pd.to_datetime(j[\"dt\"], unit=\"s\"),\n",
    "                    \"temperature\": j[\"main\"][\"temp\"],\n",
    "                    \"humidite\": j[\"main\"][\"humidity\"],\n",
    "                    \"vent_kmh\": (j.get(\"wind\", {}).get(\"speed\") or 0) * 3.6,\n",
    "                    \"pression\": j.get(\"main\", {}).get(\"pressure\"),\n",
    "                    \"meteo_type\": j[\"weather\"][0][\"main\"] if j.get(\"weather\") else None\n",
    "                })\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Erreur {city}: {str(e)[:60]}\")\n",
    "    \n",
    "    if rows:\n",
    "        df_owm = pd.DataFrame(rows)\n",
    "        local = RAW_DIR / \"api\" / \"owm\" / f\"owm_{ts()}.csv\"\n",
    "        local.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df_owm.to_csv(local, index=False)\n",
    "        minio_uri = minio_upload(local, f\"api/owm/{local.name}\")\n",
    "        \n",
    "        # Insertion PostgreSQL\n",
    "        with engine.begin() as conn:\n",
    "            flux_id = create_flux(conn, \"OpenWeatherMap\", \"json\", minio_uri)\n",
    "            for _, r in df_owm.iterrows():\n",
    "                tid = ensure_territoire(conn, r[\"ville\"], lat=r[\"lat\"], lon=r[\"lon\"])\n",
    "                conn.execute(text(\"\"\"\n",
    "                    INSERT INTO meteo(id_territoire, date_obs, temperature, humidite, vent_kmh, pression, meteo_type)\n",
    "                    VALUES(:t, :d, :T, :H, :V, :P, :MT)\n",
    "                \"\"\"), {\n",
    "                    \"t\": tid, \"d\": r[\"date_obs\"], \"T\": r[\"temperature\"],\n",
    "                    \"H\": r[\"humidite\"], \"V\": r[\"vent_kmh\"], \"P\": r[\"pression\"], \"MT\": r[\"meteo_type\"]\n",
    "                })\n",
    "        \n",
    "        print(f\"\\n‚úÖ OWM : {len(df_owm)} relev√©s ins√©r√©s en base + MinIO\")\n",
    "        print(f\"‚òÅÔ∏è MinIO : {minio_uri}\")\n",
    "        \n",
    "        # üìä Visualisations\n",
    "        print(\"\\nüìä R√©partition des relev√©s par ville :\")\n",
    "        display(df_owm[[\"ville\", \"temperature\", \"humidite\", \"meteo_type\"]])\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        bars = plt.bar(df_owm[\"ville\"], df_owm[\"temperature\"], color='#FF6B6B')\n",
    "        for bar, value in zip(bars, df_owm[\"temperature\"]):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                    f\"{value:.1f}¬∞C\", ha='center', va='bottom', fontweight='bold')\n",
    "        plt.title(\"üå°Ô∏è Temp√©rature par ville\", fontsize=12, fontweight='bold')\n",
    "        plt.ylabel(\"Temp√©rature (¬∞C)\", fontsize=11)\n",
    "        plt.xticks(rotation=15)\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        bars = plt.bar(df_owm[\"ville\"], df_owm[\"humidite\"], color='#4ECDC4')\n",
    "        for bar, value in zip(bars, df_owm[\"humidite\"]):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                    f\"{value}%\", ha='center', va='bottom', fontweight='bold')\n",
    "        plt.title(\"üíß Humidit√© par ville\", fontsize=12, fontweight='bold')\n",
    "        plt.ylabel(\"Humidit√© (%)\", fontsize=11)\n",
    "        plt.xticks(rotation=15)\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # üìã Tables de donn√©es r√©elles\n",
    "        print(\"\\nüìã Table 'meteo' - Relev√©s ins√©r√©s :\")\n",
    "        df_meteo = pd.read_sql_query(\"\"\"\n",
    "            SELECT m.id_meteo, t.ville, m.date_obs, m.temperature, m.humidite, m.meteo_type\n",
    "            FROM meteo m\n",
    "            JOIN territoire t ON m.id_territoire = t.id_territoire\n",
    "            ORDER BY m.id_meteo DESC\n",
    "            LIMIT 10\n",
    "        \"\"\", engine)\n",
    "        display(df_meteo)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Aucun relev√© m√©t√©o collect√©\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≠ Chapitre 3 : NewsAPI\n",
    "\n",
    "**Contexte narratif** : Collecte de donn√©es depuis NewsAPI\n",
    "\n",
    "**Avant cette collecte** :\n",
    "- Sources pr√©c√©dentes : 2 source(s) d√©j√† collect√©e(s)\n",
    "- Documents en base : [V√©rification en cours...]\n",
    "\n",
    "**Objectif de cette √©tape** :\n",
    "- Collecter de nouvelles donn√©es depuis NewsAPI\n",
    "- Enrichir notre dataset avec cette source\n",
    "- Progression du pipeline vers le dataset final\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üé≠ STORYTELLING : PR√âPARATION COLLECTE 3 - NewsAPI\n",
    "# ============================================================\n",
    "# Cette section raconte l'histoire de la collecte avant de l'effectuer\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"üé≠ CHAPITRE 3 : COLLECTE NewsAPI\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# V√©rifier l'√©tat actuel avant cette collecte\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        # Statistiques avant cette source\n",
    "        nb_sources_avant = conn.execute(text(\"SELECT COUNT(*) FROM source\")).scalar() or 0\n",
    "        nb_docs_avant = conn.execute(text(\"SELECT COUNT(*) FROM document\")).scalar() or 0\n",
    "        nb_flux_avant = conn.execute(text(\"SELECT COUNT(*) FROM flux\")).scalar() or 0\n",
    "        \n",
    "        print(f\"\\nüìä √âTAT ACTUEL DU PIPELINE (avant NewsAPI):\")\n",
    "        print(f\"   ‚Ä¢ Sources configur√©es : {nb_sources_avant}\")\n",
    "        print(f\"   ‚Ä¢ Documents collect√©s : {nb_docs_avant:,}\")\n",
    "        print(f\"   ‚Ä¢ Flux de collecte : {nb_flux_avant}\")\n",
    "        \n",
    "        # Visualisation √©tat actuel\n",
    "        if nb_docs_avant > 0:\n",
    "            # Graphique progression\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            \n",
    "            categories = ['Sources', 'Flux', 'Documents']\n",
    "            valeurs = [nb_sources_avant, nb_flux_avant, nb_docs_avant]\n",
    "            colors = ['#FF6B6B', '#FECA57', '#4ECDC4']\n",
    "            \n",
    "            bars = ax.bar(categories, valeurs, color=colors)\n",
    "            for bar, val in zip(bars, valeurs):\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(valeurs) * 0.02,\n",
    "                       f\"{int(val):,}\", ha='center', va='bottom', fontweight='bold')\n",
    "            \n",
    "            ax.set_title(f\"üìä √âtat du pipeline AVANT collecte NewsAPI\", fontsize=12, fontweight='bold')\n",
    "            ax.set_ylabel(\"Volume\", fontsize=11)\n",
    "            ax.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"\\nüí° Prochaine √©tape : Collecte NewsAPI pour enrichir le dataset...\")\n",
    "        else:\n",
    "            print(f\"\\nüí° D√©marrage : Premi√®re collecte avec NewsAPI...\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"\\nüí° Pr√™t pour collecte NewsAPI...\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(f\"‚û°Ô∏è Lancement de la collecte NewsAPI...\")\n",
    "print(\"-\"*80 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≠ Chapitre 3 : NewsAPI\n",
    "\n",
    "**Contexte narratif** : Collecte de donn√©es depuis NewsAPI\n",
    "\n",
    "**Avant cette collecte** :\n",
    "- Sources pr√©c√©dentes : 2 source(s) d√©j√† collect√©e(s)\n",
    "- Documents en base : [V√©rification en cours...]\n",
    "\n",
    "**Objectif de cette √©tape** :\n",
    "- Collecter de nouvelles donn√©es depuis NewsAPI\n",
    "- Enrichir notre dataset avec cette source\n",
    "- Progression du pipeline vers le dataset final\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì∞ Source 3 : NewsAPI (Actualit√©s - Optionnel)\n",
    "\n",
    "Collecte d'articles via l'API NewsAPI si la cl√© est configur√©e.\n",
    "\n",
    "**Quota gratuit** : 1000 requ√™tes/jour (peut √™tre √©puis√©)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì∞ Source 3 : NewsAPI (Optionnel)\n",
    "print(\"\\nüì∞ SOURCE 3 : NewsAPI (Optionnel)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "NEWSAPI_KEY = os.getenv(\"NEWSAPI_KEY\")\n",
    "\n",
    "if not NEWSAPI_KEY:\n",
    "    print(\"‚ö†Ô∏è NEWSAPI_KEY manquante - Source 3 ignor√©e\")\n",
    "else:\n",
    "    NEWS_CATEGORIES = [\"general\", \"technology\", \"health\", \"business\"]\n",
    "    all_articles = []\n",
    "    \n",
    "    for category in NEWS_CATEGORIES:\n",
    "        try:\n",
    "            r = requests.get(\n",
    "                \"https://newsapi.org/v2/top-headlines\",\n",
    "                params={\"apiKey\": NEWSAPI_KEY, \"country\": \"fr\", \"category\": category, \"pageSize\": 20},\n",
    "                timeout=10\n",
    "            )\n",
    "            if r.status_code == 200:\n",
    "                data = r.json()\n",
    "                articles = data.get(\"articles\", [])\n",
    "                for art in articles:\n",
    "                    all_articles.append({\n",
    "                        \"titre\": (art.get(\"title\") or \"\").strip(),\n",
    "                        \"texte\": (art.get(\"description\") or art.get(\"content\") or \"\").strip(),\n",
    "                        \"date_publication\": pd.to_datetime(art.get(\"publishedAt\"), errors=\"coerce\"),\n",
    "                        \"langue\": \"fr\",\n",
    "                        \"categorie\": category\n",
    "                    })\n",
    "            elif r.status_code in [426, 429]:\n",
    "                print(f\"   ‚ö†Ô∏è Quota √©puis√© pour {category}\")\n",
    "                break\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Erreur {category}: {str(e)[:60]}\")\n",
    "    \n",
    "    if all_articles:\n",
    "        df_news = pd.DataFrame(all_articles)\n",
    "        df_news = df_news[df_news[\"texte\"].str.len() > 20].copy()\n",
    "        df_news[\"hash_fingerprint\"] = df_news.apply(\n",
    "            lambda row: sha256_hash(row[\"titre\"] + \" \" + row[\"texte\"]), axis=1\n",
    "        )\n",
    "        df_news = df_news.drop_duplicates(subset=[\"hash_fingerprint\"])\n",
    "        \n",
    "        local = RAW_DIR / \"api\" / \"newsapi\" / f\"newsapi_{ts()}.csv\"\n",
    "        local.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df_news.to_csv(local, index=False)\n",
    "        minio_uri = minio_upload(local, f\"api/newsapi/{local.name}\")\n",
    "        \n",
    "        with engine.begin() as conn:\n",
    "            flux_id = create_flux(conn, \"NewsAPI\", \"json\", minio_uri)\n",
    "            inserted = insert_documents(conn, df_news[[\"titre\", \"texte\", \"langue\", \"date_publication\", \"hash_fingerprint\"]], flux_id)\n",
    "        \n",
    "        print(f\"\\n‚úÖ NewsAPI : {inserted} articles ins√©r√©s\")\n",
    "        \n",
    "        # üìä Visualisation\n",
    "        if len(df_news) > 0:\n",
    "            cat_counts = df_news['categorie'].value_counts()\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.pie(cat_counts.values, labels=cat_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "            plt.title(\"üìä R√©partition NewsAPI par cat√©gorie\", fontsize=12, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # üìã Table de donn√©es\n",
    "        print(\"\\nüìã Table 'document' - Articles NewsAPI (aper√ßu 5 premiers) :\")\n",
    "        df_newsapi = pd.read_sql_query(\"\"\"\n",
    "            SELECT d.id_doc, d.titre, d.date_publication\n",
    "            FROM document d\n",
    "            JOIN flux f ON d.id_flux = f.id_flux\n",
    "            JOIN source s ON f.id_source = s.id_source\n",
    "            WHERE s.nom = 'NewsAPI'\n",
    "            ORDER BY d.id_doc DESC\n",
    "            LIMIT 5\n",
    "        \"\"\", engine)\n",
    "        display(df_newsapi)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Aucun article NewsAPI r√©cup√©r√© (quota √©puis√© ou cl√© invalide)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Bilan de la collecte E1_v2\n",
    "\n",
    "R√©capitulatif de toutes les sources collect√©es avec statistiques globales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Bilan global de la collecte\n",
    "print(\"\\nüìä BILAN GLOBAL DE LA COLLECTE E1_v2\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Statistiques par source\n",
    "with engine.connect() as conn:\n",
    "    stats = pd.read_sql_query(\"\"\"\n",
    "        SELECT \n",
    "            s.nom AS source,\n",
    "            COUNT(DISTINCT f.id_flux) AS nb_flux,\n",
    "            COUNT(DISTINCT d.id_doc) AS nb_documents,\n",
    "            td.libelle AS type_donnee\n",
    "        FROM source s\n",
    "        LEFT JOIN flux f ON s.id_source = f.id_source\n",
    "        LEFT JOIN document d ON f.id_flux = d.id_flux\n",
    "        LEFT JOIN type_donnee td ON s.id_type_donnee = td.id_type_donnee\n",
    "        GROUP BY s.nom, td.libelle\n",
    "        ORDER BY nb_documents DESC\n",
    "    \"\"\", conn)\n",
    "\n",
    "print(\"\\nüìà Statistiques par source :\")\n",
    "display(stats)\n",
    "\n",
    "# Total documents\n",
    "total_docs = stats['nb_documents'].sum()\n",
    "print(f\"\\nüìä Total documents collect√©s : {total_docs}\")\n",
    "\n",
    "# Graphique global\n",
    "if len(stats) > 0:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(stats[\"source\"], stats[\"nb_documents\"], color=plt.cm.Set3(range(len(stats))))\n",
    "    for bar, value in zip(bars, stats[\"nb_documents\"]):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                str(int(value)), ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "    plt.title(\"üìä Nombre de documents collect√©s par source\", fontsize=14, fontweight='bold')\n",
    "    plt.ylabel(\"Nombre de documents\", fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Vue compl√®te : tous les documents avec contexte\n",
    "print(\"\\nüìã Vue compl√®te - Tous les documents avec contexte (50 premiers) :\")\n",
    "df_all_docs = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        d.id_doc,\n",
    "        d.titre,\n",
    "        LEFT(d.texte, 100) AS texte_apercu,\n",
    "        d.langue,\n",
    "        d.date_publication,\n",
    "        s.nom AS source,\n",
    "        f.date_collecte,\n",
    "        f.format\n",
    "    FROM document d\n",
    "    JOIN flux f ON d.id_flux = f.id_flux\n",
    "    JOIN source s ON f.id_source = s.id_source\n",
    "    ORDER BY d.id_doc DESC\n",
    "    LIMIT 50\n",
    "\"\"\", engine)\n",
    "display(df_all_docs)\n",
    "\n",
    "# Statistiques par type de donn√©e\n",
    "print(\"\\nüìä R√©partition par type de donn√©e :\")\n",
    "df_types = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        td.libelle AS type_donnee,\n",
    "        COUNT(DISTINCT s.id_source) AS nb_sources,\n",
    "        COUNT(DISTINCT d.id_doc) AS nb_documents\n",
    "    FROM type_donnee td\n",
    "    LEFT JOIN source s ON td.id_type_donnee = s.id_type_donnee\n",
    "    LEFT JOIN flux f ON s.id_source = f.id_source\n",
    "    LEFT JOIN document d ON f.id_flux = d.id_flux\n",
    "    GROUP BY td.libelle\n",
    "    ORDER BY nb_documents DESC\n",
    "\"\"\", engine)\n",
    "display(df_types)\n",
    "\n",
    "if len(df_types) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(df_types[\"type_donnee\"], df_types[\"nb_documents\"], color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "    for bar, value in zip(bars, df_types[\"nb_documents\"]):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                str(int(value)), ha='center', va='bottom', fontweight='bold')\n",
    "    plt.title(\"üìä R√©partition des documents par type de donn√©e\", fontsize=12, fontweight='bold')\n",
    "    plt.ylabel(\"Nombre de documents\", fontsize=11)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Collecte E1_v2 termin√©e : {total_docs} documents collect√©s et stock√©s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
