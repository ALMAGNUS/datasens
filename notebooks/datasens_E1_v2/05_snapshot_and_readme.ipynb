{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSens E1_v2 ‚Äî 05_snapshot_and_readme\n",
    "\n",
    "- Objectifs: Manifest JSON complet, snapshot PostgreSQL, versioning, bilan final\n",
    "- Pr√©requis: 04_quality_checks ex√©cut√©\n",
    "- Sorties: `data/raw/manifests/manifest_*.json` + snapshot DB + `README_VERSIONNING.md`\n",
    "- Guide: docs/GUIDE_TECHNIQUE_E1.md\n",
    "\n",
    "> **E1_v2** : Finalisation avec tra√ßabilit√© compl√®te et snapshot versionn√©\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üé¨ DASHBOARD NARRATIF - O√ô SOMMES-NOUS ?\n",
    "# ============================================================\n",
    "# Ce dashboard vous guide √† travers le pipeline DataSens E1\n",
    "# Il montre la progression et l'√©tat actuel des donn√©es\n",
    "# ============================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üé¨ FIL D'ARIANE VISUEL - PIPELINE DATASENS E1\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Cr√©er figure dashboard\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 6)\n",
    "ax.axis('off')\n",
    "\n",
    "# √âtapes du pipeline\n",
    "etapes = [\n",
    "    {\"nom\": \"üì• COLLECTE\", \"status\": \"‚úÖ\", \"desc\": \"Sources brutes\"},\n",
    "    {\"nom\": \"‚òÅÔ∏è DATALAKE\", \"status\": \"‚úÖ\", \"desc\": \"MinIO Raw\"},\n",
    "    {\"nom\": \"üßπ NETTOYAGE\", \"status\": \"üîÑ\", \"desc\": \"D√©duplication\"},\n",
    "    {\"nom\": \"üíæ ETL\", \"status\": \"‚è≥\", \"desc\": \"PostgreSQL\"},\n",
    "    {\"nom\": \"üìä ANNOTATION\", \"status\": \"‚è≥\", \"desc\": \"Enrichissement\"},\n",
    "    {\"nom\": \"üì¶ EXPORT\", \"status\": \"‚è≥\", \"desc\": \"Dataset IA\"}\n",
    "]\n",
    "\n",
    "# Couleurs selon statut\n",
    "colors = {\n",
    "    \"‚úÖ\": \"#4ECDC4\",\n",
    "    \"üîÑ\": \"#FECA57\", \n",
    "    \"‚è≥\": \"#E8E8E8\"\n",
    "}\n",
    "\n",
    "# Dessiner timeline\n",
    "y_pos = 4\n",
    "x_start = 1\n",
    "x_spacing = 1.4\n",
    "\n",
    "for i, etape in enumerate(etapes):\n",
    "    x_pos = x_start + i * x_spacing\n",
    "    \n",
    "    # Cercle √©tape\n",
    "    circle = plt.Circle((x_pos, y_pos), 0.25, color=colors[etape[\"status\"]], zorder=3)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(x_pos, y_pos, etape[\"status\"], ha='center', va='center', fontsize=14, fontweight='bold', zorder=4)\n",
    "    \n",
    "    # Nom √©tape\n",
    "    ax.text(x_pos, y_pos - 0.6, etape[\"nom\"], ha='center', va='top', fontsize=11, fontweight='bold')\n",
    "    ax.text(x_pos, y_pos - 0.85, etape[\"desc\"], ha='center', va='top', fontsize=9, style='italic')\n",
    "    \n",
    "    # Fl√®che vers prochaine √©tape\n",
    "    if i < len(etapes) - 1:\n",
    "        ax.arrow(x_pos + 0.3, y_pos, x_spacing - 0.6, 0, \n",
    "                head_width=0.1, head_length=0.15, fc='gray', ec='gray', zorder=2)\n",
    "\n",
    "# Titre narratif\n",
    "ax.text(5, 5.5, \"üéØ PROGRESSION DU PIPELINE E1\", ha='center', va='center', \n",
    "        fontsize=16, fontweight='bold', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# L√©gende\n",
    "legend_elements = [\n",
    "    mpatches.Patch(facecolor='#4ECDC4', label='Termin√©'),\n",
    "    mpatches.Patch(facecolor='#FECA57', label='En cours'),\n",
    "    mpatches.Patch(facecolor='#E8E8E8', label='√Ä venir')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper left', fontsize=10)\n",
    "\n",
    "# Statistiques rapides (si disponibles)\n",
    "stats_text = \"\\nüìä SNAPSHOT ACTUEL :\\n\"\n",
    "try:\n",
    "    # Essayer de charger des stats si base disponible\n",
    "    stats_text += \"   ‚Ä¢ Pipeline en cours d'ex√©cution...\\n\"\n",
    "except:\n",
    "    stats_text += \"   ‚Ä¢ D√©marrage du pipeline...\\n\"\n",
    "\n",
    "ax.text(5, 1.5, stats_text, ha='center', va='center', fontsize=10,\n",
    "        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n",
    "\n",
    "plt.title(\"üé¨ FIL D'ARIANE VISUEL - Accompagnement narratif du jury\", \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Le fil d'Ariane vous guide √©tape par √©tape √† travers le pipeline\")\n",
    "print(\"   Chaque visualisation s'inscrit dans cette progression narrative\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notes:\n",
    "> - G√©n√®re un manifest JSON (tra√ßabilit√©: version, timestamp, sources).\n",
    "> - Met √† jour `README_VERSIONNING.md` pour garder l‚Äôhistorique.\n",
    "> - √Ä adapter selon les sources r√©ellement activ√©es (OWM, RSS, NewsAPI, GDELT‚Ä¶).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataSens E1_v2 - 05_snapshot_and_readme\n",
    "# üì¶ Manifest + Snapshot + Versioning + Bilan final\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import UTC, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# R√©cup√©rer variables\n",
    "if 'PROJECT_ROOT' not in globals():\n",
    "    current = Path.cwd()\n",
    "    PROJECT_ROOT = None\n",
    "    while current != current.parent:\n",
    "        if (current / \"notebooks\").exists() and (current / \"docs\").exists():\n",
    "            PROJECT_ROOT = current\n",
    "            break\n",
    "        current = current.parent\n",
    "    else:\n",
    "        PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "if 'PG_URL' not in globals():\n",
    "    PG_URL = os.getenv(\"DATASENS_PG_URL\", \"postgresql+psycopg2://postgres:postgres@localhost:5433/postgres\")\n",
    "\n",
    "engine = create_engine(PG_URL, future=True)\n",
    "RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "VERSIONS_DIR = PROJECT_ROOT / \"data\" / \"raw\" / \"manifests\"\n",
    "VERSION_FILE = PROJECT_ROOT / \"README_VERSIONNING.md\"\n",
    "\n",
    "print(\"üì¶ FINALISATION E1_V2\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================\n",
    "# 1. MANIFEST JSON (Tra√ßabilit√© compl√®te)\n",
    "# ============================================================\n",
    "print(\"\\nüìÑ 1. GENERATION MANIFEST JSON\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # Statistiques compl√®tes\n",
    "    stats_sources = pd.read_sql_query(\"\"\"\n",
    "        SELECT \n",
    "            s.nom AS source,\n",
    "            COUNT(DISTINCT f.id_flux) AS nb_flux,\n",
    "            COUNT(DISTINCT d.id_doc) AS nb_documents\n",
    "        FROM source s\n",
    "        LEFT JOIN flux f ON s.id_source = f.id_source\n",
    "        LEFT JOIN document d ON f.id_flux = d.id_flux\n",
    "        GROUP BY s.nom\n",
    "        ORDER BY nb_documents DESC\n",
    "    \"\"\", conn)\n",
    "\n",
    "VERSIONS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "manifest = {\n",
    "    \"run_id\": datetime.now(UTC).strftime(\"%Y%m%dT%H%M%SZ\"),\n",
    "    \"notebook_version\": \"E1_v2\",\n",
    "    \"created_utc\": datetime.now(UTC).isoformat(),\n",
    "    \"sources_collected\": stats_sources[\"source\"].tolist(),\n",
    "    \"statistics\": {\n",
    "        \"total_documents\": int(stats_sources[\"nb_documents\"].sum()),\n",
    "        \"total_flux\": int(stats_sources[\"nb_flux\"].sum()),\n",
    "        \"sources_count\": len(stats_sources)\n",
    "    },\n",
    "    \"pg_db\": PG_URL.split(\"/\")[-1] if \"/\" in PG_URL else \"postgres\",\n",
    "    \"minio_bucket\": os.getenv(\"MINIO_BUCKET\", \"datasens-raw\")\n",
    "}\n",
    "\n",
    "manifest_path = VERSIONS_DIR / f\"manifest_{manifest['run_id']}.json\"\n",
    "manifest_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"‚úÖ Manifest cr√©√© : {manifest_path}\")\n",
    "print(f\"\\nüìä Contenu du manifest :\")\n",
    "print(json.dumps(manifest, indent=2, ensure_ascii=False))\n",
    "\n",
    "# Afficher le manifest comme DataFrame\n",
    "df_manifest = pd.DataFrame([manifest])\n",
    "print(\"\\nüìã Manifest (format tableau) :\")\n",
    "display(pd.DataFrame([{\n",
    "    \"Run ID\": manifest[\"run_id\"],\n",
    "    \"Version\": manifest[\"notebook_version\"],\n",
    "    \"Total Documents\": manifest[\"statistics\"][\"total_documents\"],\n",
    "    \"Total Flux\": manifest[\"statistics\"][\"total_flux\"],\n",
    "    \"Sources\": len(manifest[\"sources_collected\"])\n",
    "}]))\n",
    "\n",
    "# ============================================================\n",
    "# 2. SNAPSHOT POSTGRESQL (Optionnel - instruction manuelle)\n",
    "# ============================================================\n",
    "print(\"\\nüíæ 2. SNAPSHOT POSTGRESQL\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"üí° Pour cr√©er un snapshot PostgreSQL, ex√©cutez dans le terminal :\")\n",
    "print(f\"   docker exec datasens_pg pg_dump -U postgres postgres > data/raw/manifests/pg_snapshot_{manifest['run_id']}.sql\")\n",
    "print(\"\\n   Ou via SQLAlchemy (export CSV des tables principales) :\")\n",
    "\n",
    "# Export CSV des tables principales pour backup l√©ger\n",
    "VERSIONS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "snapshot_dir = VERSIONS_DIR / f\"snapshot_{manifest['run_id']}\"\n",
    "snapshot_dir.mkdir(exist_ok=True)\n",
    "\n",
    "tables_to_export = [\"type_donnee\", \"source\", \"flux\", \"document\"]\n",
    "for table in tables_to_export:\n",
    "    try:\n",
    "        df_snap = pd.read_sql_query(f\"SELECT * FROM {table}\", engine)\n",
    "        snap_file = snapshot_dir / f\"{table}.csv\"\n",
    "        df_snap.to_csv(snap_file, index=False)\n",
    "        print(f\"   ‚úÖ {table}: {len(df_snap)} lignes ‚Üí {snap_file.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è {table}: {e}\")\n",
    "\n",
    "print(f\"\\nüìÇ Snapshot CSV : {snapshot_dir}\")\n",
    "\n",
    "# ============================================================\n",
    "# 3. VERSIONING\n",
    "# ============================================================\n",
    "print(\"\\nüìò 3. VERSIONING\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "VERSION_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "entry = f\"- **{datetime.now(UTC).strftime('%Y-%m-%d %H:%M:%S UTC')}** | `E1_V2_COMPLETE` | Collecte r√©elle {manifest['statistics']['total_documents']} documents, {manifest['statistics']['sources_count']} sources actives\\n\"\n",
    "\n",
    "with open(VERSION_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(entry)\n",
    "\n",
    "print(f\"‚úÖ Versionning mis √† jour : {VERSION_FILE}\")\n",
    "\n",
    "# Afficher les derni√®res entr√©es\n",
    "if VERSION_FILE.exists():\n",
    "    print(\"\\nüìã Derni√®res 5 entr√©es de l'historique :\")\n",
    "    with open(VERSION_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[-5:]:\n",
    "            print(f\"   {line.strip()}\")\n",
    "\n",
    "# ============================================================\n",
    "# 4. BILAN FINAL AVEC VISUALISATIONS\n",
    "# ============================================================\n",
    "print(\"\\nüìä 4. BILAN FINAL E1_V2\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # Vue compl√®te cha√Æne de tra√ßabilit√©\n",
    "    df_chain = pd.read_sql_query(\"\"\"\n",
    "        SELECT\n",
    "            td.libelle AS type_donnee,\n",
    "            s.nom AS source,\n",
    "            COUNT(DISTINCT f.id_flux) AS nb_flux,\n",
    "            COUNT(DISTINCT d.id_doc) AS nb_documents,\n",
    "            ROUND(s.fiabilite * 100, 1) AS fiabilite_pct\n",
    "        FROM type_donnee td\n",
    "        LEFT JOIN source s ON s.id_type_donnee = td.id_type_donnee\n",
    "        LEFT JOIN flux f ON f.id_source = s.id_source\n",
    "        LEFT JOIN document d ON d.id_flux = f.id_flux\n",
    "        GROUP BY td.libelle, s.nom, s.fiabilite\n",
    "        ORDER BY nb_documents DESC\n",
    "    \"\"\", conn)\n",
    "\n",
    "print(\"\\nüìã Vue compl√®te cha√Æne de tra√ßabilit√© (Type ‚Üí Source ‚Üí Flux ‚Üí Document) :\")\n",
    "display(df_chain)\n",
    "\n",
    "# Graphique final\n",
    "if len(df_chain) > 0:\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    bars = plt.barh(df_chain[\"source\"], df_chain[\"nb_documents\"], color=plt.cm.Set3(range(len(df_chain))))\n",
    "    for i, (bar, value) in enumerate(zip(bars, df_chain[\"nb_documents\"])):\n",
    "        plt.text(bar.get_width() + max(df_chain[\"nb_documents\"]) * 0.01, bar.get_y() + bar.get_height()/2,\n",
    "                f\"{int(value):,}\", ha='left', va='center', fontweight='bold', fontsize=9)\n",
    "    plt.title(\"üìä Documents collect√©s par source\", fontsize=12, fontweight='bold')\n",
    "    plt.xlabel(\"Nombre de documents\", fontsize=11)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    if len(df_chain[\"type_donnee\"].unique()) > 0:\n",
    "        type_counts = df_chain.groupby(\"type_donnee\")[\"nb_documents\"].sum()\n",
    "        plt.pie(type_counts.values, labels=type_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "        plt.title(\"üìä R√©partition par type de donn√©e\", fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Tableau r√©capitulatif final\n",
    "print(\"\\nüìã Tableau r√©capitulatif final :\")\n",
    "display(df_chain)\n",
    "\n",
    "print(f\"\\n‚úÖ E1_V2 TERMINE :\")\n",
    "print(f\"   ‚Ä¢ {manifest['statistics']['total_documents']:,} documents collect√©s\")\n",
    "print(f\"   ‚Ä¢ {manifest['statistics']['total_flux']} flux de collecte\")\n",
    "print(f\"   ‚Ä¢ {manifest['statistics']['sources_count']} sources actives\")\n",
    "print(f\"   ‚Ä¢ Manifest : {manifest_path.name}\")\n",
    "print(f\"\\nüéØ Pr√™t pour E2 (Enrichissement IA) !\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EXPORT DATASET STRUCTUR√â POUR IA (Parquet/CSV)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üì¶ EXPORT DATASET STRUCTUR√â POUR T√âL√âCHARGEMENT (Jury)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Cr√©er le dossier export\n",
    "export_dir = PROJECT_ROOT / \"data\" / \"gold\" / \"dataset_ia\"\n",
    "export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now(UTC).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Requ√™te consolid√©e : Documents + m√©tadonn√©es pr√™tes pour IA\n",
    "dataset_query = \"\"\"\n",
    "    SELECT \n",
    "        d.id_doc,\n",
    "        d.titre,\n",
    "        d.texte,\n",
    "        d.langue,\n",
    "        d.date_publication,\n",
    "        d.hash_fingerprint,\n",
    "        td.libelle AS type_donnee,\n",
    "        s.nom AS source_nom,\n",
    "        f.date_collecte,\n",
    "        f.format AS flux_format,\n",
    "        t.ville AS territoire\n",
    "    FROM document d\n",
    "    LEFT JOIN flux f ON d.id_flux = f.id_flux\n",
    "    LEFT JOIN source s ON f.id_source = s.id_source\n",
    "    LEFT JOIN type_donnee td ON s.id_type_donnee = td.id_type_donnee\n",
    "    LEFT JOIN territoire t ON d.id_territoire = t.id_territoire\n",
    "    ORDER BY d.date_publication DESC\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    df_dataset = pd.read_sql_query(dataset_query, conn)\n",
    "\n",
    "if len(df_dataset) > 0:\n",
    "    # Export CSV (compatible universel)\n",
    "    csv_path = export_dir / f\"datasens_dataset_v2_{timestamp}.csv\"\n",
    "    df_dataset.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "    csv_size_mb = csv_path.stat().st_size / (1024 * 1024)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset v2 export√© :\")\n",
    "    print(f\"   üìÑ CSV : {csv_path.name}\")\n",
    "    print(f\"   üìä {len(df_dataset):,} documents\")\n",
    "    print(f\"   üíæ Taille : {csv_size_mb:.2f} MB\")\n",
    "    print(f\"   üìÅ Chemin : {csv_path}\")\n",
    "    \n",
    "    # Export Parquet si disponible (format optimal pour IA)\n",
    "    try:\n",
    "        import pyarrow as pa\n",
    "        import pyarrow.parquet as pq\n",
    "        \n",
    "        parquet_path = export_dir / f\"datasens_dataset_v2_{timestamp}.parquet\"\n",
    "        df_dataset.to_parquet(parquet_path, engine='pyarrow', compression='snappy', index=False)\n",
    "        parquet_size_mb = parquet_path.stat().st_size / (1024 * 1024)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Export Parquet (format optimal) :\")\n",
    "        print(f\"   üìÑ Parquet : {parquet_path.name}\")\n",
    "        print(f\"   üíæ Taille : {parquet_size_mb:.2f} MB\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"\\n‚ö†Ô∏è PyArrow non install√© - export Parquet ignor√©\")\n",
    "        print(\"   üí° Installez : pip install pyarrow\")\n",
    "    \n",
    "    # Aper√ßu du dataset\n",
    "    print(\"\\nüìã Aper√ßu dataset (5 premiers documents) :\")\n",
    "    display(df_dataset.head())\n",
    "    \n",
    "    # Visualisation statistiques\n",
    "    print(\"\\nüìä Statistiques dataset :\")\n",
    "    stats_data = {\n",
    "        'Total documents': [len(df_dataset)],\n",
    "        'Langues': [df_dataset['langue'].value_counts().to_dict()],\n",
    "        'Sources (top 5)': [df_dataset['source_nom'].value_counts().head(5).to_dict()]\n",
    "    }\n",
    "    df_stats = pd.DataFrame(stats_data)\n",
    "    display(df_stats)\n",
    "    \n",
    "    # Graphique distribution par langue\n",
    "    if 'langue' in df_dataset.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        lang_counts = df_dataset['langue'].value_counts()\n",
    "        colors = plt.cm.Set3(range(len(lang_counts)))\n",
    "        bars = plt.bar(lang_counts.index, lang_counts.values, color=colors)\n",
    "        for bar, value in zip(bars, lang_counts.values):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(lang_counts.values) * 0.02,\n",
    "                    f\"{int(value):,}\", ha='center', va='bottom', fontweight='bold')\n",
    "        plt.title(\"üìä Distribution des documents par langue (Dataset v2)\", fontsize=12, fontweight='bold')\n",
    "        plt.ylabel(\"Nombre de documents\", fontsize=11)\n",
    "        plt.xlabel(\"Langue\", fontsize=11)\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Aucun document √† exporter\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ EXPORT DATASET TERMIN√â - PR√äT POUR T√âL√âCHARGEMENT\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nüìã Fichiers disponibles pour le jury :\")\n",
    "print(f\"   ‚Ä¢ CSV : data/gold/dataset_ia/datasens_dataset_v2_{timestamp}.csv\")\n",
    "try:\n",
    "    print(f\"   ‚Ä¢ Parquet : data/gold/dataset_ia/datasens_dataset_v2_{timestamp}.parquet\")\n",
    "except:\n",
    "    pass\n",
    "print(\"\\nüéØ Ce dataset est pr√™t pour pr√©sentation au jury !\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}