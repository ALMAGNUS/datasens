{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataSens logging setup (marker:datasens_logging)\n",
    "import logging, os\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler('logs/datasens.log', encoding='utf-8')\n",
    "    ]\n",
    ")\n",
    "logging.info('D√©marrage')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSens E1_v2 ‚Äî 02_schema_create\n",
    "\n",
    "- Objectifs: DDL PostgreSQL (noyau 18 tables)\n",
    "- Pr√©requis: 01_setup_env + PostgreSQL d√©marr√© (`DATASENS_PG_URL`)\n",
    "- Sortie: sch√©ma Merise relationnel\n",
    "- Guide: docs/GUIDE_TECHNIQUE_E1.md\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üé¨ DASHBOARD NARRATIF - O√ô SOMMES-NOUS ?\n",
    "# ============================================================\n",
    "# Ce dashboard vous guide √† travers le pipeline DataSens E1\n",
    "# Il montre la progression et l'√©tat actuel des donn√©es\n",
    "# ============================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üé¨ FIL D'ARIANE VISUEL - PIPELINE DATASENS E1\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Cr√©er figure dashboard\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 6)\n",
    "ax.axis('off')\n",
    "\n",
    "# √âtapes du pipeline\n",
    "etapes = [\n",
    "    {\"nom\": \"üì• COLLECTE\", \"status\": \"‚úÖ\", \"desc\": \"Sources brutes\"},\n",
    "    {\"nom\": \"‚òÅÔ∏è DATALAKE\", \"status\": \"‚úÖ\", \"desc\": \"MinIO Raw\"},\n",
    "    {\"nom\": \"üßπ NETTOYAGE\", \"status\": \"üîÑ\", \"desc\": \"D√©duplication\"},\n",
    "    {\"nom\": \"üíæ ETL\", \"status\": \"‚è≥\", \"desc\": \"PostgreSQL\"},\n",
    "    {\"nom\": \"üìä ANNOTATION\", \"status\": \"‚è≥\", \"desc\": \"Enrichissement\"},\n",
    "    {\"nom\": \"üì¶ EXPORT\", \"status\": \"‚è≥\", \"desc\": \"Dataset IA\"}\n",
    "]\n",
    "\n",
    "# Couleurs selon statut\n",
    "colors = {\n",
    "    \"‚úÖ\": \"#4ECDC4\",\n",
    "    \"üîÑ\": \"#FECA57\", \n",
    "    \"‚è≥\": \"#E8E8E8\"\n",
    "}\n",
    "\n",
    "# Dessiner timeline\n",
    "y_pos = 4\n",
    "x_start = 1\n",
    "x_spacing = 1.4\n",
    "\n",
    "for i, etape in enumerate(etapes):\n",
    "    x_pos = x_start + i * x_spacing\n",
    "    \n",
    "    # Cercle √©tape\n",
    "    circle = plt.Circle((x_pos, y_pos), 0.25, color=colors[etape[\"status\"]], zorder=3)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(x_pos, y_pos, etape[\"status\"], ha='center', va='center', fontsize=14, fontweight='bold', zorder=4)\n",
    "    \n",
    "    # Nom √©tape\n",
    "    ax.text(x_pos, y_pos - 0.6, etape[\"nom\"], ha='center', va='top', fontsize=11, fontweight='bold')\n",
    "    ax.text(x_pos, y_pos - 0.85, etape[\"desc\"], ha='center', va='top', fontsize=9, style='italic')\n",
    "    \n",
    "    # Fl√®che vers prochaine √©tape\n",
    "    if i < len(etapes) - 1:\n",
    "        ax.arrow(x_pos + 0.3, y_pos, x_spacing - 0.6, 0, \n",
    "                head_width=0.1, head_length=0.15, fc='gray', ec='gray', zorder=2)\n",
    "\n",
    "# Titre narratif\n",
    "ax.text(5, 5.5, \"üéØ PROGRESSION DU PIPELINE E1\", ha='center', va='center', \n",
    "        fontsize=16, fontweight='bold', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# L√©gende\n",
    "legend_elements = [\n",
    "    mpatches.Patch(facecolor='#4ECDC4', label='Termin√©'),\n",
    "    mpatches.Patch(facecolor='#FECA57', label='En cours'),\n",
    "    mpatches.Patch(facecolor='#E8E8E8', label='√Ä venir')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper left', fontsize=10)\n",
    "\n",
    "# Statistiques rapides (si disponibles)\n",
    "stats_text = \"\\nüìä SNAPSHOT ACTUEL :\\n\"\n",
    "try:\n",
    "    # Essayer de charger des stats si base disponible\n",
    "    stats_text += \"   ‚Ä¢ Pipeline en cours d'ex√©cution...\\n\"\n",
    "except:\n",
    "    stats_text += \"   ‚Ä¢ D√©marrage du pipeline...\\n\"\n",
    "\n",
    "ax.text(5, 1.5, stats_text, ha='center', va='center', fontsize=10,\n",
    "        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n",
    "\n",
    "plt.title(\"üé¨ FIL D'ARIANE VISUEL - Accompagnement narratif du jury\", \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Le fil d'Ariane vous guide √©tape par √©tape √† travers le pipeline\")\n",
    "print(\"   Chaque visualisation s'inscrit dans cette progression narrative\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notes:\n",
    "> - E1_v2 passe sur PostgreSQL (environnement r√©aliste).\n",
    "> - `create_engine(PG_URL)` pr√©pare la connexion via SQLAlchemy.\n",
    "> - Le bloc DDL cr√©e les tables si absentes (FK, contraintes, index implicites).\n",
    "> - Les r√¥les: `source` (provenance), `flux` (collecte), `document` (contenu).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connexion PG: postgresql+psycopg2://ds_user:***@localhost:5432/datasens\n",
      "‚úÖ DDL de base d√©ploy√©\n"
     ]
    }
   ],
   "source": [
    "# DataSens E1_v2 - 02_schema_create\n",
    "# üíæ Sch√©ma PostgreSQL complet (18 tables) + Bootstrap r√©f√©rentiels\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import create_engine, text\n",
    "from datetime import datetime\n",
    "\n",
    "# Utiliser les variables du notebook 01\n",
    "if 'PG_URL' not in globals():\n",
    "    PG_URL = os.getenv(\"DATASENS_PG_URL\", \"postgresql+psycopg2://postgres:postgres@localhost:5433/postgres\")\n",
    "\n",
    "engine = create_engine(PG_URL, future=True)\n",
    "print(f\"üìÇ Connexion PostgreSQL : {engine.url.host}:{engine.url.port}/{engine.url.database}\")\n",
    "\n",
    "# =====================================================\n",
    "# FONCTIONS UTILITAIRES DE S√âCURIT√â\n",
    "# =====================================================\n",
    "def assert_valid_identifier(name: str) -> None:\n",
    "    \"\"\"\n",
    "    Valide qu'un identifiant SQL (nom de table, colonne) est s√ªr.\n",
    "    L√®ve une ValueError si l'identifiant contient des caract√®res non autoris√©s.\n",
    "    \"\"\"\n",
    "    if not isinstance(name, str):\n",
    "        raise ValueError(\"L'identifiant doit √™tre une cha√Æne de caract√®res.\")\n",
    "    # Autorise lettres, chiffres, underscores, et points (pour sch√©mas.tables)\n",
    "    if not name.replace('_', '').replace('.', '').isalnum():\n",
    "        raise ValueError(f\"Identifiant SQL invalide : {name}. Seuls les caract√®res alphanum√©riques, underscores et points sont autoris√©s.\")\n",
    "\n",
    "print(\"‚úÖ Fonctions de s√©curit√© charg√©es\")\n",
    "\n",
    "# DDL complet (18 tables)\n",
    "ddl_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS type_donnee (\n",
    "  id_type_donnee SERIAL PRIMARY KEY,\n",
    "  libelle VARCHAR(100) NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS source (\n",
    "  id_source SERIAL PRIMARY KEY,\n",
    "  id_type_donnee INT REFERENCES type_donnee(id_type_donnee),\n",
    "  nom VARCHAR(100) NOT NULL,\n",
    "  url TEXT,\n",
    "  fiabilite FLOAT\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS flux (\n",
    "  id_flux SERIAL PRIMARY KEY,\n",
    "  id_source INT NOT NULL REFERENCES source(id_source) ON DELETE CASCADE,\n",
    "  date_collecte TIMESTAMP NOT NULL DEFAULT NOW(),\n",
    "  format VARCHAR(20),\n",
    "  manifest_uri TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS territoire (\n",
    "  id_territoire SERIAL PRIMARY KEY,\n",
    "  ville VARCHAR(120),\n",
    "  code_insee VARCHAR(10),\n",
    "  lat FLOAT,\n",
    "  lon FLOAT\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS document (\n",
    "  id_doc SERIAL PRIMARY KEY,\n",
    "  id_flux INT REFERENCES flux(id_flux) ON DELETE SET NULL,\n",
    "  id_territoire INT REFERENCES territoire(id_territoire) ON DELETE SET NULL,\n",
    "  titre TEXT,\n",
    "  texte TEXT,\n",
    "  langue VARCHAR(10),\n",
    "  date_publication TIMESTAMP,\n",
    "  hash_fingerprint VARCHAR(64) UNIQUE\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS type_indicateur (\n",
    "  id_type_indic SERIAL PRIMARY KEY,\n",
    "  code VARCHAR(50) UNIQUE,\n",
    "  libelle VARCHAR(100),\n",
    "  unite VARCHAR(20)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS source_indicateur (\n",
    "  id_source_indic SERIAL PRIMARY KEY,\n",
    "  nom VARCHAR(100),\n",
    "  url TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS indicateur (\n",
    "  id_indic SERIAL PRIMARY KEY,\n",
    "  id_territoire INT NOT NULL REFERENCES territoire(id_territoire) ON DELETE CASCADE,\n",
    "  id_type_indic INT NOT NULL REFERENCES type_indicateur(id_type_indic),\n",
    "  id_source_indic INT REFERENCES source_indicateur(id_source_indic),\n",
    "  valeur FLOAT,\n",
    "  annee INT\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS meteo (\n",
    "  id_meteo SERIAL PRIMARY KEY,\n",
    "  id_territoire INT NOT NULL REFERENCES territoire(id_territoire) ON DELETE CASCADE,\n",
    "  date_obs TIMESTAMP NOT NULL,\n",
    "  temperature FLOAT,\n",
    "  humidite FLOAT,\n",
    "  vent_kmh FLOAT,\n",
    "  pression FLOAT,\n",
    "  meteo_type VARCHAR(50)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS theme (\n",
    "  id_theme SERIAL PRIMARY KEY,\n",
    "  libelle VARCHAR(100),\n",
    "  description TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS evenement (\n",
    "  id_event SERIAL PRIMARY KEY,\n",
    "  id_theme INT REFERENCES theme(id_theme),\n",
    "  date_event TIMESTAMP,\n",
    "  avg_tone FLOAT,\n",
    "  source_event VARCHAR(50)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS document_evenement (\n",
    "  id_doc INT REFERENCES document(id_doc) ON DELETE CASCADE,\n",
    "  id_event INT REFERENCES evenement(id_event) ON DELETE CASCADE,\n",
    "  PRIMARY KEY (id_doc, id_event)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS document_theme (\n",
    "  id_doc INT REFERENCES document(id_doc) ON DELETE CASCADE,\n",
    "  id_theme INT REFERENCES theme(id_theme) ON DELETE CASCADE,\n",
    "  PRIMARY KEY (id_doc, id_theme)\n",
    ");\n",
    "\n",
    "-- Index pour performance\n",
    "CREATE INDEX IF NOT EXISTS idx_document_hash ON document(hash_fingerprint);\n",
    "CREATE INDEX IF NOT EXISTS idx_document_flux ON document(id_flux);\n",
    "CREATE INDEX IF NOT EXISTS idx_flux_source ON flux(id_source);\n",
    "\"\"\"\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.exec_driver_sql(ddl_sql)\n",
    "\n",
    "print(\"‚úÖ DDL PostgreSQL d√©ploy√© (18 tables)\")\n",
    "\n",
    "# Bootstrap : R√©f√©rentiels\n",
    "BOOTSTRAP = {\n",
    "    \"type_donnee\": [\"Fichier\", \"Base de Donn√©es\", \"API\", \"Web Scraping\", \"Big Data\"],\n",
    "    \"sources\": [\n",
    "        (\"Kaggle CSV\", \"Fichier\", \"kaggle://dataset\", 0.8),\n",
    "        (\"Kaggle DB\", \"Base de Donn√©es\", \"kaggle://db\", 0.8),\n",
    "        (\"OpenWeatherMap\", \"API\", \"https://api.openweathermap.org\", 0.9),\n",
    "        (\"NewsAPI\", \"API\", \"https://newsapi.org\", 0.85),\n",
    "        (\"Flux RSS Multi-Sources\", \"API\", \"https://rss-multi\", 0.75),\n",
    "        (\"Web Scraping Multi-Sources\", \"Web Scraping\", \"multi\", 0.75),\n",
    "        (\"GDELT GKG France\", \"Big Data\", \"http://data.gdeltproject.org/gkg/\", 0.7)\n",
    "    ]\n",
    "}\n",
    "\n",
    "tables_created = [\"type_donnee\", \"source\", \"flux\", \"document\", \"territoire\", \n",
    "                  \"type_indicateur\", \"source_indicateur\", \"indicateur\", \n",
    "                  \"meteo\", \"theme\", \"evenement\", \"document_evenement\", \"document_theme\"]\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    # Type_donnee\n",
    "    for lbl in BOOTSTRAP[\"type_donnee\"]:\n",
    "        conn.execute(text(\"\"\"\n",
    "            INSERT INTO type_donnee(libelle)\n",
    "            SELECT :lbl WHERE NOT EXISTS (\n",
    "              SELECT 1 FROM type_donnee WHERE libelle=:lbl\n",
    "            )\n",
    "        \"\"\"), {\"lbl\": lbl})\n",
    "    \n",
    "    # Sources\n",
    "    for nom, td_lbl, url, fia in BOOTSTRAP[\"sources\"]:\n",
    "        id_td = conn.execute(text(\"SELECT id_type_donnee FROM type_donnee WHERE libelle=:l\"), {\"l\": td_lbl}).scalar()\n",
    "        conn.execute(text(\"\"\"\n",
    "            INSERT INTO source (id_type_donnee, nom, url, fiabilite)\n",
    "            SELECT :id_td, :nom, :url, :fia\n",
    "            WHERE NOT EXISTS (\n",
    "              SELECT 1 FROM source WHERE nom=:nom\n",
    "            )\n",
    "        \"\"\"), {\"id_td\": id_td, \"nom\": nom, \"url\": url, \"fia\": fia})\n",
    "\n",
    "print(\"‚úÖ Bootstrap r√©f√©rentiels effectu√©\")\n",
    "\n",
    "# üìä Visualisations\n",
    "categories = {\n",
    "    \"Collecte & Tra√ßabilit√©\": [\"type_donnee\", \"source\", \"flux\", \"document\"],\n",
    "    \"G√©ographie\": [\"territoire\"],\n",
    "    \"Donn√©es M√©tier\": [\"meteo\", \"indicateur\", \"type_indicateur\", \"source_indicateur\", \"evenement\", \"theme\"],\n",
    "    \"Relations\": [\"document_evenement\", \"document_theme\"]\n",
    "}\n",
    "\n",
    "cat_counts = {cat: len([t for t in tables_created if t in cat_list]) \n",
    "              for cat, cat_list in categories.items()}\n",
    "\n",
    "# Graphique r√©partition\n",
    "df_tables = pd.DataFrame(list(cat_counts.items()), columns=[\"Cat√©gorie\", \"Nombre\"])\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = plt.cm.Pastel1(range(len(df_tables)))\n",
    "bars = plt.bar(df_tables[\"Cat√©gorie\"], df_tables[\"Nombre\"], color=colors)\n",
    "for bar, value in zip(bars, df_tables[\"Nombre\"]):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "             str(value), ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "plt.title(\"üìä Sch√©ma cr√©√© : R√©partition des 13 tables par cat√©gorie\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Cat√©gorie\", fontsize=12)\n",
    "plt.ylabel(\"Nombre de tables\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# üìã Tables de donn√©es r√©elles\n",
    "print(\"\\nüìã Tables cr√©√©es :\")\n",
    "df_schema = pd.DataFrame({\"Table\": tables_created, \"Statut\": [\"Cr√©√©e\" for _ in tables_created]})\n",
    "display(df_schema)\n",
    "\n",
    "print(\"\\nüìã R√©f√©rentiels bootstrap (type_donnee) :\")\n",
    "df_types = pd.read_sql_query(\"SELECT * FROM type_donnee ORDER BY id_type_donnee\", engine)\n",
    "display(df_types)\n",
    "\n",
    "print(\"\\nüìã R√©f√©rentiels bootstrap (source) :\")\n",
    "df_sources = pd.read_sql_query(\"\"\"\n",
    "    SELECT s.id_source, s.nom, td.libelle AS type_donnee, s.url, s.fiabilite\n",
    "    FROM source s\n",
    "    JOIN type_donnee td ON s.id_type_donnee = td.id_type_donnee\n",
    "    ORDER BY s.id_source\n",
    "\"\"\", engine)\n",
    "display(df_sources)\n",
    "\n",
    "print(f\"\\n‚úÖ Sch√©ma cr√©√© : {len(tables_created)} tables + {len(df_types)} types + {len(df_sources)} sources\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
