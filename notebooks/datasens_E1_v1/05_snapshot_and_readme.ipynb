{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataSens logging setup (marker:datasens_logging)\n",
    "import logging, os\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler('logs/datasens.log', encoding='utf-8')\n",
    "    ]\n",
    ")\n",
    "logging.info('DÃ©marrage')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§± Table : type_donnee\n",
      "\n",
      "ğŸ§± Table : source\n",
      "\n",
      "ğŸ§± Table : flux\n",
      "\n",
      "ğŸ§± Table : document\n",
      "\n",
      "ğŸ“Š Vue d'ensemble de la chaÃ®ne de sourcing :\n",
      "ğŸ“¦ Snapshot crÃ©Ã© : datasens_v20251101_123610.db\n",
      "\n",
      "ğŸ“˜ Historique des versions DataSens :\n",
      "\n",
      "- **2025-10-30 18:54:51** | `DB_SCHEMA` | CrÃ©ation des 18 tables principales\n",
      "- **2025-10-30 18:54:56** | `DB_SCHEMA` | CrÃ©ation des 18 tables principales\n",
      "- **2025-10-30 18:55:22** | `DB_SCHEMA` | CrÃ©ation des 18 tables principales\n",
      "- **2025-10-30 18:55:46** | `DB_SCHEMA` | CrÃ©ation des 18 tables principales\n",
      "- **2025-10-30 18:56:12** | `DB_SCHEMA` | CrÃ©ation des 18 tables principales\n",
      "- **2025-10-30 18:56:42** | `DB_SEED` | Insertion du jeu de donnÃ©es minimal\n",
      "- **2025-10-30 18:56:47** | `DB_SEED` | Insertion du jeu de donnÃ©es minimal\n",
      "- **2025-10-30 18:57:08** | `DB_SEED` | Insertion du jeu de donnÃ©es minimal\n",
      "- **2025-10-30 18:57:29** | `DB_SEED` | Insertion du jeu de donnÃ©es minimal\n",
      "- **2025-10-30 18:57:50** | `DB_SEED` | Insertion du jeu de donnÃ©es minimal\n",
      "- **2025-10-30 18:59:47** | `DB_BACKUP` | datasens_v20251030_185947.db - AprÃ¨s validation de la chaÃ®ne Type-Source-Flux-Document\n",
      "- **2025-10-30 18:59:53** | `DB_BACKUP` | datasens_v20251030_185953.db - AprÃ¨s validation de la chaÃ®ne Type-Source-Flux-Document\n",
      "- **2025-10-30 19:00:15** | `DB_BACKUP` | datasens_v20251030_190015.db - AprÃ¨s validation de la chaÃ®ne Type-Source-Flux-Document\n",
      "- **2025-10-30 19:00:37** | `DB_BACKUP` | datasens_v20251030_190037.db - AprÃ¨s validation de la chaÃ®ne Type-Source-Flux-Document\n",
      "- **2025-10-30 19:00:58** | `DB_BACKUP` | datasens_v20251030_190058.db - AprÃ¨s validation de la chaÃ®ne Type-Source-Flux-Document\n",
      "- **2025-11-01 12:36:10** | `DB_BACKUP` | datasens_v20251101_123610.db - AprÃ¨s validation de la chaÃ®ne Type-Source-Flux-Document\n",
      "\n",
      "\n",
      "ğŸ DataSens E1 terminÃ© avec succÃ¨s !\n"
     ]
    }
   ],
   "source": [
    "# DataSens E1_v1 â€” 05_snapshot_and_readme\n",
    "# Objectifs: inspection tables, vue join, snapshot versionnÃ©, historique\n",
    "# PrÃ©requis: 04_crud_tests\n",
    "# Sorties: datasens/versions/ + MAJ README_VERSIONNING.md\n",
    "# RÃ©fÃ©rence: docs/GUIDE_TECHNIQUE_E1.md\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parents[2] if (Path.cwd().name.startswith('datasens_')) else Path.cwd()\n",
    "DB_PATH = PROJECT_ROOT / \"datasens\" / \"datasens.db\"\n",
    "VERSION_FILE = PROJECT_ROOT / \"README_VERSIONNING.md\"\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "# Helpers\n",
    "\n",
    "def sql_df(query, params=None):\n",
    "    return pd.read_sql_query(query, conn, params=params)\n",
    "\n",
    "# Inspection tables clÃ©s\n",
    "for t in [\"type_donnee\", \"source\", \"flux\", \"document\"]:\n",
    "    print(f\"\\nğŸ§± Table : {t}\")\n",
    "    try:\n",
    "        display(sql_df(f\"SELECT * FROM {t} LIMIT 10;\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Vue chainÃ©e\n",
    "query_sources = \"\"\"\n",
    "SELECT\n",
    "    td.libelle          AS type_donnee,\n",
    "    s.nom               AS source_nom,\n",
    "    ROUND(s.fiabilite,2) AS source_fiabilite,\n",
    "    f.format            AS flux_format,\n",
    "    f.date_collecte     AS flux_date,\n",
    "    f.manifest_uri      AS flux_manifest,\n",
    "    d.titre             AS doc_titre,\n",
    "    d.date_publication  AS doc_date\n",
    "FROM type_donnee td\n",
    "LEFT JOIN source   s ON s.id_type_donnee = td.id_type_donnee\n",
    "LEFT JOIN flux     f ON f.id_source      = s.id_source\n",
    "LEFT JOIN document d ON d.id_flux        = f.id_flux\n",
    "ORDER BY td.libelle, s.nom, f.date_collecte, d.date_publication;\n",
    "\"\"\"\n",
    "print(\"\\nğŸ“Š Vue d'ensemble de la chaÃ®ne de sourcing :\")\n",
    "try:\n",
    "    display(sql_df(query_sources))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Snapshot versionnÃ©\n",
    "BACKUP_DIR = PROJECT_ROOT / \"datasens\" / \"versions\"\n",
    "BACKUP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_database_version(note=\"Validation\"):\n",
    "    import shutil\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    backup_name = f\"datasens_v{timestamp}.db\"\n",
    "    backup_path = BACKUP_DIR / backup_name\n",
    "    shutil.copy(DB_PATH, backup_path)\n",
    "    with open(VERSION_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"- **{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}** | `DB_BACKUP` | {backup_name} - {note}\\n\")\n",
    "    print(f\"ğŸ“¦ Snapshot crÃ©Ã© : {backup_name}\")\n",
    "\n",
    "save_database_version(\"AprÃ¨s validation de la chaÃ®ne Type-Source-Flux-Document\")\n",
    "\n",
    "# Historique\n",
    "print(\"\\nğŸ“˜ Historique des versions DataSens :\\n\")\n",
    "try:\n",
    "    with open(VERSION_FILE, encoding=\"utf-8\") as f:\n",
    "        print(f.read())\n",
    "except UnicodeDecodeError:\n",
    "    with open(VERSION_FILE, encoding=\"cp1252\") as f:\n",
    "        print(f.read())\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EXPORT DATASET STRUCTURÃ‰ POUR IA (Parquet/CSV)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“¦ EXPORT DATASET STRUCTURÃ‰ POUR TÃ‰LÃ‰CHARGEMENT (Jury)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# CrÃ©er le dossier export\n",
    "export_dir = PROJECT_ROOT / \"data\" / \"gold\" / \"dataset_ia\"\n",
    "export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# RequÃªte consolidÃ©e : Documents + mÃ©tadonnÃ©es prÃªtes pour IA\n",
    "dataset_query = \"\"\"\n",
    "    SELECT \n",
    "        d.id_doc,\n",
    "        d.titre,\n",
    "        d.texte,\n",
    "        d.langue,\n",
    "        d.date_publication,\n",
    "        td.libelle AS type_donnee,\n",
    "        s.nom AS source_nom,\n",
    "        f.date_collecte,\n",
    "        f.format AS flux_format\n",
    "    FROM document d\n",
    "    LEFT JOIN flux f ON d.id_flux = f.id_flux\n",
    "    LEFT JOIN source s ON f.id_source = s.id_source\n",
    "    LEFT JOIN type_donnee td ON s.id_type_donnee = td.id_type_donnee\n",
    "    ORDER BY d.date_publication DESC\n",
    "\"\"\"\n",
    "\n",
    "df_dataset = sql_df(dataset_query)\n",
    "\n",
    "if len(df_dataset) > 0:\n",
    "    # Export CSV (compatible universel)\n",
    "    csv_path = export_dir / f\"datasens_dataset_v1_{timestamp}.csv\"\n",
    "    df_dataset.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "    csv_size_mb = csv_path.stat().st_size / (1024 * 1024)\n",
    "    \n",
    "    print(f\"\\nâœ… Dataset v1 exportÃ© :\")\n",
    "    print(f\"   ğŸ“„ CSV : {csv_path.name}\")\n",
    "    print(f\"   ğŸ“Š {len(df_dataset):,} documents\")\n",
    "    print(f\"   ğŸ’¾ Taille : {csv_size_mb:.2f} MB\")\n",
    "    print(f\"   ğŸ“ Chemin : {csv_path}\")\n",
    "    \n",
    "    # Export Parquet si disponible (format optimal pour IA)\n",
    "    try:\n",
    "        import pyarrow as pa\n",
    "        import pyarrow.parquet as pq\n",
    "        \n",
    "        parquet_path = export_dir / f\"datasens_dataset_v1_{timestamp}.parquet\"\n",
    "        df_dataset.to_parquet(parquet_path, engine='pyarrow', compression='snappy', index=False)\n",
    "        parquet_size_mb = parquet_path.stat().st_size / (1024 * 1024)\n",
    "        \n",
    "        print(f\"\\nâœ… Export Parquet (format optimal) :\")\n",
    "        print(f\"   ğŸ“„ Parquet : {parquet_path.name}\")\n",
    "        print(f\"   ğŸ’¾ Taille : {parquet_size_mb:.2f} MB\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"\\nâš ï¸ PyArrow non installÃ© - export Parquet ignorÃ©\")\n",
    "        print(\"   ğŸ’¡ Installez : pip install pyarrow\")\n",
    "    \n",
    "    # AperÃ§u du dataset\n",
    "    print(\"\\nğŸ“‹ AperÃ§u dataset (5 premiers documents) :\")\n",
    "    display(df_dataset.head())\n",
    "    \n",
    "    # Statistiques\n",
    "    print(\"\\nğŸ“Š Statistiques dataset :\")\n",
    "    print(f\"   â€¢ Total documents : {len(df_dataset):,}\")\n",
    "    print(f\"   â€¢ Par langue :\")\n",
    "    lang_stats = df_dataset['langue'].value_counts()\n",
    "    for lang, count in lang_stats.items():\n",
    "        print(f\"      - {lang}: {count:,}\")\n",
    "    print(f\"   â€¢ Par source :\")\n",
    "    source_stats = df_dataset['source_nom'].value_counts().head(5)\n",
    "    for source, count in source_stats.items():\n",
    "        print(f\"      - {source}: {count:,}\")\n",
    "        \n",
    "else:\n",
    "    print(\"âš ï¸ Aucun document Ã  exporter\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… EXPORT DATASET TERMINÃ‰ - PRÃŠT POUR TÃ‰LÃ‰CHARGEMENT\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nğŸ“‹ Fichiers disponibles pour le jury :\")\n",
    "print(f\"   â€¢ CSV : data/gold/dataset_ia/datasens_dataset_v1_{timestamp}.csv\")\n",
    "try:\n",
    "    print(f\"   â€¢ Parquet : data/gold/dataset_ia/datasens_dataset_v1_{timestamp}.parquet\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "print(\"\\nğŸ DataSens E1 terminÃ© avec succÃ¨s !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notes:\n",
    "> - AperÃ§u des tables clÃ©s (`type_donnee`, `source`, `flux`, `document`).\n",
    "> - La vue joinÃ©e illustre la traÃ§abilitÃ©: Type â†’ Source â†’ Flux â†’ Document.\n",
    "> - `save_database_version` crÃ©e un snapshot horodatÃ© de la base.\n",
    "> - Affichage de lâ€™historique complet pour audit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ğŸ¬ DASHBOARD NARRATIF - OÃ™ SOMMES-NOUS ?\n",
    "# ============================================================\n",
    "# Ce dashboard vous guide Ã  travers le pipeline DataSens E1\n",
    "# Il montre la progression et l'Ã©tat actuel des donnÃ©es\n",
    "# ============================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ¬ FIL D'ARIANE VISUEL - PIPELINE DATASENS E1\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# CrÃ©er figure dashboard\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 6)\n",
    "ax.axis('off')\n",
    "\n",
    "# Ã‰tapes du pipeline\n",
    "etapes = [\n",
    "    {\"nom\": \"ğŸ“¥ COLLECTE\", \"status\": \"âœ…\", \"desc\": \"Sources brutes\"},\n",
    "    {\"nom\": \"â˜ï¸ DATALAKE\", \"status\": \"âœ…\", \"desc\": \"MinIO Raw\"},\n",
    "    {\"nom\": \"ğŸ§¹ NETTOYAGE\", \"status\": \"ğŸ”„\", \"desc\": \"DÃ©duplication\"},\n",
    "    {\"nom\": \"ğŸ’¾ ETL\", \"status\": \"â³\", \"desc\": \"PostgreSQL\"},\n",
    "    {\"nom\": \"ğŸ“Š ANNOTATION\", \"status\": \"â³\", \"desc\": \"Enrichissement\"},\n",
    "    {\"nom\": \"ğŸ“¦ EXPORT\", \"status\": \"â³\", \"desc\": \"Dataset IA\"}\n",
    "]\n",
    "\n",
    "# Couleurs selon statut\n",
    "colors = {\n",
    "    \"âœ…\": \"#4ECDC4\",\n",
    "    \"ğŸ”„\": \"#FECA57\", \n",
    "    \"â³\": \"#E8E8E8\"\n",
    "}\n",
    "\n",
    "# Dessiner timeline\n",
    "y_pos = 4\n",
    "x_start = 1\n",
    "x_spacing = 1.4\n",
    "\n",
    "for i, etape in enumerate(etapes):\n",
    "    x_pos = x_start + i * x_spacing\n",
    "    \n",
    "    # Cercle Ã©tape\n",
    "    circle = plt.Circle((x_pos, y_pos), 0.25, color=colors[etape[\"status\"]], zorder=3)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(x_pos, y_pos, etape[\"status\"], ha='center', va='center', fontsize=14, fontweight='bold', zorder=4)\n",
    "    \n",
    "    # Nom Ã©tape\n",
    "    ax.text(x_pos, y_pos - 0.6, etape[\"nom\"], ha='center', va='top', fontsize=11, fontweight='bold')\n",
    "    ax.text(x_pos, y_pos - 0.85, etape[\"desc\"], ha='center', va='top', fontsize=9, style='italic')\n",
    "    \n",
    "    # FlÃ¨che vers prochaine Ã©tape\n",
    "    if i < len(etapes) - 1:\n",
    "        ax.arrow(x_pos + 0.3, y_pos, x_spacing - 0.6, 0, \n",
    "                head_width=0.1, head_length=0.15, fc='gray', ec='gray', zorder=2)\n",
    "\n",
    "# Titre narratif\n",
    "ax.text(5, 5.5, \"ğŸ¯ PROGRESSION DU PIPELINE E1\", ha='center', va='center', \n",
    "        fontsize=16, fontweight='bold', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# LÃ©gende\n",
    "legend_elements = [\n",
    "    mpatches.Patch(facecolor='#4ECDC4', label='TerminÃ©'),\n",
    "    mpatches.Patch(facecolor='#FECA57', label='En cours'),\n",
    "    mpatches.Patch(facecolor='#E8E8E8', label='Ã€ venir')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper left', fontsize=10)\n",
    "\n",
    "# Statistiques rapides (si disponibles)\n",
    "stats_text = \"\\nğŸ“Š SNAPSHOT ACTUEL :\\n\"\n",
    "try:\n",
    "    # Essayer de charger des stats si base disponible\n",
    "    stats_text += \"   â€¢ Pipeline en cours d'exÃ©cution...\\n\"\n",
    "except:\n",
    "    stats_text += \"   â€¢ DÃ©marrage du pipeline...\\n\"\n",
    "\n",
    "ax.text(5, 1.5, stats_text, ha='center', va='center', fontsize=10,\n",
    "        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n",
    "\n",
    "plt.title(\"ğŸ¬ FIL D'ARIANE VISUEL - Accompagnement narratif du jury\", \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ Le fil d'Ariane vous guide Ã©tape par Ã©tape Ã  travers le pipeline\")\n",
    "print(\"   Chaque visualisation s'inscrit dans cette progression narrative\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
