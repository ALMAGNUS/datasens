{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧱 Table : type_donnee\n",
      "\n",
      "🧱 Table : source\n",
      "\n",
      "🧱 Table : flux\n",
      "\n",
      "🧱 Table : document\n",
      "\n",
      "📊 Vue d'ensemble de la chaîne de sourcing :\n",
      "📦 Snapshot créé : datasens_v20251101_123610.db\n",
      "\n",
      "📘 Historique des versions DataSens :\n",
      "\n",
      "- **2025-10-30 18:54:51** | `DB_SCHEMA` | Création des 18 tables principales\n",
      "- **2025-10-30 18:54:56** | `DB_SCHEMA` | Création des 18 tables principales\n",
      "- **2025-10-30 18:55:22** | `DB_SCHEMA` | Création des 18 tables principales\n",
      "- **2025-10-30 18:55:46** | `DB_SCHEMA` | Création des 18 tables principales\n",
      "- **2025-10-30 18:56:12** | `DB_SCHEMA` | Création des 18 tables principales\n",
      "- **2025-10-30 18:56:42** | `DB_SEED` | Insertion du jeu de données minimal\n",
      "- **2025-10-30 18:56:47** | `DB_SEED` | Insertion du jeu de données minimal\n",
      "- **2025-10-30 18:57:08** | `DB_SEED` | Insertion du jeu de données minimal\n",
      "- **2025-10-30 18:57:29** | `DB_SEED` | Insertion du jeu de données minimal\n",
      "- **2025-10-30 18:57:50** | `DB_SEED` | Insertion du jeu de données minimal\n",
      "- **2025-10-30 18:59:47** | `DB_BACKUP` | datasens_v20251030_185947.db - Après validation de la chaîne Type-Source-Flux-Document\n",
      "- **2025-10-30 18:59:53** | `DB_BACKUP` | datasens_v20251030_185953.db - Après validation de la chaîne Type-Source-Flux-Document\n",
      "- **2025-10-30 19:00:15** | `DB_BACKUP` | datasens_v20251030_190015.db - Après validation de la chaîne Type-Source-Flux-Document\n",
      "- **2025-10-30 19:00:37** | `DB_BACKUP` | datasens_v20251030_190037.db - Après validation de la chaîne Type-Source-Flux-Document\n",
      "- **2025-10-30 19:00:58** | `DB_BACKUP` | datasens_v20251030_190058.db - Après validation de la chaîne Type-Source-Flux-Document\n",
      "- **2025-11-01 12:36:10** | `DB_BACKUP` | datasens_v20251101_123610.db - Après validation de la chaîne Type-Source-Flux-Document\n",
      "\n",
      "\n",
      "🏁 DataSens E1 terminé avec succès !\n"
     ]
    }
   ],
   "source": [
    "# DataSens E1_v1 — 05_snapshot_and_readme\n",
    "# Objectifs: inspection tables, vue join, snapshot versionné, historique\n",
    "# Prérequis: 04_crud_tests\n",
    "# Sorties: datasens/versions/ + MAJ README_VERSIONNING.md\n",
    "# Référence: docs/GUIDE_TECHNIQUE_E1.md\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parents[2] if (Path.cwd().name.startswith('datasens_')) else Path.cwd()\n",
    "DB_PATH = PROJECT_ROOT / \"datasens\" / \"datasens.db\"\n",
    "VERSION_FILE = PROJECT_ROOT / \"README_VERSIONNING.md\"\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "# Helpers\n",
    "\n",
    "def sql_df(query, params=None):\n",
    "    return pd.read_sql_query(query, conn, params=params)\n",
    "\n",
    "# Inspection tables clés\n",
    "for t in [\"type_donnee\", \"source\", \"flux\", \"document\"]:\n",
    "    print(f\"\\n🧱 Table : {t}\")\n",
    "    try:\n",
    "        display(sql_df(f\"SELECT * FROM {t} LIMIT 10;\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Vue chainée\n",
    "query_sources = \"\"\"\n",
    "SELECT\n",
    "    td.libelle          AS type_donnee,\n",
    "    s.nom               AS source_nom,\n",
    "    ROUND(s.fiabilite,2) AS source_fiabilite,\n",
    "    f.format            AS flux_format,\n",
    "    f.date_collecte     AS flux_date,\n",
    "    f.manifest_uri      AS flux_manifest,\n",
    "    d.titre             AS doc_titre,\n",
    "    d.date_publication  AS doc_date\n",
    "FROM type_donnee td\n",
    "LEFT JOIN source   s ON s.id_type_donnee = td.id_type_donnee\n",
    "LEFT JOIN flux     f ON f.id_source      = s.id_source\n",
    "LEFT JOIN document d ON d.id_flux        = f.id_flux\n",
    "ORDER BY td.libelle, s.nom, f.date_collecte, d.date_publication;\n",
    "\"\"\"\n",
    "print(\"\\n📊 Vue d'ensemble de la chaîne de sourcing :\")\n",
    "try:\n",
    "    display(sql_df(query_sources))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Snapshot versionné\n",
    "BACKUP_DIR = PROJECT_ROOT / \"datasens\" / \"versions\"\n",
    "BACKUP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_database_version(note=\"Validation\"):\n",
    "    import shutil\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    backup_name = f\"datasens_v{timestamp}.db\"\n",
    "    backup_path = BACKUP_DIR / backup_name\n",
    "    shutil.copy(DB_PATH, backup_path)\n",
    "    with open(VERSION_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"- **{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}** | `DB_BACKUP` | {backup_name} - {note}\\n\")\n",
    "    print(f\"📦 Snapshot créé : {backup_name}\")\n",
    "\n",
    "save_database_version(\"Après validation de la chaîne Type-Source-Flux-Document\")\n",
    "\n",
    "# Historique\n",
    "print(\"\\n📘 Historique des versions DataSens :\\n\")\n",
    "try:\n",
    "    with open(VERSION_FILE, encoding=\"utf-8\") as f:\n",
    "        print(f.read())\n",
    "except UnicodeDecodeError:\n",
    "    with open(VERSION_FILE, encoding=\"cp1252\") as f:\n",
    "        print(f.read())\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EXPORT DATASET STRUCTURÉ POUR IA (Parquet/CSV)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"📦 EXPORT DATASET STRUCTURÉ POUR TÉLÉCHARGEMENT (Jury)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Créer le dossier export\n",
    "export_dir = PROJECT_ROOT / \"data\" / \"gold\" / \"dataset_ia\"\n",
    "export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Requête consolidée : Documents + métadonnées prêtes pour IA\n",
    "dataset_query = \"\"\"\n",
    "    SELECT \n",
    "        d.id_doc,\n",
    "        d.titre,\n",
    "        d.texte,\n",
    "        d.langue,\n",
    "        d.date_publication,\n",
    "        td.libelle AS type_donnee,\n",
    "        s.nom AS source_nom,\n",
    "        f.date_collecte,\n",
    "        f.format AS flux_format\n",
    "    FROM document d\n",
    "    LEFT JOIN flux f ON d.id_flux = f.id_flux\n",
    "    LEFT JOIN source s ON f.id_source = s.id_source\n",
    "    LEFT JOIN type_donnee td ON s.id_type_donnee = td.id_type_donnee\n",
    "    ORDER BY d.date_publication DESC\n",
    "\"\"\"\n",
    "\n",
    "df_dataset = sql_df(dataset_query)\n",
    "\n",
    "if len(df_dataset) > 0:\n",
    "    # Export CSV (compatible universel)\n",
    "    csv_path = export_dir / f\"datasens_dataset_v1_{timestamp}.csv\"\n",
    "    df_dataset.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "    csv_size_mb = csv_path.stat().st_size / (1024 * 1024)\n",
    "    \n",
    "    print(f\"\\n✅ Dataset v1 exporté :\")\n",
    "    print(f\"   📄 CSV : {csv_path.name}\")\n",
    "    print(f\"   📊 {len(df_dataset):,} documents\")\n",
    "    print(f\"   💾 Taille : {csv_size_mb:.2f} MB\")\n",
    "    print(f\"   📁 Chemin : {csv_path}\")\n",
    "    \n",
    "    # Export Parquet si disponible (format optimal pour IA)\n",
    "    try:\n",
    "        import pyarrow as pa\n",
    "        import pyarrow.parquet as pq\n",
    "        \n",
    "        parquet_path = export_dir / f\"datasens_dataset_v1_{timestamp}.parquet\"\n",
    "        df_dataset.to_parquet(parquet_path, engine='pyarrow', compression='snappy', index=False)\n",
    "        parquet_size_mb = parquet_path.stat().st_size / (1024 * 1024)\n",
    "        \n",
    "        print(f\"\\n✅ Export Parquet (format optimal) :\")\n",
    "        print(f\"   📄 Parquet : {parquet_path.name}\")\n",
    "        print(f\"   💾 Taille : {parquet_size_mb:.2f} MB\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"\\n⚠️ PyArrow non installé - export Parquet ignoré\")\n",
    "        print(\"   💡 Installez : pip install pyarrow\")\n",
    "    \n",
    "    # Aperçu du dataset\n",
    "    print(\"\\n📋 Aperçu dataset (5 premiers documents) :\")\n",
    "    display(df_dataset.head())\n",
    "    \n",
    "    # Statistiques\n",
    "    print(\"\\n📊 Statistiques dataset :\")\n",
    "    print(f\"   • Total documents : {len(df_dataset):,}\")\n",
    "    print(f\"   • Par langue :\")\n",
    "    lang_stats = df_dataset['langue'].value_counts()\n",
    "    for lang, count in lang_stats.items():\n",
    "        print(f\"      - {lang}: {count:,}\")\n",
    "    print(f\"   • Par source :\")\n",
    "    source_stats = df_dataset['source_nom'].value_counts().head(5)\n",
    "    for source, count in source_stats.items():\n",
    "        print(f\"      - {source}: {count:,}\")\n",
    "        \n",
    "else:\n",
    "    print(\"⚠️ Aucun document à exporter\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ EXPORT DATASET TERMINÉ - PRÊT POUR TÉLÉCHARGEMENT\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n📋 Fichiers disponibles pour le jury :\")\n",
    "print(f\"   • CSV : data/gold/dataset_ia/datasens_dataset_v1_{timestamp}.csv\")\n",
    "try:\n",
    "    print(f\"   • Parquet : data/gold/dataset_ia/datasens_dataset_v1_{timestamp}.parquet\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "print(\"\\n🏁 DataSens E1 terminé avec succès !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notes:\n",
    "> - Aperçu des tables clés (`type_donnee`, `source`, `flux`, `document`).\n",
    "> - La vue joinée illustre la traçabilité: Type → Source → Flux → Document.\n",
    "> - `save_database_version` crée un snapshot horodaté de la base.\n",
    "> - Affichage de l’historique complet pour audit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 🎬 DASHBOARD NARRATIF - OÙ SOMMES-NOUS ?\n",
    "# ============================================================\n",
    "# Ce dashboard vous guide à travers le pipeline DataSens E1\n",
    "# Il montre la progression et l'état actuel des données\n",
    "# ============================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎬 FIL D'ARIANE VISUEL - PIPELINE DATASENS E1\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Créer figure dashboard\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 6)\n",
    "ax.axis('off')\n",
    "\n",
    "# Étapes du pipeline\n",
    "etapes = [\n",
    "    {\"nom\": \"📥 COLLECTE\", \"status\": \"✅\", \"desc\": \"Sources brutes\"},\n",
    "    {\"nom\": \"☁️ DATALAKE\", \"status\": \"✅\", \"desc\": \"MinIO Raw\"},\n",
    "    {\"nom\": \"🧹 NETTOYAGE\", \"status\": \"🔄\", \"desc\": \"Déduplication\"},\n",
    "    {\"nom\": \"💾 ETL\", \"status\": \"⏳\", \"desc\": \"PostgreSQL\"},\n",
    "    {\"nom\": \"📊 ANNOTATION\", \"status\": \"⏳\", \"desc\": \"Enrichissement\"},\n",
    "    {\"nom\": \"📦 EXPORT\", \"status\": \"⏳\", \"desc\": \"Dataset IA\"}\n",
    "]\n",
    "\n",
    "# Couleurs selon statut\n",
    "colors = {\n",
    "    \"✅\": \"#4ECDC4\",\n",
    "    \"🔄\": \"#FECA57\", \n",
    "    \"⏳\": \"#E8E8E8\"\n",
    "}\n",
    "\n",
    "# Dessiner timeline\n",
    "y_pos = 4\n",
    "x_start = 1\n",
    "x_spacing = 1.4\n",
    "\n",
    "for i, etape in enumerate(etapes):\n",
    "    x_pos = x_start + i * x_spacing\n",
    "    \n",
    "    # Cercle étape\n",
    "    circle = plt.Circle((x_pos, y_pos), 0.25, color=colors[etape[\"status\"]], zorder=3)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(x_pos, y_pos, etape[\"status\"], ha='center', va='center', fontsize=14, fontweight='bold', zorder=4)\n",
    "    \n",
    "    # Nom étape\n",
    "    ax.text(x_pos, y_pos - 0.6, etape[\"nom\"], ha='center', va='top', fontsize=11, fontweight='bold')\n",
    "    ax.text(x_pos, y_pos - 0.85, etape[\"desc\"], ha='center', va='top', fontsize=9, style='italic')\n",
    "    \n",
    "    # Flèche vers prochaine étape\n",
    "    if i < len(etapes) - 1:\n",
    "        ax.arrow(x_pos + 0.3, y_pos, x_spacing - 0.6, 0, \n",
    "                head_width=0.1, head_length=0.15, fc='gray', ec='gray', zorder=2)\n",
    "\n",
    "# Titre narratif\n",
    "ax.text(5, 5.5, \"🎯 PROGRESSION DU PIPELINE E1\", ha='center', va='center', \n",
    "        fontsize=16, fontweight='bold', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Légende\n",
    "legend_elements = [\n",
    "    mpatches.Patch(facecolor='#4ECDC4', label='Terminé'),\n",
    "    mpatches.Patch(facecolor='#FECA57', label='En cours'),\n",
    "    mpatches.Patch(facecolor='#E8E8E8', label='À venir')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper left', fontsize=10)\n",
    "\n",
    "# Statistiques rapides (si disponibles)\n",
    "stats_text = \"\\n📊 SNAPSHOT ACTUEL :\\n\"\n",
    "try:\n",
    "    # Essayer de charger des stats si base disponible\n",
    "    stats_text += \"   • Pipeline en cours d'exécution...\\n\"\n",
    "except:\n",
    "    stats_text += \"   • Démarrage du pipeline...\\n\"\n",
    "\n",
    "ax.text(5, 1.5, stats_text, ha='center', va='center', fontsize=10,\n",
    "        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n",
    "\n",
    "plt.title(\"🎬 FIL D'ARIANE VISUEL - Accompagnement narratif du jury\", \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 Le fil d'Ariane vous guide étape par étape à travers le pipeline\")\n",
    "print(\"   Chaque visualisation s'inscrit dans cette progression narrative\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}