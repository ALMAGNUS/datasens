{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSens E1_v3 ‚Äî 03_ingest_sources\n",
    "\n",
    "- Objectifs: Collecte r√©elle de **TOUTES les sources** avec stockage hybride (PostgreSQL + MinIO)\n",
    "- Pr√©requis: 01_setup_env + 02_schema_create ex√©cut√©s (36/37 tables cr√©√©es)\n",
    "- Sortie: Donn√©es collect√©es + visualisations + tables r√©elles √† chaque √©tape\n",
    "- Guide: docs/GUIDE_TECHNIQUE_E1.md + docs/datasens_sources_dictionary.md\n",
    "\n",
    "> **E1_v3** : Architecture compl√®te avec **toutes les sources r√©elles**\n",
    "> - Source 1 : Kaggle Dataset (split 50/50 PostgreSQL/MinIO)\n",
    "> - Source 2 : API OpenWeatherMap (m√©t√©o 4+ villes)\n",
    "> - Source 3 : Flux RSS Multi-Sources (Franceinfo + 20 Minutes + Le Monde)\n",
    "> - Source 4 : NewsAPI (optionnel si cl√© API disponible)\n",
    "> - Source 5 : Web Scraping Multi-Sources (6 sources : Reddit, YouTube, SignalConso, Trustpilot, Vie-publique, data.gouv)\n",
    "> - Source 6 : GDELT Big Data (√©chantillon France)\n",
    "> - Sources suppl√©mentaires : Barom√®tres d'opinion (selon docs/datasens_barometer_themes.md)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üé¨ DASHBOARD NARRATIF - O√ô SOMMES-NOUS ?\n",
    "# ============================================================\n",
    "# Ce dashboard vous guide √† travers le pipeline DataSens E1\n",
    "# Il montre la progression et l'√©tat actuel des donn√©es\n",
    "# ============================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üé¨ FIL D'ARIANE VISUEL - PIPELINE DATASENS E1\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Cr√©er figure dashboard\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 6)\n",
    "ax.axis('off')\n",
    "\n",
    "# √âtapes du pipeline\n",
    "etapes = [\n",
    "    {\"nom\": \"üì• COLLECTE\", \"status\": \"‚úÖ\", \"desc\": \"Sources brutes\"},\n",
    "    {\"nom\": \"‚òÅÔ∏è DATALAKE\", \"status\": \"‚úÖ\", \"desc\": \"MinIO Raw\"},\n",
    "    {\"nom\": \"üßπ NETTOYAGE\", \"status\": \"üîÑ\", \"desc\": \"D√©duplication\"},\n",
    "    {\"nom\": \"üíæ ETL\", \"status\": \"‚è≥\", \"desc\": \"PostgreSQL\"},\n",
    "    {\"nom\": \"üìä ANNOTATION\", \"status\": \"‚è≥\", \"desc\": \"Enrichissement\"},\n",
    "    {\"nom\": \"üì¶ EXPORT\", \"status\": \"‚è≥\", \"desc\": \"Dataset IA\"}\n",
    "]\n",
    "\n",
    "# Couleurs selon statut\n",
    "colors = {\n",
    "    \"‚úÖ\": \"#4ECDC4\",\n",
    "    \"üîÑ\": \"#FECA57\", \n",
    "    \"‚è≥\": \"#E8E8E8\"\n",
    "}\n",
    "\n",
    "# Dessiner timeline\n",
    "y_pos = 4\n",
    "x_start = 1\n",
    "x_spacing = 1.4\n",
    "\n",
    "for i, etape in enumerate(etapes):\n",
    "    x_pos = x_start + i * x_spacing\n",
    "    \n",
    "    # Cercle √©tape\n",
    "    circle = plt.Circle((x_pos, y_pos), 0.25, color=colors[etape[\"status\"]], zorder=3)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(x_pos, y_pos, etape[\"status\"], ha='center', va='center', fontsize=14, fontweight='bold', zorder=4)\n",
    "    \n",
    "    # Nom √©tape\n",
    "    ax.text(x_pos, y_pos - 0.6, etape[\"nom\"], ha='center', va='top', fontsize=11, fontweight='bold')\n",
    "    ax.text(x_pos, y_pos - 0.85, etape[\"desc\"], ha='center', va='top', fontsize=9, style='italic')\n",
    "    \n",
    "    # Fl√®che vers prochaine √©tape\n",
    "    if i < len(etapes) - 1:\n",
    "        ax.arrow(x_pos + 0.3, y_pos, x_spacing - 0.6, 0, \n",
    "                head_width=0.1, head_length=0.15, fc='gray', ec='gray', zorder=2)\n",
    "\n",
    "# Titre narratif\n",
    "ax.text(5, 5.5, \"üéØ PROGRESSION DU PIPELINE E1\", ha='center', va='center', \n",
    "        fontsize=16, fontweight='bold', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# L√©gende\n",
    "legend_elements = [\n",
    "    mpatches.Patch(facecolor='#4ECDC4', label='Termin√©'),\n",
    "    mpatches.Patch(facecolor='#FECA57', label='En cours'),\n",
    "    mpatches.Patch(facecolor='#E8E8E8', label='√Ä venir')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper left', fontsize=10)\n",
    "\n",
    "# Statistiques rapides (si disponibles)\n",
    "stats_text = \"\\nüìä SNAPSHOT ACTUEL :\\n\"\n",
    "try:\n",
    "    # Essayer de charger des stats si base disponible\n",
    "    stats_text += \"   ‚Ä¢ Pipeline en cours d'ex√©cution...\\n\"\n",
    "except:\n",
    "    stats_text += \"   ‚Ä¢ D√©marrage du pipeline...\\n\"\n",
    "\n",
    "ax.text(5, 1.5, stats_text, ha='center', va='center', fontsize=10,\n",
    "        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n",
    "\n",
    "plt.title(\"üé¨ FIL D'ARIANE VISUEL - Accompagnement narratif du jury\", \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Le fil d'Ariane vous guide √©tape par √©tape √† travers le pipeline\")\n",
    "print(\"   Chaque visualisation s'inscrit dans cette progression narrative\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "> Notes:\n",
    "> - **üéØ Configuration flexible** : Toutes les sources sont configur√©es dans `config/sources_config.json`\n",
    "> - **Pour ajouter/modifier une source** : √âditez simplement le JSON, relancez le notebook, c'est tout !\n",
    "> - **Stockage hybride** : PostgreSQL (tables t01-t37) + MinIO (DataLake brut)\n",
    "> - **D√©duplication** : SHA256 fingerprint sur titre+texte pour √©viter doublons\n",
    "> - **Tra√ßabilit√©** : Chaque collecte cr√©e un `t03_flux` avec `manifest_uri` pointant vers MinIO\n",
    "> - **Visualisations compl√®tes** : Graphiques + tables pandas √† **chaque √©tape du pipeline** :\n",
    ">   - üìä **√âtape 1** : Donn√©es brutes dans MinIO DataLake (objets, tailles, sources)\n",
    ">   - üßπ **√âtape 2** : Apr√®s nettoyage (avant/apr√®s, statistiques)\n",
    ">   - üíæ **√âtape 3** : Insertion PostgreSQL (volumes, flux)\n",
    ">   - ‚úÖ **√âtape 4** : Dataset final annot√© (05_snapshot)\n",
    "> - **Tables E1_v3** : Utilisation des tables t01-t37 selon MPD.sql (nomenclature avec pr√©fixe)\n",
    "> - **R√©f√©rences** : docs/datasens_sources_dictionary.md, config/README_SOURCES.md\n",
    "load_dotenv(PROJECT_ROOT / \".env\")\n",
    "\n",
    "PG_HOST = os.getenv(\"POSTGRES_HOST\", \"localhost\")\n",
    "PG_PORT = int(os.getenv(\"POSTGRES_PORT\", \"5432\"))\n",
    "PG_DB = os.getenv(\"POSTGRES_DB\", \"datasens\")\n",
    "PG_USER = os.getenv(\"POSTGRES_USER\", \"ds_user\")\n",
    "PG_PASS = os.getenv(\"POSTGRES_PASS\", \"ds_pass\")\n",
    "\n",
    "PG_URL = f\"postgresql+psycopg2://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{PG_DB}\"\n",
    "engine = create_engine(PG_URL, future=True)\n",
    "\n",
    "# Configuration MinIO (DataLake)\n",
    "MINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT\", \"http://localhost:9000\")\n",
    "MINIO_ACCESS_KEY = os.getenv(\"MINIO_ACCESS_KEY\", \"miniouser\")\n",
    "MINIO_SECRET_KEY = os.getenv(\"MINIO_SECRET_KEY\", \"miniosecret\")\n",
    "MINIO_BUCKET = os.getenv(\"MINIO_BUCKET\", \"datasens-raw\")\n",
    "\n",
    "RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "MANIFESTS_DIR = RAW_DIR / \"manifests\"\n",
    "LOGS_DIR = PROJECT_ROOT / \"logs\"\n",
    "\n",
    "# Cr√©er dossiers\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MANIFESTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =====================================================\n",
    "# SYST√àME DE LOGGING (comme datasens_E1_v2.ipynb)\n",
    "# =====================================================\n",
    "log_timestamp = datetime.now(UTC).strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_file = LOGS_DIR / f\"collecte_{log_timestamp}.log\"\n",
    "error_file = LOGS_DIR / f\"errors_{log_timestamp}.log\"\n",
    "\n",
    "logger = logging.getLogger(\"DataSens\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "file_formatter = logging.Formatter(\n",
    "    \"%(asctime)s | %(levelname)-8s | %(name)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "console_formatter = logging.Formatter(\n",
    "    \"[%(asctime)s] %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\"\n",
    ")\n",
    "\n",
    "file_handler = logging.FileHandler(log_file, encoding=\"utf-8\")\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(file_formatter)\n",
    "\n",
    "error_handler = logging.FileHandler(error_file, encoding=\"utf-8\")\n",
    "error_handler.setLevel(logging.ERROR)\n",
    "error_handler.setFormatter(file_formatter)\n",
    "\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "console_handler.setFormatter(console_formatter)\n",
    "\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(error_handler)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "def log_error(source: str, error: Exception, context: str = \"\"):\n",
    "    \"\"\"Log une erreur avec traceback complet\"\"\"\n",
    "    error_msg = f\"[{source}] {context}: {error!s}\"\n",
    "    logger.error(error_msg)\n",
    "    logger.error(f\"Traceback:\\n{traceback.format_exc()}\")\n",
    "\n",
    "logger.info(\"üöÄ Syst√®me de logging initialis√©\")\n",
    "logger.info(f\"üìÅ Logs: {log_file}\")\n",
    "logger.info(f\"‚ùå Erreurs: {error_file}\")\n",
    "\n",
    "# =====================================================\n",
    "# MINIO CLIENT (DataLake)\n",
    "# =====================================================\n",
    "try:\n",
    "    minio_client = Minio(\n",
    "        MINIO_ENDPOINT.replace(\"http://\", \"\").replace(\"https://\", \"\"),\n",
    "        access_key=MINIO_ACCESS_KEY,\n",
    "        secret_key=MINIO_SECRET_KEY,\n",
    "        secure=MINIO_ENDPOINT.startswith(\"https\")\n",
    "    )\n",
    "\n",
    "    def ensure_bucket(bucket: str = MINIO_BUCKET):\n",
    "        if not minio_client.bucket_exists(bucket):\n",
    "            minio_client.make_bucket(bucket)\n",
    "\n",
    "    def minio_upload(local_path: Path, dest_key: str) -> str:\n",
    "        \"\"\"Upload fichier vers MinIO DataLake\"\"\"\n",
    "        ensure_bucket(MINIO_BUCKET)\n",
    "        minio_client.fput_object(MINIO_BUCKET, dest_key, str(local_path))\n",
    "        return f\"s3://{MINIO_BUCKET}/{dest_key}\"\n",
    "\n",
    "    ensure_bucket()\n",
    "    logger.info(f\"‚úÖ MinIO OK ‚Üí bucket: {MINIO_BUCKET}\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"‚ö†Ô∏è MinIO non disponible: {e} - Mode local uniquement\")\n",
    "    minio_client = None\n",
    "    def minio_upload(local_path: Path, dest_key: str) -> str:\n",
    "        return f\"local://{local_path}\"\n",
    "\n",
    "# =====================================================\n",
    "# FONCTIONS UTILITAIRES\n",
    "# =====================================================\n",
    "def ts() -> str:\n",
    "    \"\"\"Timestamp UTC ISO compact\"\"\"\n",
    "    return datetime.now(UTC).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "\n",
    "def sha256(s: str) -> str:\n",
    "    \"\"\"Hash SHA-256 pour d√©duplication\"\"\"\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def get_source_id(conn, nom: str) -> int:\n",
    "    \"\"\"R√©cup√®re l'id_source depuis le nom\"\"\"\n",
    "    logger.info(f\"[get_source_id] Recherche source: {nom}\")\n",
    "    result = conn.execute(text(\"SELECT id_source FROM source WHERE nom = :nom\"), {\"nom\": nom}).fetchone()\n",
    "    if result:\n",
    "        logger.info(f\"   ‚Üí id_source trouv√©: {result[0]}\")\n",
    "        return result[0]\n",
    "    logger.warning(f\"   ‚Üí Source non trouv√©e: {nom}\")\n",
    "    return None\n",
    "\n",
    "def create_flux(conn, id_source: int, format_type: str = \"csv\", manifest_uri: str = None) -> int:\n",
    "    \"\"\"Cr√©e un flux et retourne id_flux\"\"\"\n",
    "    logger.info(f\"[create_flux] Cr√©ation flux pour id_source={id_source}, format={format_type}\")\n",
    "    result = conn.execute(text(\"\"\"\n",
    "        INSERT INTO flux (id_source, format, manifest_uri)\n",
    "        VALUES (:id_source, :format, :manifest_uri)\n",
    "        RETURNING id_flux\n",
    "    \"\"\"), {\"id_source\": id_source, \"format\": format_type, \"manifest_uri\": manifest_uri})\n",
    "    id_flux = result.scalar()\n",
    "    logger.info(f\"   ‚Üí id_flux cr√©√©: {id_flux}\")\n",
    "    return id_flux\n",
    "\n",
    "def ensure_territoire(conn, ville: str, code_insee: str = None, lat: float = None, lon: float = None) -> int:\n",
    "    \"\"\"Cr√©e ou r√©cup√®re un territoire\"\"\"\n",
    "    logger.info(f\"[ensure_territoire] V√©rification territoire: ville={ville}\")\n",
    "    result = conn.execute(text(\"SELECT id_territoire FROM territoire WHERE ville = :ville\"), {\"ville\": ville}).fetchone()\n",
    "    if result:\n",
    "        logger.info(f\"   ‚Üí id_territoire existant: {result[0]}\")\n",
    "        return result[0]\n",
    "    result = conn.execute(text(\"\"\"\n",
    "        INSERT INTO territoire (ville, code_insee, lat, lon)\n",
    "        VALUES (:ville, :code_insee, :lat, :lon)\n",
    "        RETURNING id_territoire\n",
    "    \"\"\"), {\"ville\": ville, \"code_insee\": code_insee, \"lat\": lat, \"lon\": lon})\n",
    "    id_territoire = result.scalar()\n",
    "    logger.info(f\"   ‚Üí id_territoire cr√©√©: {id_territoire}\")\n",
    "    return id_territoire\n",
    "\n",
    "def insert_documents(conn, docs: list) -> int:\n",
    "    \"\"\"Insertion batch de documents avec gestion doublons\"\"\"\n",
    "    logger.info(f\"[insert_documents] Insertion de {len(docs)} documents...\")\n",
    "    inserted = 0\n",
    "    for doc in docs:\n",
    "        try:\n",
    "            result = conn.execute(text(\"\"\"\n",
    "                INSERT INTO document (id_flux, id_territoire, titre, texte, langue, date_publication, hash_fingerprint)\n",
    "                VALUES (:id_flux, :id_territoire, :titre, :texte, :langue, :date_publication, :hash_fingerprint)\n",
    "                ON CONFLICT (hash_fingerprint) DO NOTHING\n",
    "                RETURNING id_doc\n",
    "            \"\"\"), doc)\n",
    "            id_doc = result.scalar()\n",
    "            if id_doc:\n",
    "                logger.info(f\"   ‚Üí Document ins√©r√©: id_doc={id_doc}, titre={doc.get('titre', '')[:40]}\")\n",
    "                inserted += 1\n",
    "        except Exception as e:\n",
    "            log_error(\"insert_documents\", e, \"Erreur insertion document\")\n",
    "    logger.info(f\"   ‚Üí Total ins√©r√©s: {inserted}/{len(docs)}\")\n",
    "    return inserted\n",
    "\n",
    "print(\"‚úÖ Configuration pipeline charg√©e\")\n",
    "print(f\"   üìç PostgreSQL : {PG_HOST}:{PG_PORT}/{PG_DB}\")\n",
    "print(f\"   ‚òÅÔ∏è MinIO : {MINIO_BUCKET if minio_client else 'Mode local'}\")\n",
    "print(f\"   üìÇ Raw data : {RAW_DIR}\")\n",
    "print(f\"   üìÑ Logs : {LOGS_DIR}\")\n",
    "print(\"\\n‚úÖ Pipeline DataLake + PostgreSQL pr√™t !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataSens E1_v3 - 03_ingest_sources\n",
    "# üì• Collecte r√©elle de TOUTES les sources avec visualisations (36/37 tables)\n",
    "\n",
    "import datetime as dt\n",
    "import hashlib\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import feedparser\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "from minio import Minio\n",
    "from sqlalchemy import create_engine, text\n",
    "from tqdm import tqdm\n",
    "\n",
    "# R√©cup√©rer les variables du notebook 01\n",
    "if 'PROJECT_ROOT' not in globals():\n",
    "    current = Path.cwd()\n",
    "    PROJECT_ROOT = None\n",
    "    while current != current.parent:\n",
    "        if (current / \"notebooks\").exists() and (current / \"docs\").exists():\n",
    "            PROJECT_ROOT = current\n",
    "            break\n",
    "        current = current.parent\n",
    "    else:\n",
    "        PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "if 'RAW_DIR' not in globals():\n",
    "    RAW_DIR = PROJECT_ROOT / 'data' / 'raw'\n",
    "\n",
    "if 'PG_URL' not in globals():\n",
    "    PG_URL = os.getenv(\"DATASENS_PG_URL\", \"postgresql+psycopg2://postgres:postgres@localhost:5433/postgres\")\n",
    "\n",
    "if 'MINIO_ENDPOINT' not in globals():\n",
    "    MINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT\", \"http://localhost:9002\")\n",
    "    MINIO_ACCESS_KEY = os.getenv(\"MINIO_ACCESS_KEY\", \"admin\")\n",
    "    MINIO_SECRET_KEY = os.getenv(\"MINIO_SECRET_KEY\", \"admin123\")\n",
    "    MINIO_BUCKET = os.getenv(\"MINIO_BUCKET\", \"datasens-raw\")\n",
    "\n",
    "if 'ts' not in globals():\n",
    "    def ts() -> str:\n",
    "        return dt.datetime.now(tz=dt.UTC).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "\n",
    "if 'sha256_hash' not in globals():\n",
    "    def sha256_hash(s: str) -> str:\n",
    "        return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "# Connexions\n",
    "engine = create_engine(PG_URL, future=True)\n",
    "\n",
    "# =====================================================\n",
    "# CHARGEMENT CONFIGURATION FLEXIBLE DES SOURCES\n",
    "# =====================================================\n",
    "import json\n",
    "\n",
    "CONFIG_FILE = PROJECT_ROOT / \"config\" / \"sources_config.json\"\n",
    "\n",
    "if CONFIG_FILE.exists():\n",
    "    with open(CONFIG_FILE, encoding='utf-8') as f:\n",
    "        sources_config = json.load(f)\n",
    "    \n",
    "    # Filtrer sources actives uniquement\n",
    "    sources_actives = [s for s in sources_config['sources'] if s.get('actif', True)]\n",
    "    \n",
    "    print(f\"\\nüéØ Configuration flexible charg√©e :\")\n",
    "    print(f\"   üìÑ Config : {CONFIG_FILE.name}\")\n",
    "    print(f\"   üìä {len(sources_config['sources'])} sources configur√©es\")\n",
    "    print(f\"   ‚úÖ {len(sources_actives)} sources actives\")\n",
    "    \n",
    "    # Afficher r√©sum√© des sources actives\n",
    "    print(\"\\nüìã Sources √† collecter :\")\n",
    "    for idx, source in enumerate(sources_actives, 1):\n",
    "        print(f\"   {idx}. {source['nom']} ({source['id']}) - {source['collector']} - Priorit√©: {source.get('priorite', 'moyenne')}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Fichier de configuration introuvable : {CONFIG_FILE}\")\n",
    "    print(f\"   üí° Cr√©ez le fichier selon config/README_SOURCES.md\")\n",
    "    sources_config = None\n",
    "    sources_actives = []\n",
    "\n",
    "try:\n",
    "    minio_client = Minio(\n",
    "        MINIO_ENDPOINT.replace(\"http://\", \"\").replace(\"https://\", \"\"),\n",
    "        access_key=MINIO_ACCESS_KEY,\n",
    "        secret_key=MINIO_SECRET_KEY,\n",
    "        secure=False\n",
    "    )\n",
    "    if not minio_client.bucket_exists(MINIO_BUCKET):\n",
    "        minio_client.make_bucket(MINIO_BUCKET)\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è MinIO: {e}\")\n",
    "    minio_client = None\n",
    "\n",
    "print(\"‚úÖ Connexions pr√™tes (PostgreSQL + MinIO)\")\n",
    "print(\"üìä Architecture E1_v3 : 36/37 tables (t01-t37) selon MPD.sql\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Utilitaires : Fonctions helpers pour la collecte E1_v3\n",
    "\n",
    "Fonctions adapt√©es aux tables t01-t37 avec pr√©fixe selon MPD.sql :\n",
    "- **minio_upload()** : Upload fichier vers MinIO (DataLake)\n",
    "- **get_source_id()** : R√©cup√©rer ou cr√©er une source dans t02_source\n",
    "- **create_flux()** : Cr√©er un flux dans t03_flux avec tra√ßabilit√©\n",
    "- **ensure_territoire()** : Cr√©er ou r√©cup√©rer un territoire (t13-t17 hi√©rarchie)\n",
    "- **insert_documents()** : Insertion batch dans t04_document avec gestion des doublons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è Fonctions utilitaires pour la collecte E1_v3 (tables t01-t37)\n",
    "\n",
    "def minio_upload(local_path: Path, minio_path: str) -> str:\n",
    "    \"\"\"Upload un fichier vers MinIO et retourne l'URI\"\"\"\n",
    "    if minio_client is None:\n",
    "        return f\"local://{local_path}\"\n",
    "    try:\n",
    "        minio_client.fput_object(MINIO_BUCKET, minio_path, str(local_path))\n",
    "        return f\"s3://{MINIO_BUCKET}/{minio_path}\"\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Erreur MinIO upload: {e}\")\n",
    "        return f\"local://{local_path}\"\n",
    "\n",
    "def get_source_id(conn, nom: str, type_source: str = None) -> int:\n",
    "    \"\"\"R√©cup√®re l'ID d'une source (t02_source) ou la cr√©e si absente\"\"\"\n",
    "    result = conn.execute(text(\"SELECT id_source FROM t02_source WHERE nom = :nom\"), {\"nom\": nom}).scalar()\n",
    "    if result:\n",
    "        return result\n",
    "    # Cr√©er la source avec le bon type\n",
    "    if type_source:\n",
    "        tid = conn.execute(text(\"SELECT id_type_donnee FROM t01_type_donnee WHERE libelle = :libelle LIMIT 1\"), {\"libelle\": type_source}).scalar()\n",
    "    else:\n",
    "        # Fallback : chercher 'Donn√©es Op√©rationnelles' par d√©faut\n",
    "        tid = conn.execute(text(\"SELECT id_type_donnee FROM t01_type_donnee WHERE libelle = 'Donn√©es Op√©rationnelles' LIMIT 1\")).scalar()\n",
    "    if not tid:\n",
    "        # Dernier fallback\n",
    "        tid = conn.execute(text(\"SELECT id_type_donnee FROM t01_type_donnee LIMIT 1\")).scalar() or 1\n",
    "    return conn.execute(text(\"\"\"\n",
    "        INSERT INTO t02_source(id_type_donnee, nom, url, fiabilite) \n",
    "        VALUES (:tid, :nom, '', 0.8) RETURNING id_source\n",
    "    \"\"\"), {\"tid\": tid, \"nom\": nom}).scalar()\n",
    "\n",
    "def create_flux(conn, source_nom: str, format_type: str = \"csv\", manifest_uri: str = None) -> int:\n",
    "    \"\"\"Cr√©e un flux (t03_flux) de collecte et retourne son ID\"\"\"\n",
    "    sid = get_source_id(conn, source_nom)\n",
    "    return conn.execute(text(\"\"\"\n",
    "        INSERT INTO t03_flux(id_source, format, manifest_uri, date_collecte)\n",
    "        VALUES (:sid, :format, :manifest, NOW()) RETURNING id_flux\n",
    "    \"\"\"), {\"sid\": sid, \"format\": format_type, \"manifest\": manifest_uri}).scalar()\n",
    "\n",
    "def ensure_territoire_complet(conn, ville: str, code_insee: str = None, lat: float = None, lon: float = None) -> int:\n",
    "    \"\"\"Cr√©e ou r√©cup√®re un territoire complet (hi√©rarchie t13-t17)\"\"\"\n",
    "    # Pour E1_v3, on simplifie : chercher dans t17_territoire via t16_commune\n",
    "    # Si code_insee fourni, chercher dans t16_commune\n",
    "    if code_insee:\n",
    "        commune = conn.execute(text(\"\"\"\n",
    "            SELECT c.id_commune FROM t16_commune c \n",
    "            WHERE c.code_insee = :code\n",
    "        \"\"\"), {\"code\": code_insee}).scalar()\n",
    "        if commune:\n",
    "            terr = conn.execute(text(\"\"\"\n",
    "                SELECT t.id_territoire FROM t17_territoire t \n",
    "                WHERE t.id_commune = :c\n",
    "            \"\"\"), {\"c\": commune}).scalar()\n",
    "            if terr:\n",
    "                return terr\n",
    "    # Sinon, cr√©er un territoire minimal (simplifi√© pour E1_v3)\n",
    "    # Pour une impl√©mentation compl√®te, il faudrait cr√©er pays ‚Üí r√©gion ‚Üí d√©partement ‚Üí commune ‚Üí territoire\n",
    "    # Ici on simplifie en cr√©ant directement dans t17 avec un id_commune fictif si n√©cessaire\n",
    "    # En pratique, on utiliserait une table territoire simplifi√©e ou cr√©erait la hi√©rarchie compl√®te\n",
    "    # Pour E1_v3, on cr√©e un territoire minimal directement\n",
    "    return conn.execute(text(\"\"\"\n",
    "        INSERT INTO t17_territoire(id_commune)\n",
    "        VALUES ((SELECT id_commune FROM t16_commune LIMIT 1))\n",
    "        RETURNING id_territoire\n",
    "    \"\"\")).scalar() if conn.execute(text(\"SELECT COUNT(*) FROM t16_commune\")).scalar() > 0 else None\n",
    "\n",
    "def insert_documents(conn, df: pd.DataFrame, flux_id: int):\n",
    "    \"\"\"Insertion batch de documents (t04_document) avec gestion des doublons\"\"\"\n",
    "    inserted = 0\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            conn.execute(text(\"\"\"\n",
    "                INSERT INTO t04_document(id_flux, titre, texte, langue, date_publication, hash_fingerprint)\n",
    "                VALUES(:fid, :titre, :texte, :langue, :date, :hash)\n",
    "                ON CONFLICT (hash_fingerprint) DO NOTHING\n",
    "            \"\"\"), {\n",
    "                \"fid\": flux_id,\n",
    "                \"titre\": row.get(\"titre\", \"\"),\n",
    "                \"texte\": row.get(\"texte\", \"\"),\n",
    "                \"langue\": row.get(\"langue\", \"fr\"),\n",
    "                \"date\": row.get(\"date_publication\"),\n",
    "                \"hash\": row.get(\"hash_fingerprint\", \"\")\n",
    "            })\n",
    "            inserted += 1\n",
    "        except Exception as e:\n",
    "            pass  # Doublon ou erreur silencieuse\n",
    "    return inserted\n",
    "\n",
    "# =====================================================\n",
    "# FONCTIONS UTILITAIRES DE S√âCURIT√â\n",
    "# =====================================================\n",
    "def assert_valid_identifier(name: str) -> None:\n",
    "    \"\"\"\n",
    "    Valide qu'un identifiant SQL (nom de table, colonne) est s√ªr.\n",
    "    L√®ve une ValueError si l'identifiant contient des caract√®res non autoris√©s.\n",
    "    \"\"\"\n",
    "    if not isinstance(name, str):\n",
    "        raise ValueError(\"L'identifiant doit √™tre une cha√Æne de caract√®res.\")\n",
    "    # Autorise lettres, chiffres, underscores, et points (pour sch√©mas.tables)\n",
    "    if not name.replace('_', '').replace('.', '').isalnum():\n",
    "        raise ValueError(f\"Identifiant SQL invalide : {name}. Seuls les caract√®res alphanum√©riques, underscores et points sont autoris√©s.\")\n",
    "\n",
    "def load_whitelist_tables(conn, schema: str = 'datasens') -> set[str]:\n",
    "    \"\"\"\n",
    "    Charge une liste blanche des noms de tables valides depuis information_schema.\n",
    "    Retourne un set des noms de tables pour validation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = conn.execute(text(f\"\"\"\n",
    "            SELECT table_name FROM information_schema.tables\n",
    "            WHERE table_schema = :schema_name\n",
    "        \"\"\"), {\"schema_name\": schema}).fetchall()\n",
    "        return {row[0] for row in result}\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur lors du chargement de la whitelist des tables: {e}\")\n",
    "        return set()  # Retourne un set vide en cas d'erreur\n",
    "\n",
    "print(\"‚úÖ Fonctions utilitaires charg√©es (adapt√©es t01-t37)\")\n",
    "print(\"‚úÖ Fonctions de s√©curit√© (assert_valid_identifier, load_whitelist_tables) charg√©es.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì∞ Source 1 : Flux RSS Multi-Sources (Presse fran√ßaise)\n",
    "\n",
    "Collecte d'articles depuis 3 flux RSS fran√ßais :\n",
    "- **Franceinfo** : Service public, actualit√©s g√©n√©rales\n",
    "- **20 Minutes** : Presse gratuite, grand public  \n",
    "- **Le Monde** : Presse de r√©f√©rence\n",
    "\n",
    "**Process** : Parsing RSS ‚Üí DataFrame ‚Üí D√©duplication SHA256 ‚Üí PostgreSQL (t04_document) + MinIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì∞ Source 1 : Flux RSS Multi-Sources\n",
    "print(\"üì∞ SOURCE 1 : Flux RSS Multi-Sources\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "RSS_SOURCES = {\n",
    "    \"Franceinfo\": \"https://www.francetvinfo.fr/titres.rss\",\n",
    "    \"20 Minutes\": \"https://www.20minutes.fr/feeds/rss-une.xml\",\n",
    "    \"Le Monde\": \"https://www.lemonde.fr/rss/une.xml\"\n",
    "}\n",
    "\n",
    "all_rss_items = []\n",
    "\n",
    "for source_name, rss_url in RSS_SOURCES.items():\n",
    "    print(f\"\\nüì° Source : {source_name}\")\n",
    "    try:\n",
    "        feed = feedparser.parse(rss_url)\n",
    "        if len(feed.entries) == 0:\n",
    "            print(\"   ‚ö†Ô∏è Aucun article\")\n",
    "            continue\n",
    "        \n",
    "        source_items = []\n",
    "        for e in feed.entries[:30]:  # Max 30 par source\n",
    "            titre = e.get(\"title\", \"\").strip()\n",
    "            texte = (e.get(\"summary\", \"\") or e.get(\"description\", \"\") or \"\").strip()\n",
    "            if titre and texte:\n",
    "                source_items.append({\n",
    "                    \"titre\": titre,\n",
    "                    \"texte\": texte,\n",
    "                    \"date_publication\": pd.to_datetime(e.get(\"published\", \"\"), errors=\"coerce\"),\n",
    "                    \"langue\": \"fr\",\n",
    "                    \"source_media\": source_name,\n",
    "                    \"url\": e.get(\"link\", \"\")\n",
    "                })\n",
    "        all_rss_items.extend(source_items)\n",
    "        print(f\"   ‚úÖ {len(source_items)} articles collect√©s\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Erreur : {str(e)[:80]}\")\n",
    "    time.sleep(1)\n",
    "\n",
    "# Consolidation\n",
    "df_rss = pd.DataFrame(all_rss_items)\n",
    "if len(df_rss) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è Aucun article RSS collect√©\")\n",
    "else:\n",
    "    print(f\"\\nüìä Total brut : {len(df_rss)} articles\")\n",
    "    \n",
    "    # D√©duplication\n",
    "    df_rss[\"hash_fingerprint\"] = df_rss.apply(\n",
    "        lambda row: sha256_hash(row[\"titre\"] + \" \" + row[\"texte\"]), axis=1\n",
    "    )\n",
    "    nb_avant = len(df_rss)\n",
    "    df_rss = df_rss.drop_duplicates(subset=[\"hash_fingerprint\"])\n",
    "    nb_apres = len(df_rss)\n",
    "    print(f\"üßπ D√©duplication : {nb_avant} ‚Üí {nb_apres} articles uniques\")\n",
    "    \n",
    "    # Sauvegarde locale + MinIO\n",
    "    local = RAW_DIR / \"rss\" / f\"rss_multi_{ts()}.csv\"\n",
    "    local.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_rss.to_csv(local, index=False)\n",
    "    minio_uri = minio_upload(local, f\"rss/{local.name}\")\n",
    "    \n",
    "    # Insertion PostgreSQL (t04_document via t03_flux)\n",
    "    with engine.begin() as conn:\n",
    "        flux_id = create_flux(conn, \"Flux RSS Multi-Sources (Franceinfo + 20 Minutes + Le Monde)\", \"rss\", minio_uri)\n",
    "        inserted = insert_documents(conn, df_rss[[\"titre\", \"texte\", \"langue\", \"date_publication\", \"hash_fingerprint\"]], flux_id)\n",
    "    \n",
    "    print(f\"\\n‚úÖ RSS : {inserted} articles ins√©r√©s en base (t04_document) + MinIO\")\n",
    "    print(f\"‚òÅÔ∏è MinIO : {minio_uri}\")\n",
    "    \n",
    "    # üìä Visualisations\n",
    "    print(\"\\nüìä R√©partition par source m√©diatique :\")\n",
    "    lang_counts = df_rss['source_media'].value_counts()\n",
    "    display(pd.DataFrame({\"Source\": lang_counts.index, \"Nombre\": lang_counts.values}))\n",
    "    \n",
    "    if len(lang_counts) > 0:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        bars = plt.bar(lang_counts.index, lang_counts.values, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "        for bar, value in zip(bars, lang_counts.values):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                    str(value), ha='center', va='bottom', fontweight='bold')\n",
    "        plt.title(\"üìä R√©partition des articles RSS par source\", fontsize=12, fontweight='bold')\n",
    "        plt.ylabel(\"Nombre d'articles\", fontsize=11)\n",
    "        plt.xticks(rotation=15, ha='right')\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # üìã Table de donn√©es r√©elles\n",
    "    print(\"\\nüìã Table 't04_document' - Articles RSS ins√©r√©s (aper√ßu 10 premiers) :\")\n",
    "    df_docs = pd.read_sql_query(\"\"\"\n",
    "        SELECT d.id_doc, d.titre, d.langue, d.date_publication, s.nom AS source\n",
    "        FROM t04_document d\n",
    "        JOIN t03_flux f ON d.id_flux = f.id_flux\n",
    "        JOIN t02_source s ON f.id_source = s.id_source\n",
    "        WHERE s.nom LIKE '%RSS%'\n",
    "        ORDER BY d.id_doc DESC\n",
    "        LIMIT 10\n",
    "    \"\"\", engine)\n",
    "    display(df_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üå¶Ô∏è Source 2 : API OpenWeatherMap (M√©t√©o en temps r√©el)\n",
    "\n",
    "Collecte de donn√©es m√©t√©o pour 4+ villes fran√ßaises.\n",
    "\n",
    "**Stockage** : PostgreSQL (t19_meteo + hi√©rarchie t13-t17) + MinIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üå¶Ô∏è Source 2 : API OpenWeatherMap\n",
    "print(\"\\nüå¶Ô∏è SOURCE 2 : API OpenWeatherMap\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "OWM_CITIES = [\"Paris,FR\", \"Lyon,FR\", \"Marseille,FR\", \"Toulouse,FR\"]\n",
    "OWM_API_KEY = os.getenv(\"OWM_API_KEY\")\n",
    "\n",
    "if not OWM_API_KEY:\n",
    "    print(\"‚ö†Ô∏è OWM_API_KEY manquante - Source 2 ignor√©e\")\n",
    "else:\n",
    "    rows = []\n",
    "    for city in tqdm(OWM_CITIES, desc=\"OWM\"):\n",
    "        try:\n",
    "            r = requests.get(\n",
    "                \"https://api.openweathermap.org/data/2.5/weather\",\n",
    "                params={\"q\": city, \"appid\": OWM_API_KEY, \"units\": \"metric\", \"lang\": \"fr\"},\n",
    "                timeout=10\n",
    "            )\n",
    "            if r.status_code == 200:\n",
    "                j = r.json()\n",
    "                rows.append({\n",
    "                    \"ville\": j[\"name\"],\n",
    "                    \"lat\": j[\"coord\"][\"lat\"],\n",
    "                    \"lon\": j[\"coord\"][\"lon\"],\n",
    "                    \"date_obs\": pd.to_datetime(j[\"dt\"], unit=\"s\"),\n",
    "                    \"temperature\": j[\"main\"][\"temp\"],\n",
    "                    \"humidite\": j[\"main\"][\"humidity\"],\n",
    "                    \"vent_kmh\": (j.get(\"wind\", {}).get(\"speed\") or 0) * 3.6,\n",
    "                    \"pression\": j.get(\"main\", {}).get(\"pressure\"),\n",
    "                    \"meteo_type\": j[\"weather\"][0][\"main\"] if j.get(\"weather\") else None\n",
    "                })\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Erreur {city}: {str(e)[:60]}\")\n",
    "    \n",
    "    if rows:\n",
    "        df_owm = pd.DataFrame(rows)\n",
    "        local = RAW_DIR / \"api\" / \"owm\" / f\"owm_{ts()}.csv\"\n",
    "        local.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df_owm.to_csv(local, index=False)\n",
    "        minio_uri = minio_upload(local, f\"api/owm/{local.name}\")\n",
    "        \n",
    "        # Insertion PostgreSQL (t19_meteo + t17_territoire)\n",
    "        # Pour E1_v3, on simplifie la cr√©ation de territoire (en production, cr√©er la hi√©rarchie compl√®te)\n",
    "        with engine.begin() as conn:\n",
    "            flux_id = create_flux(conn, \"OpenWeatherMap\", \"json\", minio_uri)\n",
    "            for _, r in df_owm.iterrows():\n",
    "                # Cr√©er ou r√©cup√©rer territoire (simplifi√© pour E1_v3)\n",
    "                # En production, cr√©er pays ‚Üí r√©gion ‚Üí d√©partement ‚Üí commune ‚Üí territoire\n",
    "                # Ici, on cr√©e directement une commune et territoire si n√©cessaire\n",
    "                commune_id = conn.execute(text(\"\"\"\n",
    "                    INSERT INTO t16_commune(id_departement, nom_commune, lat, lon)\n",
    "                    VALUES (\n",
    "                        (SELECT id_departement FROM t15_departement LIMIT 1),\n",
    "                        :ville, :lat, :lon\n",
    "                    )\n",
    "                    ON CONFLICT DO NOTHING\n",
    "                    RETURNING id_commune\n",
    "                \"\"\"), {\"ville\": r[\"ville\"], \"lat\": r[\"lat\"], \"lon\": r[\"lon\"]}).scalar()\n",
    "                \n",
    "                if not commune_id:\n",
    "                    commune_id = conn.execute(text(\"\"\"\n",
    "                        SELECT id_commune FROM t16_commune WHERE nom_commune = :ville LIMIT 1\n",
    "                    \"\"\"), {\"ville\": r[\"ville\"]}).scalar()\n",
    "                \n",
    "                if commune_id:\n",
    "                    terr_id = conn.execute(text(\"\"\"\n",
    "                        INSERT INTO t17_territoire(id_commune)\n",
    "                        VALUES (:c)\n",
    "                        ON CONFLICT DO NOTHING\n",
    "                        RETURNING id_territoire\n",
    "                    \"\"\"), {\"c\": commune_id}).scalar()\n",
    "                    \n",
    "                    if not terr_id:\n",
    "                        terr_id = conn.execute(text(\"\"\"\n",
    "                            SELECT id_territoire FROM t17_territoire WHERE id_commune = :c LIMIT 1\n",
    "                        \"\"\"), {\"c\": commune_id}).scalar()\n",
    "                    \n",
    "                    if terr_id:\n",
    "                        # Ins√©rer dans t19_meteo\n",
    "                        conn.execute(text(\"\"\"\n",
    "                            INSERT INTO t19_meteo(id_territoire, date_obs, temperature, humidite, vent_kmh, pression)\n",
    "                            VALUES(:t, :d, :T, :H, :V, :P)\n",
    "                        \"\"\"), {\n",
    "                            \"t\": terr_id, \"d\": r[\"date_obs\"], \"T\": r[\"temperature\"],\n",
    "                            \"H\": r[\"humidite\"], \"V\": r[\"vent_kmh\"], \"P\": r[\"pression\"]\n",
    "                        })\n",
    "        \n",
    "        print(f\"\\n‚úÖ OWM : {len(df_owm)} relev√©s ins√©r√©s en base (t19_meteo) + MinIO\")\n",
    "        print(f\"‚òÅÔ∏è MinIO : {minio_uri}\")\n",
    "        \n",
    "        # üìä Visualisations\n",
    "        print(\"\\nüìä R√©partition des relev√©s par ville :\")\n",
    "        display(df_owm[[\"ville\", \"temperature\", \"humidite\", \"meteo_type\"]])\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        bars = plt.bar(df_owm[\"ville\"], df_owm[\"temperature\"], color='#FF6B6B')\n",
    "        for bar, value in zip(bars, df_owm[\"temperature\"]):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                    f\"{value:.1f}¬∞C\", ha='center', va='bottom', fontweight='bold')\n",
    "        plt.title(\"üå°Ô∏è Temp√©rature par ville\", fontsize=12, fontweight='bold')\n",
    "        plt.ylabel(\"Temp√©rature (¬∞C)\", fontsize=11)\n",
    "        plt.xticks(rotation=15)\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        bars = plt.bar(df_owm[\"ville\"], df_owm[\"humidite\"], color='#4ECDC4')\n",
    "        for bar, value in zip(bars, df_owm[\"humidite\"]):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                    f\"{value}%\", ha='center', va='bottom', fontweight='bold')\n",
    "        plt.title(\"üíß Humidit√© par ville\", fontsize=12, fontweight='bold')\n",
    "        plt.ylabel(\"Humidit√© (%)\", fontsize=11)\n",
    "        plt.xticks(rotation=15)\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # üìã Tables de donn√©es r√©elles\n",
    "        print(\"\\nüìã Table 't19_meteo' - Relev√©s ins√©r√©s :\")\n",
    "        df_meteo = pd.read_sql_query(\"\"\"\n",
    "            SELECT m.id_meteo, m.date_obs, m.temperature, m.humidite, m.meteo_type\n",
    "            FROM t19_meteo m\n",
    "            ORDER BY m.id_meteo DESC\n",
    "            LIMIT 10\n",
    "        \"\"\", engine)\n",
    "        display(df_meteo)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Aucun relev√© m√©t√©o collect√©\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåê Source 3 : Web Scraping Multi-Sources (6 sources citoyennes)\n",
    "\n",
    "Collecte depuis 6 sources l√©gales et √©thiques :\n",
    "- **Reddit** (API PRAW) : r/france, r/Paris\n",
    "- **YouTube** (API) : Commentaires vid√©os actualit√©s\n",
    "- **SignalConso** (Open Data gouv.fr) : Signalements consommateurs\n",
    "- **Trustpilot FR** : Avis services publics\n",
    "- **Vie-publique.fr** : Consultations citoyennes\n",
    "- **data.gouv.fr** (API) : Datasets Open Data\n",
    "\n",
    "**Stockage** : PostgreSQL (t04_document) + MinIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üåê Source 3 : Web Scraping Multi-Sources (6 sources)\n",
    "print(\"\\nüåê SOURCE 3 : Web Scraping Multi-Sources (6 sources)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "all_scraping_data = []\n",
    "\n",
    "# Source 1/6 : Reddit (si credentials disponibles)\n",
    "try:\n",
    "    import praw\n",
    "    REDDIT_CLIENT_ID = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "    REDDIT_CLIENT_SECRET = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
    "    \n",
    "    if REDDIT_CLIENT_ID and REDDIT_CLIENT_SECRET:\n",
    "        reddit = praw.Reddit(\n",
    "            client_id=REDDIT_CLIENT_ID,\n",
    "            client_secret=REDDIT_CLIENT_SECRET,\n",
    "            user_agent=\"DataSensBot/1.0\"\n",
    "        )\n",
    "        for subreddit_name in [\"france\", \"Paris\"]:\n",
    "            subreddit = reddit.subreddit(subreddit_name)\n",
    "            for post in subreddit.hot(limit=25):\n",
    "                all_scraping_data.append({\n",
    "                    \"titre\": post.title,\n",
    "                    \"texte\": post.selftext or post.title,\n",
    "                    \"source_site\": \"reddit.com\",\n",
    "                    \"url\": f\"https://reddit.com{post.permalink}\",\n",
    "                    \"date_publication\": pd.to_datetime(post.created_utc, unit=\"s\"),\n",
    "                    \"langue\": \"fr\"\n",
    "                })\n",
    "        print(f\"   ‚úÖ Reddit: {len([d for d in all_scraping_data if 'reddit' in d['source_site']])} posts collect√©s\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è Reddit: Credentials manquantes (REDDIT_CLIENT_ID/SECRET)\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Reddit: {str(e)[:60]} (skip)\")\n",
    "\n",
    "# Source 2/6 : YouTube (API)\n",
    "try:\n",
    "    from googleapiclient.discovery import build\n",
    "    YOUTUBE_API_KEY = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "    \n",
    "    if YOUTUBE_API_KEY:\n",
    "        youtube = build(\"youtube\", \"v3\", developerKey=YOUTUBE_API_KEY)\n",
    "        request = youtube.search().list(\n",
    "            part=\"snippet\", q=\"france actualit√©s\", type=\"video\",\n",
    "            maxResults=20, regionCode=\"FR\", relevanceLanguage=\"fr\"\n",
    "        )\n",
    "        response = request.execute()\n",
    "        \n",
    "        for item in response.get(\"items\", []):\n",
    "            snippet = item[\"snippet\"]\n",
    "            all_scraping_data.append({\n",
    "                \"titre\": snippet[\"title\"],\n",
    "                \"texte\": snippet[\"description\"] or snippet[\"title\"],\n",
    "                \"source_site\": \"youtube.com\",\n",
    "                \"url\": f\"https://www.youtube.com/watch?v={item['id']['videoId']}\",\n",
    "                \"date_publication\": pd.to_datetime(snippet[\"publishedAt\"], errors=\"coerce\"),\n",
    "                \"langue\": \"fr\"\n",
    "            })\n",
    "        print(f\"   ‚úÖ YouTube: {len([d for d in all_scraping_data if 'youtube' in d['source_site']])} vid√©os collect√©es\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è YouTube: YOUTUBE_API_KEY manquante\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è YouTube: {str(e)[:60]} (skip)\")\n",
    "\n",
    "# Source 3/6 : Vie-publique.fr (RSS)\n",
    "try:\n",
    "    feed_url = \"https://www.vie-publique.fr/rss\"\n",
    "    feed = feedparser.parse(feed_url)\n",
    "    for entry in feed.entries[:30]:\n",
    "        all_scraping_data.append({\n",
    "            \"titre\": entry.get(\"title\", \"\"),\n",
    "            \"texte\": entry.get(\"summary\", entry.get(\"description\", \"\")),\n",
    "            \"source_site\": \"vie-publique.fr\",\n",
    "            \"url\": entry.get(\"link\", \"\"),\n",
    "            \"date_publication\": pd.to_datetime(entry.get(\"published\", \"\"), errors=\"coerce\"),\n",
    "            \"langue\": \"fr\"\n",
    "        })\n",
    "    print(f\"   ‚úÖ Vie-publique.fr: {len([d for d in all_scraping_data if 'vie-publique' in d['source_site']])} articles collect√©s\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Vie-publique.fr: {str(e)[:60]} (skip)\")\n",
    "\n",
    "# Source 4/6 : data.gouv.fr (API)\n",
    "try:\n",
    "    url = \"https://www.data.gouv.fr/api/1/datasets/\"\n",
    "    params = {\"q\": \"france\", \"page_size\": 30}\n",
    "    response = requests.get(url, params=params, timeout=10)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    \n",
    "    for dataset in data.get(\"data\", []):\n",
    "        all_scraping_data.append({\n",
    "            \"titre\": dataset.get(\"title\", \"\"),\n",
    "            \"texte\": dataset.get(\"description\", dataset.get(\"title\", \"\")),\n",
    "            \"source_site\": \"data.gouv.fr\",\n",
    "            \"url\": f\"https://www.data.gouv.fr/fr/datasets/{dataset.get('slug', '')}\",\n",
    "            \"date_publication\": pd.to_datetime(dataset.get(\"created_at\", \"\"), errors=\"coerce\"),\n",
    "            \"langue\": \"fr\"\n",
    "        })\n",
    "    print(f\"   ‚úÖ data.gouv.fr: {len([d for d in all_scraping_data if 'data.gouv' in d['source_site']])} datasets collect√©s\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è data.gouv.fr: {str(e)[:60]} (skip)\")\n",
    "\n",
    "# Consolidation et insertion\n",
    "if all_scraping_data:\n",
    "    df_scraping = pd.DataFrame(all_scraping_data)\n",
    "    df_scraping = df_scraping[df_scraping[\"texte\"].str.len() > 20].copy()\n",
    "    df_scraping[\"hash_fingerprint\"] = df_scraping.apply(\n",
    "        lambda row: sha256_hash(row[\"titre\"] + \" \" + row[\"texte\"]), axis=1\n",
    "    )\n",
    "    df_scraping = df_scraping.drop_duplicates(subset=[\"hash_fingerprint\"])\n",
    "    \n",
    "    local = RAW_DIR / \"scraping\" / \"multi\" / f\"scraping_multi_{ts()}.csv\"\n",
    "    local.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_scraping.to_csv(local, index=False)\n",
    "    minio_uri = minio_upload(local, f\"scraping/multi/{local.name}\")\n",
    "    \n",
    "    with engine.begin() as conn:\n",
    "        flux_id = create_flux(conn, \"Web Scraping Multi-Sources\", \"html\", minio_uri)\n",
    "        inserted = insert_documents(conn, df_scraping[[\"titre\", \"texte\", \"langue\", \"date_publication\", \"hash_fingerprint\"]], flux_id)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Web Scraping Multi-Sources : {inserted} documents ins√©r√©s (t04_document) + MinIO\")\n",
    "    print(f\"‚òÅÔ∏è MinIO : {minio_uri}\")\n",
    "    \n",
    "    # üìä Visualisation par source\n",
    "    if len(df_scraping) > 0:\n",
    "        site_counts = df_scraping['source_site'].value_counts()\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        bars = plt.bar(site_counts.index, site_counts.values, color=plt.cm.Pastel1(range(len(site_counts))))\n",
    "        for bar, value in zip(bars, site_counts.values):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                    str(value), ha='center', va='bottom', fontweight='bold')\n",
    "        plt.title(\"üìä R√©partition Web Scraping par site\", fontsize=12, fontweight='bold')\n",
    "        plt.ylabel(\"Nombre de documents\", fontsize=11)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # üìã Table de donn√©es\n",
    "        print(\"\\nüìã Table 't04_document' - Web Scraping (aper√ßu 10 premiers) :\")\n",
    "        df_scrap_docs = pd.read_sql_query(\"\"\"\n",
    "            SELECT d.id_doc, LEFT(d.titre, 80) AS titre, d.date_publication\n",
    "            FROM t04_document d\n",
    "            JOIN t03_flux f ON d.id_flux = f.id_flux\n",
    "            JOIN t02_source s ON f.id_source = s.id_source\n",
    "            WHERE s.nom LIKE '%Scraping%'\n",
    "            ORDER BY d.id_doc DESC\n",
    "            LIMIT 10\n",
    "        \"\"\", engine)\n",
    "        display(df_scrap_docs)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Aucune donn√©e Web Scraping collect√©e\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì∞ Source 4 : NewsAPI (Optionnel)\n",
    "\n",
    "Collecte d'articles via l'API NewsAPI si la cl√© est configur√©e.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì∞ Source 4 : NewsAPI (Optionnel)\n",
    "print(\"\\nüì∞ SOURCE 4 : NewsAPI (Optionnel)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "NEWSAPI_KEY = os.getenv(\"NEWSAPI_KEY\")\n",
    "\n",
    "if not NEWSAPI_KEY:\n",
    "    print(\"‚ö†Ô∏è NEWSAPI_KEY manquante - Source 4 ignor√©e\")\n",
    "else:\n",
    "    NEWS_CATEGORIES = [\"general\", \"technology\", \"health\", \"business\"]\n",
    "    all_articles = []\n",
    "    \n",
    "    for category in NEWS_CATEGORIES:\n",
    "        try:\n",
    "            r = requests.get(\n",
    "                \"https://newsapi.org/v2/top-headlines\",\n",
    "                params={\"apiKey\": NEWSAPI_KEY, \"country\": \"fr\", \"category\": category, \"pageSize\": 20},\n",
    "                timeout=10\n",
    "            )\n",
    "            if r.status_code == 200:\n",
    "                data = r.json()\n",
    "                articles = data.get(\"articles\", [])\n",
    "                for art in articles:\n",
    "                    all_articles.append({\n",
    "                        \"titre\": (art.get(\"title\") or \"\").strip(),\n",
    "                        \"texte\": (art.get(\"description\") or art.get(\"content\") or \"\").strip(),\n",
    "                        \"date_publication\": pd.to_datetime(art.get(\"publishedAt\"), errors=\"coerce\"),\n",
    "                        \"langue\": \"fr\",\n",
    "                        \"categorie\": category\n",
    "                    })\n",
    "            elif r.status_code in [426, 429]:\n",
    "                print(f\"   ‚ö†Ô∏è Quota √©puis√© pour {category}\")\n",
    "                break\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Erreur {category}: {str(e)[:60]}\")\n",
    "    \n",
    "    if all_articles:\n",
    "        df_news = pd.DataFrame(all_articles)\n",
    "        df_news = df_news[df_news[\"texte\"].str.len() > 20].copy()\n",
    "        df_news[\"hash_fingerprint\"] = df_news.apply(\n",
    "            lambda row: sha256_hash(row[\"titre\"] + \" \" + row[\"texte\"]), axis=1\n",
    "        )\n",
    "        df_news = df_news.drop_duplicates(subset=[\"hash_fingerprint\"])\n",
    "        \n",
    "        local = RAW_DIR / \"api\" / \"newsapi\" / f\"newsapi_{ts()}.csv\"\n",
    "        local.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df_news.to_csv(local, index=False)\n",
    "        minio_uri = minio_upload(local, f\"api/newsapi/{local.name}\")\n",
    "        \n",
    "        with engine.begin() as conn:\n",
    "            flux_id = create_flux(conn, \"NewsAPI\", \"json\", minio_uri)\n",
    "            inserted = insert_documents(conn, df_news[[\"titre\", \"texte\", \"langue\", \"date_publication\", \"hash_fingerprint\"]], flux_id)\n",
    "        \n",
    "        print(f\"\\n‚úÖ NewsAPI : {inserted} articles ins√©r√©s (t04_document)\")\n",
    "        \n",
    "        # üìä Visualisation\n",
    "        if len(df_news) > 0:\n",
    "            cat_counts = df_news['categorie'].value_counts()\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.pie(cat_counts.values, labels=cat_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "            plt.title(\"üìä R√©partition NewsAPI par cat√©gorie\", fontsize=12, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Aucun article NewsAPI r√©cup√©r√© (quota √©puis√© ou cl√© invalide)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Bilan global de la collecte E1_v3\n",
    "\n",
    "R√©capitulatif de toutes les sources collect√©es avec statistiques globales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Bilan global de la collecte E1_v3\n",
    "print(\"\\nüìä BILAN GLOBAL DE LA COLLECTE E1_V3\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Statistiques par source (tables t02_source, t03_flux, t04_document)\n",
    "with engine.connect() as conn:\n",
    "    stats = pd.read_sql_query(\"\"\"\n",
    "        SELECT \n",
    "            s.nom AS source,\n",
    "            COUNT(DISTINCT f.id_flux) AS nb_flux,\n",
    "            COUNT(DISTINCT d.id_doc) AS nb_documents,\n",
    "            td.libelle AS type_donnee\n",
    "        FROM t02_source s\n",
    "        LEFT JOIN t03_flux f ON s.id_source = f.id_source\n",
    "        LEFT JOIN t04_document d ON f.id_flux = d.id_flux\n",
    "        LEFT JOIN t01_type_donnee td ON s.id_type_donnee = td.id_type_donnee\n",
    "        GROUP BY s.nom, td.libelle\n",
    "        ORDER BY nb_documents DESC\n",
    "    \"\"\", conn)\n",
    "\n",
    "print(\"\\nüìà Statistiques par source :\")\n",
    "display(stats)\n",
    "\n",
    "# Total documents\n",
    "total_docs = stats['nb_documents'].sum()\n",
    "print(f\"\\nüìä Total documents collect√©s : {total_docs}\")\n",
    "\n",
    "# Graphique global\n",
    "if len(stats) > 0:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(stats[\"source\"], stats[\"nb_documents\"], color=plt.cm.Set3(range(len(stats))))\n",
    "    for bar, value in zip(bars, stats[\"nb_documents\"]):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                str(int(value)), ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "    plt.title(\"üìä Nombre de documents collect√©s par source (E1_v3)\", fontsize=14, fontweight='bold')\n",
    "    plt.ylabel(\"Nombre de documents\", fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Vue compl√®te : tous les documents avec contexte\n",
    "print(\"\\nüìã Vue compl√®te - Tous les documents avec contexte (50 premiers) :\")\n",
    "df_all_docs = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        d.id_doc,\n",
    "        LEFT(d.titre, 60) AS titre,\n",
    "        LEFT(d.texte, 100) AS texte_apercu,\n",
    "        d.langue,\n",
    "        d.date_publication,\n",
    "        s.nom AS source,\n",
    "        f.date_collecte,\n",
    "        f.format\n",
    "    FROM t04_document d\n",
    "    JOIN t03_flux f ON d.id_flux = f.id_flux\n",
    "    JOIN t02_source s ON f.id_source = s.id_source\n",
    "    ORDER BY d.id_doc DESC\n",
    "    LIMIT 50\n",
    "\"\"\", engine)\n",
    "display(df_all_docs)\n",
    "\n",
    "# Statistiques par type de donn√©e\n",
    "print(\"\\nüìä R√©partition par type de donn√©e :\")\n",
    "df_types = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        td.libelle AS type_donnee,\n",
    "        COUNT(DISTINCT s.id_source) AS nb_sources,\n",
    "        COUNT(DISTINCT d.id_doc) AS nb_documents\n",
    "    FROM t01_type_donnee td\n",
    "    LEFT JOIN t02_source s ON td.id_type_donnee = s.id_type_donnee\n",
    "    LEFT JOIN t03_flux f ON s.id_source = f.id_source\n",
    "    LEFT JOIN t04_document d ON f.id_flux = d.id_flux\n",
    "    GROUP BY td.libelle\n",
    "    ORDER BY nb_documents DESC\n",
    "\"\"\", engine)\n",
    "display(df_types)\n",
    "\n",
    "if len(df_types) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(df_types[\"type_donnee\"], df_types[\"nb_documents\"], color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "    for bar, value in zip(bars, df_types[\"nb_documents\"]):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                str(int(value)), ha='center', va='bottom', fontweight='bold')\n",
    "    plt.title(\"üìä R√©partition des documents par type de donn√©e (E1_v3)\", fontsize=12, fontweight='bold')\n",
    "    plt.ylabel(\"Nombre de documents\", fontsize=11)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Collecte E1_v3 termin√©e : {total_docs} documents collect√©s et stock√©s\")\n",
    "print(\"   üìä Architecture compl√®te : 36/37 tables (t01-t37)\")\n",
    "print(\"   ‚û°Ô∏è Passez au notebook 04_quality_checks.ipynb pour valider la qualit√©\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALISATIONS GDELT (Section 5/5) - Compl√©ment audit\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä VISUALISATIONS SOURCE GDELT (Section 5/5)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        # V√©rifier si des √©v√©nements GDELT ont √©t√© ins√©r√©s\n",
    "        nb_events_gdelt = conn.execute(text(\"\"\"\n",
    "            SELECT COUNT(*) FROM t25_evenement WHERE source_event = 'GDELT'\n",
    "        \"\"\")).scalar()\n",
    "        \n",
    "        if nb_events_gdelt > 0:\n",
    "            # Bar chart : √âv√©nements par th√®me\n",
    "            df_events_theme = pd.read_sql_query(\"\"\"\n",
    "                SELECT \n",
    "                    th.libelle AS theme,\n",
    "                    COUNT(e.id_event) AS nb_evenements,\n",
    "                    AVG(e.avg_tone) AS tonalite_moyenne\n",
    "                FROM t25_evenement e\n",
    "                JOIN t24_theme th ON e.id_theme = th.id_theme\n",
    "                WHERE e.source_event = 'GDELT'\n",
    "                GROUP BY th.libelle\n",
    "                ORDER BY nb_evenements DESC\n",
    "                LIMIT 10\n",
    "            \"\"\", conn)\n",
    "            \n",
    "            if len(df_events_theme) > 0:\n",
    "                print(\"\\nüìä √âv√©nements par th√®me :\")\n",
    "                display(df_events_theme)\n",
    "                \n",
    "                plt.figure(figsize=(14, 6))\n",
    "                \n",
    "                plt.subplot(1, 2, 1)\n",
    "                bars = plt.barh(df_events_theme[\"theme\"], df_events_theme[\"nb_evenements\"], color=plt.cm.Set2(range(len(df_events_theme))))\n",
    "                for i, (bar, value) in enumerate(zip(bars, df_events_theme[\"nb_evenements\"])):\n",
    "                    plt.text(bar.get_width() + max(df_events_theme[\"nb_evenements\"]) * 0.02, bar.get_y() + bar.get_height()/2,\n",
    "                            f\"{int(value)}\", ha='left', va='center', fontweight='bold', fontsize=9)\n",
    "                plt.title(\"üìä √âv√©nements par th√®me (GDELT France)\", fontsize=12, fontweight='bold')\n",
    "                plt.xlabel(\"Nombre d'√©v√©nements\", fontsize=11)\n",
    "                plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n",
    "                \n",
    "                plt.subplot(1, 2, 2)\n",
    "                bars = plt.barh(df_events_theme[\"theme\"], df_events_theme[\"tonalite_moyenne\"], color=plt.cm.RdYlGn_r(range(len(df_events_theme))))\n",
    "                for i, (bar, value) in enumerate(zip(bars, df_events_theme[\"tonalite_moyenne\"])):\n",
    "                    plt.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2,\n",
    "                            f\"{value:.1f}\", ha='left', va='center', fontweight='bold', fontsize=9)\n",
    "                plt.title(\"üìä Tonalit√© moyenne par th√®me (GDELT)\", fontsize=12, fontweight='bold')\n",
    "                plt.xlabel(\"Tonalit√© moyenne (-100 n√©gatif ‚Üí +100 positif)\", fontsize=11)\n",
    "                plt.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "                plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "            # Table pandas : √âv√©nements France ins√©r√©s\n",
    "            df_events_france = pd.read_sql_query(\"\"\"\n",
    "                SELECT \n",
    "                    e.id_event,\n",
    "                    th.libelle AS theme,\n",
    "                    e.date_event,\n",
    "                    e.avg_tone AS tonalite,\n",
    "                    e.source_event\n",
    "                FROM t25_evenement e\n",
    "                LEFT JOIN t24_theme th ON e.id_theme = th.id_theme\n",
    "                WHERE e.source_event = 'GDELT'\n",
    "                ORDER BY e.date_event DESC\n",
    "                LIMIT 20\n",
    "            \"\"\", conn)\n",
    "            \n",
    "            if len(df_events_france) > 0:\n",
    "                print(\"\\nüìã √âv√©nements France ins√©r√©s (20 derniers) :\")\n",
    "                display(df_events_france)\n",
    "            else:\n",
    "                print(\"\\n‚ö†Ô∏è Aucun √©v√©nement GDELT √† afficher\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è Aucun √©v√©nement GDELT ins√©r√© dans la base\")\n",
    "            print(\"   üí° La collecte GDELT peut avoir √©chou√© ou aucun √©v√©nement France trouv√©\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Erreur lors de la r√©cup√©ration des donn√©es GDELT : {str(e)[:100]}\")\n",
    "    print(\"   üí° Les visualisations seront disponibles apr√®s une collecte GDELT r√©ussie\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≠ Chapitre 1 : Kaggle CSV\n",
    "\n",
    "**Contexte narratif** : Collecte de donn√©es depuis Kaggle CSV\n",
    "\n",
    "**Avant cette collecte** :\n",
    "- Sources pr√©c√©dentes : 0 source(s) d√©j√† collect√©e(s)\n",
    "- Documents en base : [V√©rification en cours...]\n",
    "\n",
    "**Objectif de cette √©tape** :\n",
    "- Collecter de nouvelles donn√©es depuis Kaggle CSV\n",
    "- Enrichir notre dataset avec cette source\n",
    "- Progression du pipeline vers le dataset final\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üé≠ STORYTELLING : PR√âPARATION COLLECTE 1 - Kaggle CSV\n",
    "# ============================================================\n",
    "# Cette section raconte l'histoire de la collecte avant de l'effectuer\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"üé≠ CHAPITRE 1 : COLLECTE Kaggle CSV\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# V√©rifier l'√©tat actuel avant cette collecte\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        # Statistiques avant cette source\n",
    "        nb_sources_avant = conn.execute(text(\"SELECT COUNT(*) FROM t02_source\")).scalar() or 0\n",
    "        nb_docs_avant = conn.execute(text(\"SELECT COUNT(*) FROM t04_document\")).scalar() or 0\n",
    "        nb_flux_avant = conn.execute(text(\"SELECT COUNT(*) FROM t03_flux\")).scalar() or 0\n",
    "        \n",
    "        print(f\"\\nüìä √âTAT ACTUEL DU PIPELINE (avant Kaggle CSV):\")\n",
    "        print(f\"   ‚Ä¢ Sources configur√©es : {nb_sources_avant}\")\n",
    "        print(f\"   ‚Ä¢ Documents collect√©s : {nb_docs_avant:,}\")\n",
    "        print(f\"   ‚Ä¢ Flux de collecte : {nb_flux_avant}\")\n",
    "        \n",
    "        # Visualisation √©tat actuel\n",
    "        if nb_docs_avant > 0:\n",
    "            # Graphique progression\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            \n",
    "            categories = ['Sources', 'Flux', 'Documents']\n",
    "            valeurs = [nb_sources_avant, nb_flux_avant, nb_docs_avant]\n",
    "            colors = ['#FF6B6B', '#FECA57', '#4ECDC4']\n",
    "            \n",
    "            bars = ax.bar(categories, valeurs, color=colors)\n",
    "            for bar, val in zip(bars, valeurs):\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(valeurs) * 0.02,\n",
    "                       f\"{int(val):,}\", ha='center', va='bottom', fontweight='bold')\n",
    "            \n",
    "            ax.set_title(f\"üìä √âtat du pipeline AVANT collecte Kaggle CSV\", fontsize=12, fontweight='bold')\n",
    "            ax.set_ylabel(\"Volume\", fontsize=11)\n",
    "            ax.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"\\nüí° Prochaine √©tape : Collecte Kaggle CSV pour enrichir le dataset...\")\n",
    "        else:\n",
    "            print(f\"\\nüí° D√©marrage : Premi√®re collecte avec Kaggle CSV...\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"\\nüí° Pr√™t pour collecte Kaggle CSV...\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(f\"‚û°Ô∏è Lancement de la collecte Kaggle CSV...\")\n",
    "print(\"-\"*80 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÑ Source 1/5 : Fichier plat CSV (Kaggle)\n",
    "\n",
    "**Architecture hybride (comme datasens_E1_v2.ipynb)** :\n",
    "- **50% ‚Üí PostgreSQL** : Donn√©es structur√©es pour requ√™tes SQL\n",
    "- **50% ‚Üí MinIO DataLake** : Donn√©es brutes pour analyses Big Data futures\n",
    "\n",
    "**Process** :\n",
    "1. Chargement CSV depuis `data/raw/kaggle/`\n",
    "2. Calcul SHA256 fingerprint pour d√©duplication\n",
    "3. Split al√©atoire 50/50\n",
    "4. Upload 50% vers MinIO (DataLake)\n",
    "5. Insertion 50% dans PostgreSQL avec tra√ßabilit√© (id_flux)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"üìÑ SOURCE 1/5 : Fichier plat CSV (Kaggle)\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "# Rechercher fichier Kaggle existant ou cr√©er √©chantillon\n",
    "kaggle_csv_paths = [\n",
    "    RAW_DIR / \"kaggle\" / \"kaggle_sample.csv\",\n",
    "    PROJECT_ROOT / \"data\" / \"raw\" / \"kaggle\" / \"*.csv\",\n",
    "    Path.cwd() / \"data\" / \"raw\" / \"kaggle\" / \"*.csv\"\n",
    "]\n",
    "\n",
    "kaggle_csv_path = None\n",
    "for path in kaggle_csv_paths:\n",
    "    if path.exists():\n",
    "        kaggle_csv_path = path\n",
    "        break\n",
    "\n",
    "if not kaggle_csv_path or not kaggle_csv_path.exists():\n",
    "    logger.warning(\"‚ö†Ô∏è Fichier Kaggle non trouv√© ‚Äî Cr√©ation √©chantillon pour d√©mo\")\n",
    "    sample_data = pd.DataFrame({\n",
    "        \"text\": [\n",
    "            \"Great product, very satisfied!\",\n",
    "            \"Service terrible, avoid at all costs\",\n",
    "            \"Excellent quality, recommend\",\n",
    "            \"Bon produit, je recommande\",\n",
    "            \"Mauvais service, d√©√ßu\"\n",
    "        ],\n",
    "        \"langue\": [\"en\", \"en\", \"en\", \"fr\", \"fr\"],\n",
    "        \"date\": [datetime.now(UTC)] * 5\n",
    "    })\n",
    "    kaggle_csv_path = RAW_DIR / \"kaggle\" / \"kaggle_sample.csv\"\n",
    "    kaggle_csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    sample_data.to_csv(kaggle_csv_path, index=False)\n",
    "    logger.info(f\"   ‚úÖ √âchantillon cr√©√© : {kaggle_csv_path.name}\")\n",
    "\n",
    "# Charger le CSV\n",
    "df_kaggle = pd.read_csv(kaggle_csv_path)\n",
    "logger.info(f\"üìä {len(df_kaggle)} lignes charg√©es\")\n",
    "\n",
    "# Split 50/50 (architecture hybride : PostgreSQL + MinIO)\n",
    "df_kaggle[\"hash_fingerprint\"] = df_kaggle[\"text\"].apply(lambda x: sha256(str(x)))\n",
    "mid_point = len(df_kaggle) // 2\n",
    "df_pg = df_kaggle.iloc[:mid_point].copy()  # 50% ‚Üí PostgreSQL\n",
    "df_raw = df_kaggle.iloc[mid_point:].copy()  # 50% ‚Üí MinIO DataLake\n",
    "\n",
    "logger.info(f\"   ‚Ä¢ 50% PostgreSQL : {len(df_pg)} lignes\")\n",
    "logger.info(f\"   ‚Ä¢ 50% MinIO DataLake : {len(df_raw)} lignes\")\n",
    "\n",
    "# Sauvegarder 50% en raw local + upload MinIO\n",
    "raw_output = RAW_DIR / \"kaggle\" / f\"kaggle_raw_{ts()}.csv\"\n",
    "df_raw.to_csv(raw_output, index=False)\n",
    "logger.info(f\"   ‚úÖ Sauvegard√© local : {raw_output.name}\")\n",
    "\n",
    "# Upload MinIO (50% bruts vers DataLake)\n",
    "try:\n",
    "    minio_uri = minio_upload(raw_output, f\"kaggle/{raw_output.name}\")\n",
    "    logger.info(f\"   ‚òÅÔ∏è Upload MinIO : {minio_uri}\")\n",
    "except Exception as e:\n",
    "    log_error(\"MinIO\", e, \"Upload fichier Kaggle\")\n",
    "    minio_uri = f\"local://{raw_output}\"\n",
    "\n",
    "# Ins√©rer 50% dans PostgreSQL\n",
    "with engine.begin() as conn:\n",
    "    id_source = get_source_id(conn, \"Kaggle CSV\")\n",
    "    if not id_source:\n",
    "        id_type = conn.execute(text(\"SELECT id_type_donnee FROM type_donnee WHERE libelle = 'Fichier plat'\")).scalar()\n",
    "        conn.execute(text(\"\"\"\n",
    "            INSERT INTO source (id_type_donnee, nom, url, fiabilite)\n",
    "            VALUES (:id_type, 'Kaggle CSV', 'https://www.kaggle.com', 0.8)\n",
    "        \"\"\"), {\"id_type\": id_type})\n",
    "        id_source = conn.execute(text(\"SELECT id_source FROM source WHERE nom = 'Kaggle CSV'\")).scalar()\n",
    "\n",
    "    id_flux = create_flux(conn, id_source, \"csv\", minio_uri)\n",
    "\n",
    "    # Pr√©parer documents pour insertion batch\n",
    "    docs = []\n",
    "    for _, row in df_pg.iterrows():\n",
    "        docs.append({\n",
    "            \"id_flux\": id_flux,\n",
    "            \"id_territoire\": None,\n",
    "            \"titre\": \"\",\n",
    "            \"texte\": str(row[\"text\"]),\n",
    "            \"langue\": row.get(\"langue\", \"en\"),\n",
    "            \"date_publication\": row.get(\"date\", datetime.now(UTC)),\n",
    "            \"hash_fingerprint\": row[\"hash_fingerprint\"]\n",
    "        })\n",
    "\n",
    "    inserted = insert_documents(conn, docs)\n",
    "\n",
    "logger.info(f\"\\n‚úÖ Source 1/5 termin√©e : {inserted} docs PostgreSQL + {len(df_raw)} docs MinIO\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Architecture Pipeline (R√©f√©rence datasens_E1_v2.ipynb)\n",
    "\n",
    "**Ce notebook suit l'architecture du pipeline existant** :\n",
    "\n",
    "‚úÖ **Logging structur√©** : `logs/collecte_*.log` + `logs/errors_*.log`  \n",
    "‚úÖ **MinIO DataLake** : Upload automatique fichiers bruts ‚Üí `s3://datasens-raw/`  \n",
    "‚úÖ **PostgreSQL** : Insertion structur√©e avec tra√ßabilit√© (flux, manifests)  \n",
    "‚úÖ **Fonctions helpers** : `create_flux()`, `insert_documents()`, `ensure_territoire()`, `minio_upload()`  \n",
    "‚úÖ **D√©duplication** : Hash SHA-256 pour √©viter doublons  \n",
    "‚úÖ **RGPD** : Pas de donn√©es personnelles directes  \n",
    "\n",
    "**Sources 2-5** : Impl√©ment√©es ci-dessous avec vraies sources (code extrait de `datasens_E1_v2.ipynb`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Visualisation Pipeline Compl√®te - √âtat des Donn√©es\n",
    "\n",
    "Visualisations compl√®tes √† chaque √©tape du pipeline ETL pour suivre le flux de donn√©es.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALISATIONS PIPELINE COMPLET - √âTAT DES DONN√âES\n",
    "# ============================================================\n",
    "# √âtape 1 : MinIO DataLake (Donn√©es Brutes)\n",
    "# √âtape 2 : Apr√®s Nettoyage (Statistiques d√©duplication)\n",
    "# √âtape 3 : PostgreSQL (Donn√©es Structur√©es)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä VISUALISATIONS PIPELINE COMPLET E1_V3\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================\n",
    "# √âTAPE 1 : √âTAT MINIO DATALAKE (DONN√âES BRUTES)\n",
    "# ============================================================\n",
    "print(\"\\nüìä √âTAPE 1 : MINIO DATALAKE (Donn√©es Brutes)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "try:\n",
    "    if minio_client and minio_client.bucket_exists(MINIO_BUCKET):\n",
    "        objects = list(minio_client.list_objects(MINIO_BUCKET, recursive=True))\n",
    "        \n",
    "        if len(objects) > 0:\n",
    "            total_size = sum(obj.size for obj in objects)\n",
    "            total_size_mb = total_size / (1024 * 1024)\n",
    "            \n",
    "            # R√©partition par pr√©fixe/source\n",
    "            prefixes = {}\n",
    "            sizes_by_prefix = {}\n",
    "            for obj in objects:\n",
    "                prefix = obj.object_name.split('/')[0] if '/' in obj.object_name else 'root'\n",
    "                prefixes[prefix] = prefixes.get(prefix, 0) + 1\n",
    "                sizes_by_prefix[prefix] = sizes_by_prefix.get(prefix, 0) + obj.size\n",
    "            \n",
    "            df_minio = pd.DataFrame([{\n",
    "                \"Type\": prefix,\n",
    "                \"Nb objets\": prefixes[prefix],\n",
    "                \"Taille (MB)\": round(sizes_by_prefix[prefix] / (1024 * 1024), 2)\n",
    "            } for prefix in sorted(prefixes.keys())])\n",
    "            \n",
    "            print(f\"\\nüì¶ MinIO DataLake '{MINIO_BUCKET}' :\")\n",
    "            print(f\"   ‚Ä¢ {len(objects)} objets bruts\")\n",
    "            print(f\"   ‚Ä¢ Taille totale : {total_size_mb:.2f} MB\")\n",
    "            print(\"\\nüìã R√©partition des donn√©es brutes :\")\n",
    "            display(df_minio)\n",
    "            \n",
    "            # Graphiques MinIO\n",
    "            plt.figure(figsize=(14, 5))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            bars = plt.bar(df_minio[\"Type\"], df_minio[\"Nb objets\"], color=plt.cm.Set3(range(len(df_minio))))\n",
    "            for bar, value in zip(bars, df_minio[\"Nb objets\"]):\n",
    "                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(df_minio[\"Nb objets\"]) * 0.02,\n",
    "                        str(value), ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "            plt.title(\"üìä Objets bruts par type (MinIO)\", fontsize=12, fontweight='bold')\n",
    "            plt.ylabel(\"Nombre d'objets\", fontsize=11)\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            bars = plt.bar(df_minio[\"Type\"], df_minio[\"Taille (MB)\"], color=plt.cm.Pastel1(range(len(df_minio))))\n",
    "            for bar, value in zip(bars, df_minio[\"Taille (MB)\"]):\n",
    "                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(df_minio[\"Taille (MB)\"]) * 0.02,\n",
    "                        f\"{value:.1f} MB\", ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "            plt.title(\"üíæ Taille donn√©es brutes (MinIO)\", fontsize=12, fontweight='bold')\n",
    "            plt.ylabel(\"Taille (MB)\", fontsize=11)\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"   ‚ÑπÔ∏è Aucun objet dans MinIO (collecte en cours...)\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è MinIO non accessible\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Erreur MinIO : {str(e)[:80]}\")\n",
    "\n",
    "# ============================================================\n",
    "# √âTAPE 2 : APR√àS NETTOYAGE (D√âDUPLICATION)\n",
    "# ============================================================\n",
    "print(\"\\nüßπ √âTAPE 2 : APR√àS NETTOYAGE (D√©duplication)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        # Statistiques d√©duplication\n",
    "        dedup_stats = pd.read_sql_query(\"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) AS total_docs,\n",
    "                COUNT(DISTINCT hash_fingerprint) AS docs_uniques,\n",
    "                COUNT(*) - COUNT(DISTINCT hash_fingerprint) AS doublons_detectes,\n",
    "                ROUND(100.0 * (COUNT(*) - COUNT(DISTINCT hash_fingerprint)) / COUNT(*), 2) AS pct_doublons\n",
    "            FROM t04_document\n",
    "        \"\"\", conn)\n",
    "        \n",
    "        if len(dedup_stats) > 0 and dedup_stats.iloc[0]['total_docs'] > 0:\n",
    "            row = dedup_stats.iloc[0]\n",
    "            print(\"\\nüìã Statistiques nettoyage/d√©duplication :\")\n",
    "            display(dedup_stats)\n",
    "            \n",
    "            # Graphique d√©duplication\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            categories = ['Total brut', 'Uniques', 'Doublons']\n",
    "            values = [row['total_docs'], row['docs_uniques'], row['doublons_detectes']]\n",
    "            colors = ['#FF6B6B', '#4ECDC4', '#FECA57']\n",
    "            bars = plt.bar(categories, values, color=colors)\n",
    "            for bar, value in zip(bars, values):\n",
    "                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(values) * 0.02,\n",
    "                        f\"{int(value):,}\", ha='center', va='bottom', fontweight='bold')\n",
    "            plt.title(\"üßπ Impact du nettoyage et d√©duplication (SHA256)\", fontsize=12, fontweight='bold')\n",
    "            plt.ylabel(\"Nombre de documents\", fontsize=11)\n",
    "            plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"\\n‚úÖ D√©duplication : {row['pct_doublons']:.1f}% de doublons d√©tect√©s et √©vit√©s\")\n",
    "        else:\n",
    "            print(\"   ‚ÑπÔ∏è Aucune donn√©e pour statistiques nettoyage\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Erreur statistiques nettoyage : {str(e)[:80]}\")\n",
    "\n",
    "# ============================================================\n",
    "# √âTAPE 3 : √âTAT POSTGRESQL (DONN√âES STRUCTUR√âES)\n",
    "# ============================================================\n",
    "print(\"\\nüíæ √âTAPE 3 : POSTGRESQL (Donn√©es Structur√©es)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        # Volumes par table\n",
    "        stats_pg = pd.read_sql_query(\"\"\"\n",
    "            SELECT 't04_document' AS table_name, COUNT(*) AS nb_lignes FROM t04_document\n",
    "            UNION ALL SELECT 't03_flux', COUNT(*) FROM t03_flux\n",
    "            UNION ALL SELECT 't02_source', COUNT(*) FROM t02_source\n",
    "            UNION ALL SELECT 't19_meteo', COUNT(*) FROM t19_meteo\n",
    "            UNION ALL SELECT 't25_evenement', COUNT(*) FROM t25_evenement\n",
    "        \"\"\", conn)\n",
    "        \n",
    "        # Documents par source\n",
    "        stats_sources = pd.read_sql_query(\"\"\"\n",
    "            SELECT s.nom AS source, COUNT(d.id_doc) AS nb_documents\n",
    "            FROM t02_source s\n",
    "            LEFT JOIN t03_flux f ON s.id_source = f.id_source\n",
    "            LEFT JOIN t04_document d ON f.id_flux = d.id_flux\n",
    "            GROUP BY s.nom\n",
    "            ORDER BY nb_documents DESC\n",
    "        \"\"\", conn)\n",
    "        \n",
    "        if len(stats_pg) > 0:\n",
    "            print(\"\\nüìã Volumes PostgreSQL par table :\")\n",
    "            display(stats_pg)\n",
    "            \n",
    "            # Graphiques PostgreSQL\n",
    "            plt.figure(figsize=(14, 5))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            bars = plt.bar(stats_pg[\"table_name\"], stats_pg[\"nb_lignes\"], color=plt.cm.Set2(range(len(stats_pg))))\n",
    "            for bar, value in zip(bars, stats_pg[\"nb_lignes\"]):\n",
    "                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(stats_pg[\"nb_lignes\"]) * 0.02,\n",
    "                        f\"{int(value):,}\", ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "            plt.title(\"üìä Volumes PostgreSQL par table\", fontsize=12, fontweight='bold')\n",
    "            plt.ylabel(\"Nombre de lignes\", fontsize=11)\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "            \n",
    "            if len(stats_sources) > 0:\n",
    "                plt.subplot(1, 2, 2)\n",
    "                top = stats_sources.head(10)\n",
    "                bars = plt.barh(top[\"source\"], top[\"nb_documents\"], color=plt.cm.Pastel2(range(len(top))))\n",
    "                for i, (bar, value) in enumerate(zip(bars, top[\"nb_documents\"])):\n",
    "                    plt.text(bar.get_width() + max(top[\"nb_documents\"]) * 0.02, bar.get_y() + bar.get_height()/2,\n",
    "                            f\"{int(value):,}\", ha='left', va='center', fontweight='bold', fontsize=9)\n",
    "                plt.title(\"üìä Documents par source (Top 10)\", fontsize=12, fontweight='bold')\n",
    "                plt.xlabel(\"Nombre de documents\", fontsize=11)\n",
    "                plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            total_docs = stats_pg[stats_pg['table_name'] == 't04_document']['nb_lignes'].iloc[0] if len(stats_pg[stats_pg['table_name'] == 't04_document']) > 0 else 0\n",
    "            print(f\"\\n‚úÖ PostgreSQL : {total_docs:,} documents structur√©s apr√®s ETL\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Erreur PostgreSQL : {str(e)[:80]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Visualisations pipeline compl√®tes termin√©es\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üé≠ STORYTELLING : R√âSULTAT COLLECTE 1 - Kaggle CSV\n",
    "# ============================================================\n",
    "# Cette section montre l'impact de la collecte sur le pipeline\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚úÖ CHAPITRE 1 TERMIN√â : Kaggle CSV\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# V√©rifier l'√©tat apr√®s cette collecte\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        # Statistiques apr√®s cette source\n",
    "        nb_sources_apres = conn.execute(text(\"SELECT COUNT(*) FROM t02_source\")).scalar() or 0\n",
    "        nb_docs_apres = conn.execute(text(\"SELECT COUNT(*) FROM t04_document\")).scalar() or 0\n",
    "        nb_flux_apres = conn.execute(text(\"SELECT COUNT(*) FROM t03_flux\")).scalar() or 0\n",
    "        \n",
    "        # Calculer la progression\n",
    "        docs_source = conn.execute(text(\"\"\"\n",
    "            SELECT COUNT(*) FROM t04_document d\n",
    "            JOIN t03_flux f ON d.id_flux = f.id_flux\n",
    "            JOIN t02_source s ON f.id_source = s.id_source\n",
    "            WHERE s.nom LIKE :pattern\n",
    "        \"\"\"), {\"pattern\": f\"%Kaggle CSV%\"}).scalar() or 0\n",
    "        \n",
    "        print(f\"\\nüìä R√âSULTAT DE LA COLLECTE Kaggle CSV:\")\n",
    "        print(f\"   ‚Ä¢ Documents ajout√©s : {docs_source:,}\")\n",
    "        print(f\"   ‚Ä¢ Sources totales : {nb_sources_apres}\")\n",
    "        print(f\"   ‚Ä¢ Documents totaux : {nb_docs_apres:,}\")\n",
    "        \n",
    "        if docs_source > 0:\n",
    "            # Visualisation r√©sultat\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "            \n",
    "            # Impact de cette source\n",
    "            ax1.bar([f\"Kaggle CSV\\n(ajout√©s)\"], [docs_source], color='#4ECDC4')\n",
    "            ax1.text(0, docs_source, f\"{int(docs_source):,}\", ha='center', va='bottom', \n",
    "                    fontweight='bold', fontsize=12)\n",
    "            ax1.set_title(f\"üìä Impact collecte Kaggle CSV\", fontweight='bold')\n",
    "            ax1.set_ylabel(\"Documents\", fontsize=11)\n",
    "            ax1.grid(axis='y', alpha=0.3)\n",
    "            \n",
    "            # Progression globale\n",
    "            progression = nb_docs_apres\n",
    "            ax2.bar([\"Pipeline\\nGlobal\"], [progression], color='#45B7D1')\n",
    "            ax2.text(0, progression, f\"{int(progression):,}\", ha='center', va='bottom',\n",
    "                    fontweight='bold', fontsize=12)\n",
    "            ax2.set_title(\"üìà Progression totale pipeline\", fontweight='bold')\n",
    "            ax2.set_ylabel(\"Documents totaux\", fontsize=11)\n",
    "            ax2.grid(axis='y', alpha=0.3)\n",
    "            \n",
    "            plt.suptitle(f\"‚úÖ Kaggle CSV : Contribution au dataset final\", \n",
    "                        fontsize=14, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"\\n‚úÖ Contribution : {docs_source:,} documents ajout√©s au dataset\")\n",
    "            print(f\"üìà Progression : {nb_docs_apres:,} documents totaux dans le pipeline\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è Aucun document collect√© pour Kaggle CSV\")\n",
    "            \n",
    "        print(f\"\\n‚û°Ô∏è Prochaine √©tape : Source 2...\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚úÖ Collecte Kaggle CSV termin√©e\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Chapitre 1 Compl√©t√© : Kaggle CSV\n",
    "\n",
    "**R√©sultat de la collecte** : 1 source collect√©e avec succ√®s\n",
    "\n",
    "**Impact sur le pipeline** :\n",
    "- ‚úÖ Donn√©es ajout√©es au DataLake MinIO\n",
    "- ‚úÖ Documents structur√©s dans PostgreSQL\n",
    "- ‚úÖ Pipeline progress√© vers le dataset final\n",
    "\n",
    "**Progression** : 1/6 sources collect√©es\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Chapitre 1 Compl√©t√© : Kaggle CSV\n",
    "\n",
    "**R√©sultat de la collecte** : 1 source collect√©e avec succ√®s\n",
    "\n",
    "**Impact sur le pipeline** :\n",
    "- ‚úÖ Donn√©es ajout√©es au DataLake MinIO\n",
    "- ‚úÖ Documents structur√©s dans PostgreSQL\n",
    "- ‚úÖ Pipeline progress√© vers le dataset final\n",
    "\n",
    "**Progression** : 1/6 sources collect√©es\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Chapitre 1 Compl√©t√© : Kaggle CSV\n",
    "\n",
    "**R√©sultat de la collecte** : 1 source collect√©e avec succ√®s\n",
    "\n",
    "**Impact sur le pipeline** :\n",
    "- ‚úÖ Donn√©es ajout√©es au DataLake MinIO\n",
    "- ‚úÖ Documents structur√©s dans PostgreSQL\n",
    "- ‚úÖ Pipeline progress√© vers le dataset final\n",
    "\n",
    "**Progression** : 1/6 sources collect√©es\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Chapitre 1 Compl√©t√© : Kaggle CSV\n",
    "\n",
    "**R√©sultat de la collecte** : 1 source collect√©e avec succ√®s\n",
    "\n",
    "**Impact sur le pipeline** :\n",
    "- ‚úÖ Donn√©es ajout√©es au DataLake MinIO\n",
    "- ‚úÖ Documents structur√©s dans PostgreSQL\n",
    "- ‚úÖ Pipeline progress√© vers le dataset final\n",
    "\n",
    "**Progression** : 1/6 sources collect√©es\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Chapitre 1 Compl√©t√© : Kaggle CSV\n",
    "\n",
    "**R√©sultat de la collecte** : 1 source collect√©e avec succ√®s\n",
    "\n",
    "**Impact sur le pipeline** :\n",
    "- ‚úÖ Donn√©es ajout√©es au DataLake MinIO\n",
    "- ‚úÖ Documents structur√©s dans PostgreSQL\n",
    "- ‚úÖ Pipeline progress√© vers le dataset final\n",
    "\n",
    "**Progression** : 1/6 sources collect√©es\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Chapitre 1 Compl√©t√© : Kaggle CSV\n",
    "\n",
    "**R√©sultat de la collecte** : 1 source collect√©e avec succ√®s\n",
    "\n",
    "**Impact sur le pipeline** :\n",
    "- ‚úÖ Donn√©es ajout√©es au DataLake MinIO\n",
    "- ‚úÖ Documents structur√©s dans PostgreSQL\n",
    "- ‚úÖ Pipeline progress√© vers le dataset final\n",
    "\n",
    "**Progression** : 1/6 sources collect√©es\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Chapitre 1 Compl√©t√© : Kaggle CSV\n",
    "\n",
    "**R√©sultat de la collecte** : 1 source collect√©e avec succ√®s\n",
    "\n",
    "**Impact sur le pipeline** :\n",
    "- ‚úÖ Donn√©es ajout√©es au DataLake MinIO\n",
    "- ‚úÖ Documents structur√©s dans PostgreSQL\n",
    "- ‚úÖ Pipeline progress√© vers le dataset final\n",
    "\n",
    "**Progression** : 1/6 sources collect√©es\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Chapitre 1 Compl√©t√© : Kaggle CSV\n",
    "\n",
    "**R√©sultat de la collecte** : 1 source collect√©e avec succ√®s\n",
    "\n",
    "**Impact sur le pipeline** :\n",
    "- ‚úÖ Donn√©es ajout√©es au DataLake MinIO\n",
    "- ‚úÖ Documents structur√©s dans PostgreSQL\n",
    "- ‚úÖ Pipeline progress√© vers le dataset final\n",
    "\n",
    "**Progression** : 1/6 sources collect√©es\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üå¶Ô∏è Source 2/5 : API OpenWeatherMap\n",
    "\n",
    "Collecte de donn√©es m√©t√©o en temps r√©el via l'API OpenWeatherMap.\n",
    "\n",
    "**Villes collect√©es** : Paris, Lyon, Marseille, Lille\n",
    "\n",
    "**Donn√©es r√©cup√©r√©es** :\n",
    "- Temp√©rature (¬∞C), Humidit√© (%), Pression (hPa)\n",
    "- Description m√©t√©o (clair, nuageux, pluie...)\n",
    "- Vitesse du vent (m/s)\n",
    "- Timestamp de mesure\n",
    "\n",
    "**Stockage** :\n",
    "- **PostgreSQL** : Table `meteo` avec g√©olocalisation (id_territoire FK)\n",
    "- **MinIO** : CSV brut pour historisation compl√®te\n",
    "\n",
    "**RGPD** : Aucune donn√©e personnelle, donn√©es publiques uniquement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"üå¶Ô∏è SOURCE 2/5 : API OpenWeatherMap\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "# Variables d'environnement\n",
    "OWM_API_KEY = os.getenv(\"OWM_API_KEY\")\n",
    "if not OWM_API_KEY:\n",
    "    logger.warning(\"‚ö†Ô∏è OWM_API_KEY manquante dans .env - Source 2 ignor√©e\")\n",
    "else:\n",
    "    OWM_CITIES = [\"Paris,FR\", \"Lyon,FR\", \"Marseille,FR\", \"Lille,FR\"]\n",
    "\n",
    "    rows = []\n",
    "    for c in tqdm(OWM_CITIES, desc=\"OWM\"):\n",
    "        try:\n",
    "            r = requests.get(\n",
    "                \"https://api.openweathermap.org/data/2.5/weather\",\n",
    "                params={\"q\": c, \"appid\": OWM_API_KEY, \"units\": \"metric\", \"lang\": \"fr\"},\n",
    "                timeout=10\n",
    "            )\n",
    "            if r.status_code == 200:\n",
    "                j = r.json()\n",
    "                rows.append({\n",
    "                    \"ville\": j[\"name\"],\n",
    "                    \"lat\": j[\"coord\"][\"lat\"],\n",
    "                    \"lon\": j[\"coord\"][\"lon\"],\n",
    "                    \"date_obs\": pd.to_datetime(j[\"dt\"], unit=\"s\"),\n",
    "                    \"temperature\": j[\"main\"][\"temp\"],\n",
    "                    \"humidite\": j[\"main\"][\"humidity\"],\n",
    "                    \"vent_kmh\": (j.get(\"wind\", {}).get(\"speed\") or 0) * 3.6,\n",
    "                    \"pression\": j.get(\"main\", {}).get(\"pressure\"),\n",
    "                    \"meteo_type\": j[\"weather\"][0][\"main\"] if j.get(\"weather\") else None\n",
    "                })\n",
    "        except Exception as e:\n",
    "            log_error(\"OpenWeatherMap\", e, f\"Collecte m√©t√©o {c}\")\n",
    "\n",
    "        time.sleep(1)  # Respect rate limit\n",
    "\n",
    "    if len(rows) > 0:\n",
    "        dfm = pd.DataFrame(rows)\n",
    "        local = RAW_DIR / \"api\" / \"owm\" / f\"owm_{ts()}.csv\"\n",
    "        local.parent.mkdir(parents=True, exist_ok=True)\n",
    "        dfm.to_csv(local, index=False)\n",
    "\n",
    "        try:\n",
    "            minio_uri = minio_upload(local, f\"api/owm/{local.name}\")\n",
    "            logger.info(f\"   ‚òÅÔ∏è Upload MinIO : {minio_uri}\")\n",
    "        except Exception as e:\n",
    "            log_error(\"MinIO\", e, \"Upload fichier OWM\")\n",
    "            minio_uri = f\"local://{local}\"\n",
    "\n",
    "        # Insertion PostgreSQL\n",
    "        with engine.begin() as conn:\n",
    "            id_source = get_source_id(conn, \"OpenWeatherMap\")\n",
    "            if not id_source:\n",
    "                id_type = conn.execute(text(\"SELECT id_type_donnee FROM type_donnee WHERE libelle = 'API'\")).scalar()\n",
    "                if id_type:\n",
    "                    conn.execute(text(\"\"\"\n",
    "                        INSERT INTO source (id_type_donnee, nom, url, fiabilite)\n",
    "                        VALUES (:id_type, 'OpenWeatherMap', 'https://openweathermap.org/api', 0.9)\n",
    "                    \"\"\"), {\"id_type\": id_type})\n",
    "                    id_source = conn.execute(text(\"SELECT id_source FROM source WHERE nom = 'OpenWeatherMap'\")).scalar()\n",
    "                else:\n",
    "                    logger.warning(\"   ‚ö†Ô∏è Type 'API' non trouv√© dans type_donnee\")\n",
    "\n",
    "            if id_source:\n",
    "                id_flux = create_flux(conn, id_source, \"json\", minio_uri)\n",
    "\n",
    "                # Ins√©rer territoires et m√©t√©o\n",
    "                for _, r in dfm.iterrows():\n",
    "                    tid = ensure_territoire(conn, ville=r[\"ville\"], lat=r[\"lat\"], lon=r[\"lon\"])\n",
    "                    try:\n",
    "                        conn.execute(text(\"\"\"\n",
    "                            INSERT INTO meteo(id_territoire, date_obs, temperature, humidite, vent_kmh, pression, meteo_type)\n",
    "                            VALUES(:t, :d, :T, :H, :V, :P, :MT)\n",
    "                        \"\"\"), {\n",
    "                            \"t\": tid, \"d\": r[\"date_obs\"], \"T\": r[\"temperature\"],\n",
    "                            \"H\": r[\"humidite\"], \"V\": r[\"vent_kmh\"], \"P\": r[\"pression\"], \"MT\": r[\"meteo_type\"]\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        log_error(\"meteo\", e, f\"Insertion relev√© {r['ville']}\")\n",
    "\n",
    "                logger.info(f\"‚úÖ Source 2/5 termin√©e : {len(dfm)} relev√©s m√©t√©o ins√©r√©s\")\n",
    "            else:\n",
    "                logger.warning(\"   ‚ö†Ô∏è Source OpenWeatherMap non cr√©√©e - insertion m√©t√©o ignor√©e\")\n",
    "    else:\n",
    "        logger.warning(\"‚ö†Ô∏è Aucun relev√© m√©t√©o collect√©\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì∞ Source 3/5 : Flux RSS Multi-Sources (Presse fran√ßaise)\n",
    "\n",
    "Collecte d'articles d'actualit√© via 3 flux RSS fran√ßais compl√©mentaires.\n",
    "\n",
    "**Sources** :\n",
    "- **Franceinfo** : flux principal actualit√©s nationales\n",
    "- **20 Minutes** : actualit√©s fran√ßaises grand public\n",
    "- **Le Monde** : presse de r√©f√©rence\n",
    "\n",
    "**Extraction** : titre, description, date publication, URL source\n",
    "\n",
    "**Stockage** : PostgreSQL + MinIO\n",
    "\n",
    "**D√©duplication** : SHA256 sur (titre + description) pour √©viter doublons inter-sources\n",
    "\n",
    "**Parser** : Utilisation de `feedparser` pour robustesse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"üì∞ SOURCE 3/5 : Flux RSS Multi-Sources (Presse fran√ßaise)\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    import feedparser\n",
    "except ImportError:\n",
    "    logger.error(\"‚ùå Module feedparser manquant - install: pip install feedparser\")\n",
    "    feedparser = None\n",
    "\n",
    "if feedparser:\n",
    "    RSS_SOURCES = {\n",
    "        \"Franceinfo\": \"https://www.francetvinfo.fr/titres.rss\",\n",
    "        \"20 Minutes\": \"https://www.20minutes.fr/feeds/rss-une.xml\",\n",
    "        \"Le Monde\": \"https://www.lemonde.fr/rss/une.xml\"\n",
    "    }\n",
    "\n",
    "    all_rss_items = []\n",
    "\n",
    "    for source_name, rss_url in RSS_SOURCES.items():\n",
    "        logger.info(f\"üì° Source : {source_name}\")\n",
    "        logger.info(f\"   URL : {rss_url}\")\n",
    "\n",
    "        try:\n",
    "            feed = feedparser.parse(rss_url)\n",
    "\n",
    "            if len(feed.entries) == 0:\n",
    "                logger.warning(\"   ‚ö†Ô∏è Aucun article trouv√©\")\n",
    "                continue\n",
    "\n",
    "            source_items = []\n",
    "            for e in feed.entries[:100]:  # Max 100 par source\n",
    "                titre = e.get(\"title\", \"\").strip()\n",
    "                texte = (e.get(\"summary\", \"\") or e.get(\"description\", \"\") or \"\").strip()\n",
    "                dp = pd.to_datetime(e.get(\"published\", \"\"), errors=\"coerce\")\n",
    "                url = e.get(\"link\", \"\")\n",
    "\n",
    "                if titre and texte:\n",
    "                    source_items.append({\n",
    "                        \"titre\": titre,\n",
    "                        \"texte\": texte,\n",
    "                        \"date_publication\": dp if pd.notna(dp) else datetime.now(UTC),\n",
    "                        \"langue\": \"fr\",\n",
    "                        \"source_media\": source_name,\n",
    "                        \"url\": url\n",
    "                    })\n",
    "\n",
    "            all_rss_items.extend(source_items)\n",
    "            logger.info(f\"   ‚úÖ {len(source_items)} articles collect√©s\")\n",
    "\n",
    "        except Exception as e:\n",
    "            log_error(f\"RSS_{source_name}\", e, \"Parsing flux RSS\")\n",
    "            logger.warning(f\"   ‚ö†Ô∏è Erreur : {str(e)[:80]}\")\n",
    "\n",
    "        time.sleep(1)  # Respect rate limit\n",
    "\n",
    "    # Consolidation DataFrame\n",
    "    if len(all_rss_items) > 0:\n",
    "        dfr = pd.DataFrame(all_rss_items)\n",
    "\n",
    "        # D√©duplication inter-sources\n",
    "        dfr[\"hash_fingerprint\"] = dfr.apply(lambda row: sha256(row[\"titre\"] + \" \" + row[\"texte\"]), axis=1)\n",
    "        nb_avant = len(dfr)\n",
    "        dfr = dfr.drop_duplicates(subset=[\"hash_fingerprint\"])\n",
    "        nb_apres = len(dfr)\n",
    "\n",
    "        logger.info(f\"üßπ D√©duplication : {nb_avant} ‚Üí {nb_apres} articles uniques ({nb_avant - nb_apres} doublons supprim√©s)\")\n",
    "\n",
    "        # Distribution par source\n",
    "        logger.info(\"üìä Distribution par source :\")\n",
    "        for source in dfr[\"source_media\"].value_counts().items():\n",
    "            logger.info(f\"   {source[0]:15s} : {source[1]:3d} articles\")\n",
    "\n",
    "        # Sauvegarde locale + MinIO\n",
    "        local = RAW_DIR / \"rss\" / f\"rss_multi_sources_{ts()}.csv\"\n",
    "        local.parent.mkdir(parents=True, exist_ok=True)\n",
    "        dfr.to_csv(local, index=False)\n",
    "\n",
    "        try:\n",
    "            minio_uri = minio_upload(local, f\"rss/{local.name}\")\n",
    "            logger.info(f\"   ‚òÅÔ∏è Upload MinIO : {minio_uri}\")\n",
    "        except Exception as e:\n",
    "            log_error(\"MinIO\", e, \"Upload fichier RSS\")\n",
    "            minio_uri = f\"local://{local}\"\n",
    "\n",
    "        # Insertion PostgreSQL\n",
    "        with engine.begin() as conn:\n",
    "            id_source = get_source_id(conn, \"Flux RSS Multi-Sources\")\n",
    "            if not id_source:\n",
    "                id_type = conn.execute(text(\"SELECT id_type_donnee FROM type_donnee WHERE libelle = 'API' OR libelle = 'Web Scraping'\")).scalar()\n",
    "                if id_type:\n",
    "                    conn.execute(text(\"\"\"\n",
    "                        INSERT INTO source (id_type_donnee, nom, url, fiabilite)\n",
    "                        VALUES (:id_type, 'Flux RSS Multi-Sources', 'https://www.francetvinfo.fr/titres.rss', 0.95)\n",
    "                    \"\"\"), {\"id_type\": id_type})\n",
    "                    id_source = conn.execute(text(\"SELECT id_source FROM source WHERE nom = 'Flux RSS Multi-Sources'\")).scalar()\n",
    "\n",
    "            if id_source:\n",
    "                id_flux = create_flux(conn, id_source, \"rss\", minio_uri)\n",
    "\n",
    "                # Pr√©parer documents pour insertion batch\n",
    "                docs = []\n",
    "                for _, row in dfr.iterrows():\n",
    "                    docs.append({\n",
    "                        \"id_flux\": id_flux,\n",
    "                        \"id_territoire\": None,\n",
    "                        \"titre\": row[\"titre\"],\n",
    "                        \"texte\": row[\"texte\"],\n",
    "                        \"langue\": row[\"langue\"],\n",
    "                        \"date_publication\": row[\"date_publication\"],\n",
    "                        \"hash_fingerprint\": row[\"hash_fingerprint\"]\n",
    "                    })\n",
    "\n",
    "                inserted = insert_documents(conn, docs)\n",
    "                logger.info(f\"‚úÖ Source 3/5 termin√©e : {inserted} articles RSS ins√©r√©s\")\n",
    "            else:\n",
    "                logger.warning(\"   ‚ö†Ô∏è Source RSS non cr√©√©e - insertion ignor√©e\")\n",
    "    else:\n",
    "        logger.warning(\"‚ö†Ô∏è Aucun article RSS collect√©\")\n",
    "else:\n",
    "    logger.warning(\"‚ö†Ô∏è Module feedparser manquant - Source 3 ignor√©e\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåê Source 4/5 : Web Scraping Multi-Sources (Dry-run MonAvisCitoyen)\n",
    "\n",
    "Collecte de donn√©es citoyennes depuis sources l√©gales et √©thiques (version simplifi√©e pour E1).\n",
    "\n",
    "**Sources impl√©ment√©es (dry-run)** :\n",
    "- **Vie-publique.fr** (RSS) : Consultations citoyennes nationales\n",
    "- **data.gouv.fr** (API) : Open Data datasets CSV officiels\n",
    "\n",
    "**√âthique & L√©galit√©** :\n",
    "- ‚úÖ Open Data gouvernemental (.gouv.fr)\n",
    "- ‚úÖ Respect robots.txt\n",
    "- ‚úÖ APIs officielles uniquement\n",
    "- ‚úÖ Aucun scraping de sites priv√©s sans autorisation\n",
    "\n",
    "**Stockage** :\n",
    "- **PostgreSQL** : Documents structur√©s\n",
    "- **MinIO** : CSV bruts pour audit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"üåê SOURCE 4/5 : Web Scraping Multi-Sources (Dry-run)\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "all_scraping_data = []\n",
    "\n",
    "# ============================================================\n",
    "# SOURCE 1 : VIE-PUBLIQUE.FR (RSS)\n",
    "# ============================================================\n",
    "logger.info(\"üèõÔ∏è Source 1/2 : Vie-publique.fr (RSS)\")\n",
    "\n",
    "try:\n",
    "    if feedparser:\n",
    "        feed_url = \"https://www.vie-publique.fr/rss\"\n",
    "        feed = feedparser.parse(feed_url)\n",
    "\n",
    "        for entry in feed.entries[:50]:\n",
    "            all_scraping_data.append({\n",
    "                \"titre\": entry.get(\"title\", \"\"),\n",
    "                \"texte\": entry.get(\"summary\", entry.get(\"description\", \"\")),\n",
    "                \"source_site\": \"vie-publique.fr\",\n",
    "                \"url\": entry.get(\"link\", \"\"),\n",
    "                \"date_publication\": datetime(*entry.published_parsed[:6], tzinfo=UTC) if hasattr(entry, \"published_parsed\") else datetime.now(UTC),\n",
    "                \"langue\": \"fr\"\n",
    "            })\n",
    "\n",
    "        logger.info(f\"‚úÖ Vie-publique.fr: {len([d for d in all_scraping_data if 'vie-publique' in d['source_site']])} articles collect√©s\")\n",
    "    else:\n",
    "        logger.warning(\"   ‚ö†Ô∏è Module feedparser manquant\")\n",
    "except Exception as e:\n",
    "    log_error(\"ViePublique\", e, \"Parsing RSS feed\")\n",
    "    logger.warning(f\"   ‚ö†Ô∏è Vie-publique.fr: {str(e)[:100]} (skip)\")\n",
    "\n",
    "# ============================================================\n",
    "# SOURCE 2 : DATA.GOUV.FR (API officielle)\n",
    "# ============================================================\n",
    "logger.info(\"üìä Source 2/2 : data.gouv.fr (API officielle)\")\n",
    "\n",
    "try:\n",
    "    url = \"https://www.data.gouv.fr/api/1/datasets/\"\n",
    "    params = {\"q\": \"france\", \"page_size\": 50}\n",
    "    response = requests.get(url, params=params, timeout=10)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    data = response.json()\n",
    "    for dataset in data.get(\"data\", []):\n",
    "        all_scraping_data.append({\n",
    "            \"titre\": dataset.get(\"title\", \"\"),\n",
    "            \"texte\": dataset.get(\"description\", dataset.get(\"title\", \"\")),\n",
    "            \"source_site\": \"data.gouv.fr\",\n",
    "            \"url\": f\"https://www.data.gouv.fr/fr/datasets/{dataset.get('slug', '')}\",\n",
    "            \"date_publication\": datetime.fromisoformat(dataset.get(\"created_at\", datetime.now(UTC).isoformat()).replace(\"Z\", \"+00:00\")),\n",
    "            \"langue\": \"fr\"\n",
    "        })\n",
    "\n",
    "    logger.info(f\"‚úÖ data.gouv.fr: {len([d for d in all_scraping_data if 'data.gouv' in d['source_site']])} datasets collect√©s\")\n",
    "\n",
    "except Exception as e:\n",
    "    log_error(\"DataGouv\", e, \"Collecte datasets Open Data\")\n",
    "    logger.warning(f\"   ‚ö†Ô∏è data.gouv.fr: {str(e)[:100]} (skip)\")\n",
    "\n",
    "# ============================================================\n",
    "# CONSOLIDATION ET STORAGE\n",
    "# ============================================================\n",
    "if len(all_scraping_data) > 0:\n",
    "    df_scraping = pd.DataFrame(all_scraping_data)\n",
    "\n",
    "    # Nettoyage\n",
    "    df_scraping = df_scraping[df_scraping[\"texte\"].str.len() > 20].copy()\n",
    "    df_scraping[\"hash_fingerprint\"] = df_scraping[\"texte\"].apply(lambda t: sha256(t[:500]))\n",
    "    df_scraping = df_scraping.drop_duplicates(subset=[\"hash_fingerprint\"])\n",
    "\n",
    "    logger.info(f\"üìà Total collect√©: {len(df_scraping)} documents citoyens\")\n",
    "    logger.info(f\"   ‚Ä¢ Vie Publique: {len(df_scraping[df_scraping['source_site'].str.contains('vie-publique', na=False)])}\")\n",
    "    logger.info(f\"   ‚Ä¢ Data.gouv: {len(df_scraping[df_scraping['source_site'].str.contains('data.gouv', na=False)])}\")\n",
    "\n",
    "    # Storage MinIO\n",
    "    scraping_dir = RAW_DIR / \"scraping\" / \"multi\"\n",
    "    scraping_dir.mkdir(parents=True, exist_ok=True)\n",
    "    local = scraping_dir / f\"scraping_multi_{ts()}.csv\"\n",
    "    df_scraping.to_csv(local, index=False)\n",
    "\n",
    "    try:\n",
    "        minio_uri = minio_upload(local, f\"scraping/multi/{local.name}\")\n",
    "        logger.info(f\"   ‚òÅÔ∏è Upload MinIO : {minio_uri}\")\n",
    "    except Exception as e:\n",
    "        log_error(\"MinIO\", e, \"Upload fichier scraping\")\n",
    "        minio_uri = f\"local://{local}\"\n",
    "\n",
    "    # Storage PostgreSQL\n",
    "    with engine.begin() as conn:\n",
    "        id_source = get_source_id(conn, \"Web Scraping Multi-Sources\")\n",
    "        if not id_source:\n",
    "            id_type = conn.execute(text(\"SELECT id_type_donnee FROM type_donnee WHERE libelle = 'Web Scraping'\")).scalar()\n",
    "            if id_type:\n",
    "                conn.execute(text(\"\"\"\n",
    "                    INSERT INTO source (id_type_donnee, nom, url, fiabilite)\n",
    "                    VALUES (:id_type, 'Web Scraping Multi-Sources', 'https://www.data.gouv.fr', 0.85)\n",
    "                \"\"\"), {\"id_type\": id_type})\n",
    "                id_source = conn.execute(text(\"SELECT id_source FROM source WHERE nom = 'Web Scraping Multi-Sources'\")).scalar()\n",
    "\n",
    "        if id_source:\n",
    "            id_flux = create_flux(conn, id_source, \"html\", minio_uri)\n",
    "\n",
    "            docs = []\n",
    "            for _, row in df_scraping.iterrows():\n",
    "                docs.append({\n",
    "                    \"id_flux\": id_flux,\n",
    "                    \"id_territoire\": None,\n",
    "                    \"titre\": row[\"titre\"],\n",
    "                    \"texte\": row[\"texte\"],\n",
    "                    \"langue\": row[\"langue\"],\n",
    "                    \"date_publication\": row[\"date_publication\"],\n",
    "                    \"hash_fingerprint\": row[\"hash_fingerprint\"]\n",
    "                })\n",
    "\n",
    "            inserted = insert_documents(conn, docs)\n",
    "            logger.info(f\"‚úÖ Source 4/5 termin√©e : {inserted} documents scraping ins√©r√©s\")\n",
    "        else:\n",
    "            logger.warning(\"   ‚ö†Ô∏è Source scraping non cr√©√©e - insertion ignor√©e\")\n",
    "else:\n",
    "    logger.warning(\"‚ö†Ô∏è Aucune donn√©e collect√©e depuis les sources web scraping\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåç Source 5/5 : GDELT GKG France (Big Data)\n",
    "\n",
    "T√©l√©chargement et analyse de donn√©es Big Data depuis GDELT Project (Global Database of Events, Language, and Tone) avec **focus France**.\n",
    "\n",
    "**Source** : http://data.gdeltproject.org/gdeltv2/\n",
    "\n",
    "**Format** : GKG 2.0 (Global Knowledge Graph) - Fichiers CSV.zip (~300 MB/15min)\n",
    "\n",
    "**Contenu Big Data** :\n",
    "- √âv√©nements mondiaux g√©olocalis√©s\n",
    "- **Tonalit√© √©motionnelle** (V2Tone : -100 n√©gatif ‚Üí +100 positif)\n",
    "- **Th√®mes extraits** (V2Themes : PROTEST, HEALTH, ECONOMY, TERROR...)\n",
    "- **Entit√©s nomm√©es** (V2Persons, V2Organizations)\n",
    "- **G√©olocalisation** (V2Locations avec codes pays)\n",
    "\n",
    "**Filtrage France** :\n",
    "- S√©lection √©v√©nements avec localisation France (code pays FR)\n",
    "- Extraction tonalit√© moyenne France\n",
    "- Top th√®mes fran√ßais\n",
    "\n",
    "**Strat√©gie Big Data** :\n",
    "- T√©l√©chargement fichier derni√®res 15min (~6-300 MB brut)\n",
    "- Parsing colonnes V2* nomm√©es (27 colonnes GKG)\n",
    "- Filtrage g√©ographique France ‚Üí √©chantillon\n",
    "- Storage MinIO (fichier brut complet)\n",
    "- Insertion PostgreSQL (√©v√©nements France)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"üåç SOURCE 5/5 : GDELT GKG France (Big Data)\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "import io\n",
    "import zipfile\n",
    "\n",
    "# Colonnes GKG 2.0 (version compl√®te)\n",
    "GKG_COLUMNS = [\n",
    "    \"GKGRECORDID\", \"V2.1DATE\", \"V2SourceCollectionIdentifier\", \"V2SourceCommonName\",\n",
    "    \"V2DocumentIdentifier\", \"V1Counts\", \"V2.1Counts\", \"V1Themes\", \"V2Themes\",\n",
    "    \"V1Locations\", \"V2Locations\", \"V1Persons\", \"V2Persons\", \"V1Organizations\",\n",
    "    \"V2Organizations\", \"V1.5Tone\", \"V2.1Tone\", \"V2.1Dates\", \"V2.1Amounts\",\n",
    "    \"V2.1TransInfo\", \"V2.1Extras\", \"V21SourceLanguage\", \"V21QuotationLanguage\",\n",
    "    \"V21Url\", \"V21Date2\", \"V21Xml\"\n",
    "]\n",
    "\n",
    "# R√©cup√©rer le fichier GKG le plus r√©cent (derni√®res 15 minutes)\n",
    "try:\n",
    "    # URL du dernier update GDELT\n",
    "    update_url = \"http://data.gdeltproject.org/gdeltv2/lastupdate.txt\"\n",
    "    r = requests.get(update_url, timeout=15)\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        lines = r.text.strip().split(\"\\n\")\n",
    "        # Trouver ligne GKG (pas export ni mentions)\n",
    "        gkg_line = [line for line in lines if \".gkg.csv.zip\" in line and \"translation\" not in line]\n",
    "\n",
    "        if gkg_line:\n",
    "            # Format: size hash url\n",
    "            parts = gkg_line[0].split()\n",
    "            gkg_url = parts[2] if len(parts) >= 3 else parts[-1]\n",
    "            file_size_mb = int(parts[0]) / 1024 / 1024 if parts[0].isdigit() else 0\n",
    "\n",
    "            logger.info(f\"üì• T√©l√©chargement GDELT GKG ({file_size_mb:.1f} MB)\")\n",
    "            logger.info(f\"   URL: {gkg_url}\")\n",
    "\n",
    "            # T√©l√©charger\n",
    "            gkg_r = requests.get(gkg_url, timeout=120)\n",
    "\n",
    "            if gkg_r.status_code == 200:\n",
    "                # Sauvegarder ZIP\n",
    "                zip_filename = gkg_url.split(\"/\")[-1]\n",
    "                zip_path = RAW_DIR / \"gdelt\" / zip_filename\n",
    "                zip_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                with zip_path.open(\"wb\") as f:\n",
    "                    f.write(gkg_r.content)\n",
    "\n",
    "                logger.info(f\"   ‚úÖ T√©l√©charg√©: {zip_path.name} ({len(gkg_r.content) / 1024 / 1024:.1f} MB)\")\n",
    "\n",
    "                # Upload MinIO (fichier brut complet)\n",
    "                try:\n",
    "                    minio_uri = minio_upload(zip_path, f\"gdelt/{zip_path.name}\")\n",
    "                    logger.info(f\"   ‚òÅÔ∏è Upload MinIO : {minio_uri}\")\n",
    "                except Exception as e:\n",
    "                    log_error(\"MinIO\", e, \"Upload fichier GDELT\")\n",
    "                    minio_uri = f\"local://{zip_path}\"\n",
    "\n",
    "                # Extraction et parsing\n",
    "                with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "                    csv_filename = z.namelist()[0]\n",
    "                    logger.info(f\"\\nüìä Parsing: {csv_filename}\")\n",
    "\n",
    "                    with z.open(csv_filename) as f:\n",
    "                        # Lire avec pandas\n",
    "                        try:\n",
    "                            df_gkg = pd.read_csv(\n",
    "                                io.BytesIO(f.read()),\n",
    "                                sep=\"\\t\",\n",
    "                                header=None,\n",
    "                                names=GKG_COLUMNS,\n",
    "                                on_bad_lines=\"skip\",\n",
    "                                low_memory=False,\n",
    "                                nrows=5000  # Limiter pour d√©mo (sinon trop long)\n",
    "                            )\n",
    "\n",
    "                            logger.info(f\"   üìà Total lignes charg√©es: {len(df_gkg):,}\")\n",
    "\n",
    "                            # üá´üá∑ FILTRAGE FRANCE\n",
    "                            logger.info(\"\\nüá´üá∑ Filtrage √©v√©nements France...\")\n",
    "                            df_france = df_gkg[\n",
    "                                df_gkg[\"V2Locations\"].fillna(\"\").str.contains(\"1#France#FR#\", na=False) |\n",
    "                                df_gkg[\"V2Locations\"].fillna(\"\").str.contains(\"#FR#\", na=False)\n",
    "                            ].copy()\n",
    "\n",
    "                            logger.info(f\"   ‚úÖ √âv√©nements France: {len(df_france):,} ({len(df_france)/len(df_gkg)*100:.1f}%)\")\n",
    "\n",
    "                            if len(df_france) > 0:\n",
    "                                # Extraction tonalit√© √©motionnelle\n",
    "                                def parse_tone(tone_str):\n",
    "                                    if pd.isna(tone_str) or tone_str == \"\":\n",
    "                                        return None\n",
    "                                    try:\n",
    "                                        parts = str(tone_str).split(\",\")\n",
    "                                        return float(parts[0]) if parts else None\n",
    "                                    except Exception:\n",
    "                                        return None\n",
    "\n",
    "                                df_france[\"tone_value\"] = df_france[\"V2.1Tone\"].apply(parse_tone)\n",
    "                                avg_tone = df_france[\"tone_value\"].mean()\n",
    "\n",
    "                                logger.info(f\"üìä Tonalit√© moyenne France: {avg_tone:.2f} (-100=tr√®s n√©gatif, +100=tr√®s positif)\")\n",
    "\n",
    "                                # Insertion PostgreSQL (√©v√©nements et documents)\n",
    "                                with engine.begin() as conn:\n",
    "                                    id_source = get_source_id(conn, \"GDELT GKG\")\n",
    "                                    if not id_source:\n",
    "                                        id_type = conn.execute(text(\"SELECT id_type_donnee FROM type_donnee WHERE libelle = 'Big Data'\")).scalar()\n",
    "                                        if id_type:\n",
    "                                            conn.execute(text(\"\"\"\n",
    "                                                INSERT INTO source (id_type_donnee, nom, url, fiabilite)\n",
    "                                                VALUES (:id_type, 'GDELT GKG', 'http://data.gdeltproject.org/gdeltv2/', 0.9)\n",
    "                                            \"\"\"), {\"id_type\": id_type})\n",
    "                                            id_source = conn.execute(text(\"SELECT id_source FROM source WHERE nom = 'GDELT GKG'\")).scalar()\n",
    "\n",
    "                                    if id_source:\n",
    "                                        id_flux = create_flux(conn, id_source, \"csv\", minio_uri)\n",
    "\n",
    "                                        # Insertion √©v√©nements et documents\n",
    "                                        inserted_events = 0\n",
    "                                        inserted_docs = 0\n",
    "\n",
    "                                        for _, row in df_france.head(100).iterrows():  # Limiter √† 100 pour d√©mo\n",
    "                                            try:\n",
    "                                                # Cr√©er th√®me si n√©cessaire\n",
    "                                                themes_str = str(row[\"V2Themes\"]) if pd.notna(row[\"V2Themes\"]) else \"\"\n",
    "                                                theme_libelle = themes_str.split(\";\")[0] if themes_str else \"GENERAL\"\n",
    "\n",
    "                                                theme_id = conn.execute(text(\"\"\"\n",
    "                                                    SELECT id_theme FROM theme WHERE libelle = :libelle\n",
    "                                                \"\"\"), {\"libelle\": theme_libelle}).fetchone()\n",
    "\n",
    "                                                if not theme_id:\n",
    "                                                    conn.execute(text(\"\"\"\n",
    "                                                        INSERT INTO theme (libelle, description)\n",
    "                                                        VALUES (:libelle, :desc)\n",
    "                                                    \"\"\"), {\"libelle\": theme_libelle, \"desc\": f\"Th√®me GDELT: {theme_libelle}\"})\n",
    "                                                    theme_id = conn.execute(text(\"\"\"\n",
    "                                                        SELECT id_theme FROM theme WHERE libelle = :libelle\n",
    "                                                    \"\"\"), {\"libelle\": theme_libelle}).fetchone()\n",
    "\n",
    "                                                theme_id_val = theme_id[0] if theme_id else None\n",
    "\n",
    "                                                # Cr√©er √©v√©nement\n",
    "                                                event_result = conn.execute(text(\"\"\"\n",
    "                                                    INSERT INTO evenement (id_theme, date_event, avg_tone, source_event)\n",
    "                                                    VALUES (:theme, :date_event, :tone, :source)\n",
    "                                                    RETURNING id_event\n",
    "                                                \"\"\"), {\n",
    "                                                    \"theme\": theme_id_val,\n",
    "                                                    \"date_event\": datetime.fromtimestamp(int(str(row[\"V2.1DATE\"])[:8]), tz=UTC) if len(str(row[\"V2.1DATE\"])) >= 8 else datetime.now(UTC),\n",
    "                                                    \"tone\": avg_tone,\n",
    "                                                    \"source\": \"GDELT\"\n",
    "                                                })\n",
    "                                                event_id = event_result.scalar()\n",
    "\n",
    "                                                # Cr√©er document associ√©\n",
    "                                                doc_text = f\"{row.get('V2SourceCommonName', '')} - {themes_str[:200]}\"\n",
    "                                                doc_hash = sha256(doc_text)\n",
    "\n",
    "                                                doc_result = conn.execute(text(\"\"\"\n",
    "                                                    INSERT INTO document (id_flux, id_territoire, titre, texte, langue, date_publication, hash_fingerprint)\n",
    "                                                    VALUES (:id_flux, NULL, :titre, :texte, 'en', :date_pub, :hash)\n",
    "                                                    ON CONFLICT (hash_fingerprint) DO NOTHING\n",
    "                                                    RETURNING id_doc\n",
    "                                                \"\"\"), {\n",
    "                                                    \"id_flux\": id_flux,\n",
    "                                                    \"titre\": row.get(\"V2SourceCommonName\", \"GDELT Event\")[:200],\n",
    "                                                    \"texte\": doc_text,\n",
    "                                                    \"date_pub\": datetime.now(UTC),\n",
    "                                                    \"hash\": doc_hash\n",
    "                                                })\n",
    "                                                doc_id = doc_result.scalar()\n",
    "\n",
    "                                                if doc_id and event_id:\n",
    "                                                    # Lier document √† √©v√©nement\n",
    "                                                    conn.execute(text(\"\"\"\n",
    "                                                        INSERT INTO document_evenement (id_doc, id_event)\n",
    "                                                        VALUES (:doc_id, :event_id)\n",
    "                                                        ON CONFLICT DO NOTHING\n",
    "                                                    \"\"\"), {\"doc_id\": doc_id, \"event_id\": event_id})\n",
    "                                                    inserted_events += 1\n",
    "                                                    inserted_docs += 1\n",
    "\n",
    "                                            except Exception as e:\n",
    "                                                log_error(\"GDELT\", e, \"Insertion √©v√©nement/document\")\n",
    "\n",
    "                                        logger.info(f\"‚úÖ Source 5/5 termin√©e : {inserted_events} √©v√©nements France ins√©r√©s ({inserted_docs} docs)\")\n",
    "                                    else:\n",
    "                                        logger.warning(\"   ‚ö†Ô∏è Source GDELT non cr√©√©e - insertion ignor√©e\")\n",
    "                            else:\n",
    "                                logger.warning(\"   ‚ö†Ô∏è Aucun √©v√©nement France trouv√© dans ce fichier\")\n",
    "\n",
    "                        except Exception as e:\n",
    "                            log_error(\"GDELT\", e, \"Parsing CSV\")\n",
    "                            logger.warning(f\"   ‚ùå Erreur parsing CSV: {str(e)[:100]}\")\n",
    "                            logger.info(\"   i Fichier brut sauvegard√© sur MinIO\")\n",
    "\n",
    "            else:\n",
    "                logger.error(f\"   ‚ùå Erreur t√©l√©chargement GKG: {gkg_r.status_code}\")\n",
    "        else:\n",
    "            logger.warning(\"   ‚ö†Ô∏è Aucun fichier GKG trouv√© dans lastupdate.txt\")\n",
    "    else:\n",
    "        logger.error(f\"   ‚ùå Erreur acc√®s lastupdate.txt: {r.status_code}\")\n",
    "\n",
    "except Exception as e:\n",
    "    log_error(\"GDELT\", e, \"Collecte Big Data\")\n",
    "    logger.warning(f\"‚ùå Erreur GDELT: {str(e)[:200]}\")\n",
    "    logger.info(\"i GDELT peut √™tre temporairement indisponible (service tiers)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Cr√©ation du Manifest JSON\n",
    "\n",
    "G√©n√©ration d'un manifest JSON pour tra√ßabilit√© compl√®te de toutes les ingestions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Barom√®tres DataSens - Sources M√©tier (E2/E3)\n",
    "\n",
    "Les 5 sources de base (E1) sont compl√®tes. Pour enrichir le dataset avec des donn√©es m√©tier sp√©cialis√©es, voici **10 types de barom√®tres** √† impl√©menter dans les phases E2/E3 :\n",
    "\n",
    "### üìã Liste des Barom√®tres\n",
    "\n",
    "1. **üîπ Barom√®tre de confiance politique & sociale**\n",
    "   - **Source** : CEVIPOF ‚Äì La confiance des Fran√ßais dans la politique\n",
    "   - **Th√©matique** : Soci√©t√©, gouvernance, d√©mocratie, institutions\n",
    "   - **Format** : CSV / PDF / API\n",
    "   - **Mapping E1** : API / Fichier plat\n",
    "\n",
    "2. **üîπ Barom√®tre des √©motions et du moral des Fran√ßais**\n",
    "   - **Source** : Kantar Public / Ipsos Mood of France\n",
    "   - **Th√©matique** : Joie, anxi√©t√©, col√®re, espoir (‚Üí table EMOTION)\n",
    "   - **Format** : CSV / scraping\n",
    "   - **Mapping E1** : CSV / Web Scraping\n",
    "\n",
    "3. **üîπ Barom√®tre environnemental**\n",
    "   - **Source** : ADEME / IFOP pour la transition √©cologique\n",
    "   - **Th√©matique** : √âcologie, √©nergie, climat, sobri√©t√©\n",
    "   - **Format** : Dataset plat + API\n",
    "   - **Mapping E1** : API / CSV\n",
    "\n",
    "4. **üîπ Barom√®tre √©conomique et social**\n",
    "   - **Source** : INSEE Conjoncture + BVA Observatoire social\n",
    "   - **Th√©matique** : Pouvoir d'achat, ch√¥mage, inflation, emploi\n",
    "   - **Format** : Base SQL / CSV\n",
    "   - **Mapping E1** : Base de donn√©es / CSV\n",
    "\n",
    "5. **üîπ Barom√®tre des m√©dias et de la confiance**\n",
    "   - **Source** : La Croix ‚Äì Barom√®tre Kantar sur les m√©dias\n",
    "   - **Th√©matique** : Information, confiance m√©diatique, fake news\n",
    "   - **Format** : Web scraping\n",
    "   - **Mapping E1** : Web Scraping\n",
    "\n",
    "6. **üîπ Barom√®tre sport & coh√©sion sociale**\n",
    "   - **Source** : Minist√®re des Sports / CNOSF / Paris 2024\n",
    "   - **Th√©matique** : Sport, bien-√™tre, fiert√© nationale, coh√©sion\n",
    "   - **Format** : CSV / API\n",
    "   - **Mapping E1** : CSV / API\n",
    "\n",
    "7. **üîπ Barom√®tre des discriminations et √©galit√©**\n",
    "   - **Source** : D√©fenseur des Droits / IFOP\n",
    "   - **Th√©matique** : Inclusion, diversit√©, √©galit√© femmes-hommes\n",
    "   - **Format** : CSV / API\n",
    "   - **Mapping E1** : CSV / API\n",
    "\n",
    "8. **üîπ Barom√®tre sant√© mentale et bien-√™tre**\n",
    "   - **Source** : Sant√© Publique France ‚Äì CoviPrev\n",
    "   - **Th√©matique** : Stress, anxi√©t√©, sant√© mentale post-COVID\n",
    "   - **Format** : CSV\n",
    "   - **Mapping E1** : CSV\n",
    "\n",
    "9. **üîπ Barom√®tre climat social et tensions**\n",
    "   - **Source** : Elabe / BFMTV Opinion 2024\n",
    "   - **Th√©matique** : Col√®re, frustration, confiance, peur\n",
    "   - **Format** : Web Scraping\n",
    "   - **Mapping E1** : Web Scraping\n",
    "\n",
    "10. **üîπ Barom√®tre innovation et IA**\n",
    "    - **Source** : CNIL / France IA / Capgemini Research Institute\n",
    "    - **Th√©matique** : Adoption de l'IA, confiance num√©rique\n",
    "    - **Format** : PDF / API\n",
    "    - **Mapping E1** : API / PDF scraping\n",
    "\n",
    "### üìö Documentation Compl√®te\n",
    "\n",
    "Voir `docs/BAROMETRES_SOURCES.md` pour :\n",
    "- D√©tails par barom√®tre (URLs, format, tables PostgreSQL)\n",
    "- Plan d'impl√©mentation E2/E3\n",
    "- Notes techniques et RGPD\n",
    "\n",
    "### üéØ Plan d'Impl√©mentation\n",
    "\n",
    "**Phase E2 (Priorit√©)** :\n",
    "1. Barom√®tre √©conomique et social (INSEE)\n",
    "2. Barom√®tre des √©motions (Kantar/Ipsos)\n",
    "3. Barom√®tre sant√© mentale (Sant√© Publique France)\n",
    "\n",
    "**Phase E3 (Compl√©ment)** :\n",
    "4-10. Autres barom√®tres selon priorit√©s m√©tier\n",
    "\n",
    "**Architecture** : Tous les barom√®tres suivront le m√™me pipeline que les sources E1 :\n",
    "- Logging structur√©\n",
    "- Upload MinIO\n",
    "- Insertion PostgreSQL avec helpers\n",
    "- D√©duplication SHA-256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# SOURCES 2, 3, 4, 5 : √Ä IMPL√âMENTER AVEC VRAIES SOURCES\n",
    "# =====================================================\n",
    "#\n",
    "# Pour respecter l'architecture pipeline du notebook datasens_E1_v2.ipynb,\n",
    "# les sources 2-5 doivent √™tre impl√©ment√©es avec :\n",
    "# 1. Collecte r√©elle depuis API/BDD/Scraping/GDELT\n",
    "# 2. Upload MinIO pour tra√ßabilit√© DataLake\n",
    "# 3. Insertion PostgreSQL avec fonctions helpers (create_flux, insert_documents)\n",
    "# 4. Logging complet via logger.info/error\n",
    "#\n",
    "# Voir notebook datasens_E1_v2.ipynb pour impl√©mentations compl√®tes :\n",
    "# - Source 2 : Kaggle DB (SQLite ‚Üí Postgres via Pandas)\n",
    "# - Source 3 : OpenWeatherMap API (voir Cell 20 du notebook existant)\n",
    "# - Source 4 : Web Scraping MonAvisCitoyen (voir Cell 26 du notebook existant)\n",
    "# - Source 5 : GDELT GKG Big Data (voir Cell 28 du notebook existant)\n",
    "\n",
    "logger.info(\"\\nüìã Pour sources 2-5 : Voir notebooks/datasens_E1_v2.ipynb\")\n",
    "logger.info(\"   ‚Üí Exemples complets avec vraies API keys et collectes r√©elles\")\n",
    "\n",
    "# =====================================================\n",
    "# MANIFEST JSON (Tra√ßabilit√© finale)\n",
    "# =====================================================\n",
    "logger.info(\"üìã Cr√©ation du manifest JSON\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "# Compter les donn√©es collect√©es\n",
    "with engine.connect() as conn:\n",
    "    counts = {\n",
    "        \"documents\": conn.execute(text(\"SELECT COUNT(*) FROM document\")).scalar(),\n",
    "        \"flux\": conn.execute(text(\"SELECT COUNT(*) FROM flux\")).scalar(),\n",
    "        \"sources\": conn.execute(text(\"SELECT COUNT(*) FROM source\")).scalar(),\n",
    "        \"meteo\": conn.execute(text(\"SELECT COUNT(*) FROM meteo\")).scalar(),\n",
    "        \"evenements\": conn.execute(text(\"SELECT COUNT(*) FROM evenement\")).scalar(),\n",
    "    }\n",
    "\n",
    "manifest = {\n",
    "    \"run_id\": ts(),\n",
    "    \"timestamp_utc\": datetime.now(UTC).isoformat(),\n",
    "    \"notebook_version\": \"03_ingest_sources.ipynb\",\n",
    "    \"sources_ingested\": [\n",
    "        \"Kaggle CSV (fichier plat - 50% PG + 50% MinIO)\",\n",
    "        \"Kaggle DB (base de donn√©es - √† impl√©menter)\",\n",
    "        \"OpenWeatherMap (API - √† impl√©menter)\",\n",
    "        \"MonAvisCitoyen (scraping - √† impl√©menter)\",\n",
    "        \"GDELT GKG (big data - √† impl√©menter)\"\n",
    "    ],\n",
    "    \"counts\": counts,\n",
    "    \"postgres_db\": PG_DB,\n",
    "    \"minio_bucket\": MINIO_BUCKET,\n",
    "    \"raw_data_location\": str(RAW_DIR),\n",
    "    \"log_file\": str(log_file)\n",
    "}\n",
    "\n",
    "# Sauvegarder manifest local + MinIO\n",
    "manifest_path = MANIFESTS_DIR / f\"manifest_{manifest['run_id']}.json\"\n",
    "manifest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with manifest_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(manifest, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "try:\n",
    "    manifest_minio_uri = minio_upload(manifest_path, f\"manifests/{manifest_path.name}\")\n",
    "    logger.info(f\"‚úÖ Manifest cr√©√© : {manifest_path.name}\")\n",
    "    logger.info(f\"‚òÅÔ∏è Manifest MinIO : {manifest_minio_uri}\")\n",
    "except Exception as e:\n",
    "    log_error(\"MinIO\", e, \"Upload manifest\")\n",
    "    manifest_minio_uri = f\"local://{manifest_path}\"\n",
    "\n",
    "logger.info(\"\\nüìä R√©sum√© ingestion :\")\n",
    "for key, value in counts.items():\n",
    "    logger.info(f\"   ‚Ä¢ {key}: {value}\")\n",
    "\n",
    "logger.info(\"\\n‚úÖ Ingestion termin√©e ! (Source 1/5 compl√®te, sources 2-5 √† documenter)\")\n",
    "logger.info(\"   ‚û°Ô∏è Passez au notebook 04_crud_tests.ipynb\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}