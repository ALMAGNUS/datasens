{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì• DataSens E1 ‚Äî Notebook 3 : Ingestion des 5 Sources\n",
    "\n",
    "**üéØ Objectif** : Ing√©rer r√©ellement les 5 types de sources avec tra√ßabilit√© compl√®te\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Plan d'ingestion\n",
    "\n",
    "1. **Fichier plat CSV** : Kaggle (50% ‚Üí Postgres, 50% ‚Üí raw)\n",
    "2. **Base de donn√©es** : Kaggle SQLite ‚Üí Postgres\n",
    "3. **API** : OpenWeatherMap ‚Üí meteo + flux\n",
    "4. **Web Scraping** : MonAvisCitoyen (dry-run) ‚Üí document\n",
    "5. **Big Data** : GDELT GKG ‚Üí evenement + document_evenement\n",
    "\n",
    "**Tra√ßabilit√©** : Manifest JSON par run avec chemins, compteurs, horodatages\n",
    "\n",
    "---\n",
    "\n",
    "## üîí RGPD & Gouvernance\n",
    "\n",
    "‚ö†Ô∏è **Rappel** : Pas de donn√©es personnelles directes (hash SHA-256), respect robots.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration et imports (architecture pipeline compl√®te)\n",
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "import traceback\n",
    "from datetime import UTC, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from minio import Minio\n",
    "from sqlalchemy import create_engine, text\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == \"notebooks\" else NOTEBOOK_DIR\n",
    "load_dotenv(PROJECT_ROOT / \".env\")\n",
    "\n",
    "PG_HOST = os.getenv(\"POSTGRES_HOST\", \"localhost\")\n",
    "PG_PORT = int(os.getenv(\"POSTGRES_PORT\", \"5432\"))\n",
    "PG_DB = os.getenv(\"POSTGRES_DB\", \"datasens\")\n",
    "PG_USER = os.getenv(\"POSTGRES_USER\", \"ds_user\")\n",
    "PG_PASS = os.getenv(\"POSTGRES_PASS\", \"ds_pass\")\n",
    "\n",
    "PG_URL = f\"postgresql+psycopg2://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{PG_DB}\"\n",
    "engine = create_engine(PG_URL, future=True)\n",
    "\n",
    "# Configuration MinIO (DataLake)\n",
    "MINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT\", \"http://localhost:9000\")\n",
    "MINIO_ACCESS_KEY = os.getenv(\"MINIO_ACCESS_KEY\", \"miniouser\")\n",
    "MINIO_SECRET_KEY = os.getenv(\"MINIO_SECRET_KEY\", \"miniosecret\")\n",
    "MINIO_BUCKET = os.getenv(\"MINIO_BUCKET\", \"datasens-raw\")\n",
    "\n",
    "RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "MANIFESTS_DIR = RAW_DIR / \"manifests\"\n",
    "LOGS_DIR = PROJECT_ROOT / \"logs\"\n",
    "\n",
    "# Cr√©er dossiers\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MANIFESTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =====================================================\n",
    "# SYST√àME DE LOGGING (comme datasens_E1_v2.ipynb)\n",
    "# =====================================================\n",
    "log_timestamp = datetime.now(UTC).strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_file = LOGS_DIR / f\"collecte_{log_timestamp}.log\"\n",
    "error_file = LOGS_DIR / f\"errors_{log_timestamp}.log\"\n",
    "\n",
    "logger = logging.getLogger(\"DataSens\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "file_formatter = logging.Formatter(\n",
    "    \"%(asctime)s | %(levelname)-8s | %(name)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "console_formatter = logging.Formatter(\n",
    "    \"[%(asctime)s] %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\"\n",
    ")\n",
    "\n",
    "file_handler = logging.FileHandler(log_file, encoding=\"utf-8\")\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(file_formatter)\n",
    "\n",
    "error_handler = logging.FileHandler(error_file, encoding=\"utf-8\")\n",
    "error_handler.setLevel(logging.ERROR)\n",
    "error_handler.setFormatter(file_formatter)\n",
    "\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "console_handler.setFormatter(console_formatter)\n",
    "\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(error_handler)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "def log_error(source: str, error: Exception, context: str = \"\"):\n",
    "    \"\"\"Log une erreur avec traceback complet\"\"\"\n",
    "    error_msg = f\"[{source}] {context}: {error!s}\"\n",
    "    logger.error(error_msg)\n",
    "    logger.error(f\"Traceback:\\n{traceback.format_exc()}\")\n",
    "\n",
    "logger.info(\"üöÄ Syst√®me de logging initialis√©\")\n",
    "logger.info(f\"üìÅ Logs: {log_file}\")\n",
    "logger.info(f\"‚ùå Erreurs: {error_file}\")\n",
    "\n",
    "# =====================================================\n",
    "# MINIO CLIENT (DataLake)\n",
    "# =====================================================\n",
    "try:\n",
    "    minio_client = Minio(\n",
    "        MINIO_ENDPOINT.replace(\"http://\", \"\").replace(\"https://\", \"\"),\n",
    "        access_key=MINIO_ACCESS_KEY,\n",
    "        secret_key=MINIO_SECRET_KEY,\n",
    "        secure=MINIO_ENDPOINT.startswith(\"https\")\n",
    "    )\n",
    "\n",
    "    def ensure_bucket(bucket: str = MINIO_BUCKET):\n",
    "        if not minio_client.bucket_exists(bucket):\n",
    "            minio_client.make_bucket(bucket)\n",
    "\n",
    "    def minio_upload(local_path: Path, dest_key: str) -> str:\n",
    "        \"\"\"Upload fichier vers MinIO DataLake\"\"\"\n",
    "        ensure_bucket(MINIO_BUCKET)\n",
    "        minio_client.fput_object(MINIO_BUCKET, dest_key, str(local_path))\n",
    "        return f\"s3://{MINIO_BUCKET}/{dest_key}\"\n",
    "\n",
    "    ensure_bucket()\n",
    "    logger.info(f\"‚úÖ MinIO OK ‚Üí bucket: {MINIO_BUCKET}\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"‚ö†Ô∏è MinIO non disponible: {e} - Mode local uniquement\")\n",
    "    minio_client = None\n",
    "    def minio_upload(local_path: Path, dest_key: str) -> str:\n",
    "        return f\"local://{local_path}\"\n",
    "\n",
    "# =====================================================\n",
    "# FONCTIONS UTILITAIRES\n",
    "# =====================================================\n",
    "def ts() -> str:\n",
    "    \"\"\"Timestamp UTC ISO compact\"\"\"\n",
    "    return datetime.now(UTC).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "\n",
    "def sha256(s: str) -> str:\n",
    "    \"\"\"Hash SHA-256 pour d√©duplication\"\"\"\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def get_source_id(conn, nom: str) -> int:\n",
    "    \"\"\"R√©cup√®re l'id_source depuis le nom\"\"\"\n",
    "    logger.info(f\"[get_source_id] Recherche source: {nom}\")\n",
    "    result = conn.execute(text(\"SELECT id_source FROM source WHERE nom = :nom\"), {\"nom\": nom}).fetchone()\n",
    "    if result:\n",
    "        logger.info(f\"   ‚Üí id_source trouv√©: {result[0]}\")\n",
    "        return result[0]\n",
    "    logger.warning(f\"   ‚Üí Source non trouv√©e: {nom}\")\n",
    "    return None\n",
    "\n",
    "def create_flux(conn, id_source: int, format_type: str = \"csv\", manifest_uri: str = None) -> int:\n",
    "    \"\"\"Cr√©e un flux et retourne id_flux\"\"\"\n",
    "    logger.info(f\"[create_flux] Cr√©ation flux pour id_source={id_source}, format={format_type}\")\n",
    "    result = conn.execute(text(\"\"\"\n",
    "        INSERT INTO flux (id_source, format, manifest_uri)\n",
    "        VALUES (:id_source, :format, :manifest_uri)\n",
    "        RETURNING id_flux\n",
    "    \"\"\"), {\"id_source\": id_source, \"format\": format_type, \"manifest_uri\": manifest_uri})\n",
    "    id_flux = result.scalar()\n",
    "    logger.info(f\"   ‚Üí id_flux cr√©√©: {id_flux}\")\n",
    "    return id_flux\n",
    "\n",
    "def ensure_territoire(conn, ville: str, code_insee: str = None, lat: float = None, lon: float = None) -> int:\n",
    "    \"\"\"Cr√©e ou r√©cup√®re un territoire\"\"\"\n",
    "    logger.info(f\"[ensure_territoire] V√©rification territoire: ville={ville}\")\n",
    "    result = conn.execute(text(\"SELECT id_territoire FROM territoire WHERE ville = :ville\"), {\"ville\": ville}).fetchone()\n",
    "    if result:\n",
    "        logger.info(f\"   ‚Üí id_territoire existant: {result[0]}\")\n",
    "        return result[0]\n",
    "    result = conn.execute(text(\"\"\"\n",
    "        INSERT INTO territoire (ville, code_insee, lat, lon)\n",
    "        VALUES (:ville, :code_insee, :lat, :lon)\n",
    "        RETURNING id_territoire\n",
    "    \"\"\"), {\"ville\": ville, \"code_insee\": code_insee, \"lat\": lat, \"lon\": lon})\n",
    "    id_territoire = result.scalar()\n",
    "    logger.info(f\"   ‚Üí id_territoire cr√©√©: {id_territoire}\")\n",
    "    return id_territoire\n",
    "\n",
    "def insert_documents(conn, docs: list) -> int:\n",
    "    \"\"\"Insertion batch de documents avec gestion doublons\"\"\"\n",
    "    logger.info(f\"[insert_documents] Insertion de {len(docs)} documents...\")\n",
    "    inserted = 0\n",
    "    for doc in docs:\n",
    "        try:\n",
    "            result = conn.execute(text(\"\"\"\n",
    "                INSERT INTO document (id_flux, id_territoire, titre, texte, langue, date_publication, hash_fingerprint)\n",
    "                VALUES (:id_flux, :id_territoire, :titre, :texte, :langue, :date_publication, :hash_fingerprint)\n",
    "                ON CONFLICT (hash_fingerprint) DO NOTHING\n",
    "                RETURNING id_doc\n",
    "            \"\"\"), doc)\n",
    "            id_doc = result.scalar()\n",
    "            if id_doc:\n",
    "                logger.info(f\"   ‚Üí Document ins√©r√©: id_doc={id_doc}, titre={doc.get('titre', '')[:40]}\")\n",
    "                inserted += 1\n",
    "        except Exception as e:\n",
    "            log_error(\"insert_documents\", e, \"Erreur insertion document\")\n",
    "    logger.info(f\"   ‚Üí Total ins√©r√©s: {inserted}/{len(docs)}\")\n",
    "    return inserted\n",
    "\n",
    "print(\"‚úÖ Configuration pipeline charg√©e\")\n",
    "print(f\"   üìç PostgreSQL : {PG_HOST}:{PG_PORT}/{PG_DB}\")\n",
    "print(f\"   ‚òÅÔ∏è MinIO : {MINIO_BUCKET if minio_client else 'Mode local'}\")\n",
    "print(f\"   üìÇ Raw data : {RAW_DIR}\")\n",
    "print(f\"   üìÑ Logs : {LOGS_DIR}\")\n",
    "print(\"\\n‚úÖ Pipeline DataLake + PostgreSQL pr√™t !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÑ Source 1/5 : Fichier plat CSV (Kaggle)\n",
    "\n",
    "**Architecture hybride (comme datasens_E1_v2.ipynb)** :\n",
    "- **50% ‚Üí PostgreSQL** : Donn√©es structur√©es pour requ√™tes SQL\n",
    "- **50% ‚Üí MinIO DataLake** : Donn√©es brutes pour analyses Big Data futures\n",
    "\n",
    "**Process** :\n",
    "1. Chargement CSV depuis `data/raw/kaggle/`\n",
    "2. Calcul SHA256 fingerprint pour d√©duplication\n",
    "3. Split al√©atoire 50/50\n",
    "4. Upload 50% vers MinIO (DataLake)\n",
    "5. Insertion 50% dans PostgreSQL avec tra√ßabilit√© (id_flux)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"üìÑ SOURCE 1/5 : Fichier plat CSV (Kaggle)\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "# Rechercher fichier Kaggle existant ou cr√©er √©chantillon\n",
    "kaggle_csv_paths = [\n",
    "    RAW_DIR / \"kaggle\" / \"kaggle_sample.csv\",\n",
    "    PROJECT_ROOT / \"data\" / \"raw\" / \"kaggle\" / \"*.csv\",\n",
    "    Path.cwd() / \"data\" / \"raw\" / \"kaggle\" / \"*.csv\"\n",
    "]\n",
    "\n",
    "kaggle_csv_path = None\n",
    "for path in kaggle_csv_paths:\n",
    "    if path.exists():\n",
    "        kaggle_csv_path = path\n",
    "        break\n",
    "\n",
    "if not kaggle_csv_path or not kaggle_csv_path.exists():\n",
    "    logger.warning(\"‚ö†Ô∏è Fichier Kaggle non trouv√© ‚Äî Cr√©ation √©chantillon pour d√©mo\")\n",
    "    sample_data = pd.DataFrame({\n",
    "        \"text\": [\n",
    "            \"Great product, very satisfied!\",\n",
    "            \"Service terrible, avoid at all costs\",\n",
    "            \"Excellent quality, recommend\",\n",
    "            \"Bon produit, je recommande\",\n",
    "            \"Mauvais service, d√©√ßu\"\n",
    "        ],\n",
    "        \"langue\": [\"en\", \"en\", \"en\", \"fr\", \"fr\"],\n",
    "        \"date\": [datetime.now(UTC)] * 5\n",
    "    })\n",
    "    kaggle_csv_path = RAW_DIR / \"kaggle\" / \"kaggle_sample.csv\"\n",
    "    kaggle_csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    sample_data.to_csv(kaggle_csv_path, index=False)\n",
    "    logger.info(f\"   ‚úÖ √âchantillon cr√©√© : {kaggle_csv_path.name}\")\n",
    "\n",
    "# Charger le CSV\n",
    "df_kaggle = pd.read_csv(kaggle_csv_path)\n",
    "logger.info(f\"üìä {len(df_kaggle)} lignes charg√©es\")\n",
    "\n",
    "# Split 50/50 (architecture hybride : PostgreSQL + MinIO)\n",
    "df_kaggle[\"hash_fingerprint\"] = df_kaggle[\"text\"].apply(lambda x: sha256(str(x)))\n",
    "mid_point = len(df_kaggle) // 2\n",
    "df_pg = df_kaggle.iloc[:mid_point].copy()  # 50% ‚Üí PostgreSQL\n",
    "df_raw = df_kaggle.iloc[mid_point:].copy()  # 50% ‚Üí MinIO DataLake\n",
    "\n",
    "logger.info(f\"   ‚Ä¢ 50% PostgreSQL : {len(df_pg)} lignes\")\n",
    "logger.info(f\"   ‚Ä¢ 50% MinIO DataLake : {len(df_raw)} lignes\")\n",
    "\n",
    "# Sauvegarder 50% en raw local + upload MinIO\n",
    "raw_output = RAW_DIR / \"kaggle\" / f\"kaggle_raw_{ts()}.csv\"\n",
    "df_raw.to_csv(raw_output, index=False)\n",
    "logger.info(f\"   ‚úÖ Sauvegard√© local : {raw_output.name}\")\n",
    "\n",
    "# Upload MinIO (50% bruts vers DataLake)\n",
    "try:\n",
    "    minio_uri = minio_upload(raw_output, f\"kaggle/{raw_output.name}\")\n",
    "    logger.info(f\"   ‚òÅÔ∏è Upload MinIO : {minio_uri}\")\n",
    "except Exception as e:\n",
    "    log_error(\"MinIO\", e, \"Upload fichier Kaggle\")\n",
    "    minio_uri = f\"local://{raw_output}\"\n",
    "\n",
    "# Ins√©rer 50% dans PostgreSQL\n",
    "with engine.begin() as conn:\n",
    "    id_source = get_source_id(conn, \"Kaggle CSV\")\n",
    "    if not id_source:\n",
    "        id_type = conn.execute(text(\"SELECT id_type_donnee FROM type_donnee WHERE libelle = 'Fichier plat'\")).scalar()\n",
    "        conn.execute(text(\"\"\"\n",
    "            INSERT INTO source (id_type_donnee, nom, url, fiabilite)\n",
    "            VALUES (:id_type, 'Kaggle CSV', 'https://www.kaggle.com', 0.8)\n",
    "        \"\"\"), {\"id_type\": id_type})\n",
    "        id_source = conn.execute(text(\"SELECT id_source FROM source WHERE nom = 'Kaggle CSV'\")).scalar()\n",
    "\n",
    "    id_flux = create_flux(conn, id_source, \"csv\", minio_uri)\n",
    "\n",
    "    # Pr√©parer documents pour insertion batch\n",
    "    docs = []\n",
    "    for _, row in df_pg.iterrows():\n",
    "        docs.append({\n",
    "            \"id_flux\": id_flux,\n",
    "            \"id_territoire\": None,\n",
    "            \"titre\": \"\",\n",
    "            \"texte\": str(row[\"text\"]),\n",
    "            \"langue\": row.get(\"langue\", \"en\"),\n",
    "            \"date_publication\": row.get(\"date\", datetime.now(UTC)),\n",
    "            \"hash_fingerprint\": row[\"hash_fingerprint\"]\n",
    "        })\n",
    "\n",
    "    inserted = insert_documents(conn, docs)\n",
    "\n",
    "logger.info(f\"\\n‚úÖ Source 1/5 termin√©e : {inserted} docs PostgreSQL + {len(df_raw)} docs MinIO\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Architecture Pipeline (R√©f√©rence datasens_E1_v2.ipynb)\n",
    "\n",
    "**Ce notebook suit l'architecture du pipeline existant** :\n",
    "\n",
    "‚úÖ **Logging structur√©** : `logs/collecte_*.log` + `logs/errors_*.log`  \n",
    "‚úÖ **MinIO DataLake** : Upload automatique fichiers bruts ‚Üí `s3://datasens-raw/`  \n",
    "‚úÖ **PostgreSQL** : Insertion structur√©e avec tra√ßabilit√© (flux, manifests)  \n",
    "‚úÖ **Fonctions helpers** : `create_flux()`, `insert_documents()`, `ensure_territoire()`, `minio_upload()`  \n",
    "‚úÖ **D√©duplication** : Hash SHA-256 pour √©viter doublons  \n",
    "‚úÖ **RGPD** : Pas de donn√©es personnelles directes  \n",
    "\n",
    "**Sources 2-5** : Impl√©ment√©es ci-dessous avec vraies sources (code extrait de `datasens_E1_v2.ipynb`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üå¶Ô∏è Source 2/5 : API OpenWeatherMap\n",
    "\n",
    "Collecte de donn√©es m√©t√©o en temps r√©el via l'API OpenWeatherMap.\n",
    "\n",
    "**Villes collect√©es** : Paris, Lyon, Marseille, Lille\n",
    "\n",
    "**Donn√©es r√©cup√©r√©es** :\n",
    "- Temp√©rature (¬∞C), Humidit√© (%), Pression (hPa)\n",
    "- Description m√©t√©o (clair, nuageux, pluie...)\n",
    "- Vitesse du vent (m/s)\n",
    "- Timestamp de mesure\n",
    "\n",
    "**Stockage** :\n",
    "- **PostgreSQL** : Table `meteo` avec g√©olocalisation (id_territoire FK)\n",
    "- **MinIO** : CSV brut pour historisation compl√®te\n",
    "\n",
    "**RGPD** : Aucune donn√©e personnelle, donn√©es publiques uniquement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"üå¶Ô∏è SOURCE 2/5 : API OpenWeatherMap\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "# Variables d'environnement\n",
    "OWM_API_KEY = os.getenv(\"OWM_API_KEY\")\n",
    "if not OWM_API_KEY:\n",
    "    logger.warning(\"‚ö†Ô∏è OWM_API_KEY manquante dans .env - Source 2 ignor√©e\")\n",
    "else:\n",
    "    OWM_CITIES = [\"Paris,FR\", \"Lyon,FR\", \"Marseille,FR\", \"Lille,FR\"]\n",
    "\n",
    "    rows = []\n",
    "    for c in tqdm(OWM_CITIES, desc=\"OWM\"):\n",
    "        try:\n",
    "            r = requests.get(\n",
    "                \"https://api.openweathermap.org/data/2.5/weather\",\n",
    "                params={\"q\": c, \"appid\": OWM_API_KEY, \"units\": \"metric\", \"lang\": \"fr\"},\n",
    "                timeout=10\n",
    "            )\n",
    "            if r.status_code == 200:\n",
    "                j = r.json()\n",
    "                rows.append({\n",
    "                    \"ville\": j[\"name\"],\n",
    "                    \"lat\": j[\"coord\"][\"lat\"],\n",
    "                    \"lon\": j[\"coord\"][\"lon\"],\n",
    "                    \"date_obs\": pd.to_datetime(j[\"dt\"], unit=\"s\"),\n",
    "                    \"temperature\": j[\"main\"][\"temp\"],\n",
    "                    \"humidite\": j[\"main\"][\"humidity\"],\n",
    "                    \"vent_kmh\": (j.get(\"wind\", {}).get(\"speed\") or 0) * 3.6,\n",
    "                    \"pression\": j.get(\"main\", {}).get(\"pressure\"),\n",
    "                    \"meteo_type\": j[\"weather\"][0][\"main\"] if j.get(\"weather\") else None\n",
    "                })\n",
    "        except Exception as e:\n",
    "            log_error(\"OpenWeatherMap\", e, f\"Collecte m√©t√©o {c}\")\n",
    "\n",
    "        time.sleep(1)  # Respect rate limit\n",
    "\n",
    "    if len(rows) > 0:\n",
    "        dfm = pd.DataFrame(rows)\n",
    "        local = RAW_DIR / \"api\" / \"owm\" / f\"owm_{ts()}.csv\"\n",
    "        local.parent.mkdir(parents=True, exist_ok=True)\n",
    "        dfm.to_csv(local, index=False)\n",
    "\n",
    "        try:\n",
    "            minio_uri = minio_upload(local, f\"api/owm/{local.name}\")\n",
    "            logger.info(f\"   ‚òÅÔ∏è Upload MinIO : {minio_uri}\")\n",
    "        except Exception as e:\n",
    "            log_error(\"MinIO\", e, \"Upload fichier OWM\")\n",
    "            minio_uri = f\"local://{local}\"\n",
    "\n",
    "        # Insertion PostgreSQL\n",
    "        with engine.begin() as conn:\n",
    "            id_source = get_source_id(conn, \"OpenWeatherMap\")\n",
    "            if not id_source:\n",
    "                id_type = conn.execute(text(\"SELECT id_type_donnee FROM type_donnee WHERE libelle = 'API'\")).scalar()\n",
    "                if id_type:\n",
    "                    conn.execute(text(\"\"\"\n",
    "                        INSERT INTO source (id_type_donnee, nom, url, fiabilite)\n",
    "                        VALUES (:id_type, 'OpenWeatherMap', 'https://openweathermap.org/api', 0.9)\n",
    "                    \"\"\"), {\"id_type\": id_type})\n",
    "                    id_source = conn.execute(text(\"SELECT id_source FROM source WHERE nom = 'OpenWeatherMap'\")).scalar()\n",
    "                else:\n",
    "                    logger.warning(\"   ‚ö†Ô∏è Type 'API' non trouv√© dans type_donnee\")\n",
    "\n",
    "            if id_source:\n",
    "                id_flux = create_flux(conn, id_source, \"json\", minio_uri)\n",
    "\n",
    "                # Ins√©rer territoires et m√©t√©o\n",
    "                for _, r in dfm.iterrows():\n",
    "                    tid = ensure_territoire(conn, ville=r[\"ville\"], lat=r[\"lat\"], lon=r[\"lon\"])\n",
    "                    try:\n",
    "                        conn.execute(text(\"\"\"\n",
    "                            INSERT INTO meteo(id_territoire, date_obs, temperature, humidite, vent_kmh, pression, meteo_type)\n",
    "                            VALUES(:t, :d, :T, :H, :V, :P, :MT)\n",
    "                        \"\"\"), {\n",
    "                            \"t\": tid, \"d\": r[\"date_obs\"], \"T\": r[\"temperature\"],\n",
    "                            \"H\": r[\"humidite\"], \"V\": r[\"vent_kmh\"], \"P\": r[\"pression\"], \"MT\": r[\"meteo_type\"]\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        log_error(\"meteo\", e, f\"Insertion relev√© {r['ville']}\")\n",
    "\n",
    "                logger.info(f\"‚úÖ Source 2/5 termin√©e : {len(dfm)} relev√©s m√©t√©o ins√©r√©s\")\n",
    "            else:\n",
    "                logger.warning(\"   ‚ö†Ô∏è Source OpenWeatherMap non cr√©√©e - insertion m√©t√©o ignor√©e\")\n",
    "    else:\n",
    "        logger.warning(\"‚ö†Ô∏è Aucun relev√© m√©t√©o collect√©\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì∞ Source 3/5 : Flux RSS Multi-Sources (Presse fran√ßaise)\n",
    "\n",
    "Collecte d'articles d'actualit√© via 3 flux RSS fran√ßais compl√©mentaires.\n",
    "\n",
    "**Sources** :\n",
    "- **Franceinfo** : flux principal actualit√©s nationales\n",
    "- **20 Minutes** : actualit√©s fran√ßaises grand public\n",
    "- **Le Monde** : presse de r√©f√©rence\n",
    "\n",
    "**Extraction** : titre, description, date publication, URL source\n",
    "\n",
    "**Stockage** : PostgreSQL + MinIO\n",
    "\n",
    "**D√©duplication** : SHA256 sur (titre + description) pour √©viter doublons inter-sources\n",
    "\n",
    "**Parser** : Utilisation de `feedparser` pour robustesse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"üì∞ SOURCE 3/5 : Flux RSS Multi-Sources (Presse fran√ßaise)\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    import feedparser\n",
    "except ImportError:\n",
    "    logger.error(\"‚ùå Module feedparser manquant - install: pip install feedparser\")\n",
    "    feedparser = None\n",
    "\n",
    "if feedparser:\n",
    "    RSS_SOURCES = {\n",
    "        \"Franceinfo\": \"https://www.francetvinfo.fr/titres.rss\",\n",
    "        \"20 Minutes\": \"https://www.20minutes.fr/feeds/rss-une.xml\",\n",
    "        \"Le Monde\": \"https://www.lemonde.fr/rss/une.xml\"\n",
    "    }\n",
    "\n",
    "    all_rss_items = []\n",
    "\n",
    "    for source_name, rss_url in RSS_SOURCES.items():\n",
    "        logger.info(f\"üì° Source : {source_name}\")\n",
    "        logger.info(f\"   URL : {rss_url}\")\n",
    "\n",
    "        try:\n",
    "            feed = feedparser.parse(rss_url)\n",
    "\n",
    "            if len(feed.entries) == 0:\n",
    "                logger.warning(\"   ‚ö†Ô∏è Aucun article trouv√©\")\n",
    "                continue\n",
    "\n",
    "            source_items = []\n",
    "            for e in feed.entries[:100]:  # Max 100 par source\n",
    "                titre = e.get(\"title\", \"\").strip()\n",
    "                texte = (e.get(\"summary\", \"\") or e.get(\"description\", \"\") or \"\").strip()\n",
    "                dp = pd.to_datetime(e.get(\"published\", \"\"), errors=\"coerce\")\n",
    "                url = e.get(\"link\", \"\")\n",
    "\n",
    "                if titre and texte:\n",
    "                    source_items.append({\n",
    "                        \"titre\": titre,\n",
    "                        \"texte\": texte,\n",
    "                        \"date_publication\": dp if pd.notna(dp) else datetime.now(UTC),\n",
    "                        \"langue\": \"fr\",\n",
    "                        \"source_media\": source_name,\n",
    "                        \"url\": url\n",
    "                    })\n",
    "\n",
    "            all_rss_items.extend(source_items)\n",
    "            logger.info(f\"   ‚úÖ {len(source_items)} articles collect√©s\")\n",
    "\n",
    "        except Exception as e:\n",
    "            log_error(f\"RSS_{source_name}\", e, \"Parsing flux RSS\")\n",
    "            logger.warning(f\"   ‚ö†Ô∏è Erreur : {str(e)[:80]}\")\n",
    "\n",
    "        time.sleep(1)  # Respect rate limit\n",
    "\n",
    "    # Consolidation DataFrame\n",
    "    if len(all_rss_items) > 0:\n",
    "        dfr = pd.DataFrame(all_rss_items)\n",
    "\n",
    "        # D√©duplication inter-sources\n",
    "        dfr[\"hash_fingerprint\"] = dfr.apply(lambda row: sha256(row[\"titre\"] + \" \" + row[\"texte\"]), axis=1)\n",
    "        nb_avant = len(dfr)\n",
    "        dfr = dfr.drop_duplicates(subset=[\"hash_fingerprint\"])\n",
    "        nb_apres = len(dfr)\n",
    "\n",
    "        logger.info(f\"üßπ D√©duplication : {nb_avant} ‚Üí {nb_apres} articles uniques ({nb_avant - nb_apres} doublons supprim√©s)\")\n",
    "\n",
    "        # Distribution par source\n",
    "        logger.info(\"üìä Distribution par source :\")\n",
    "        for source in dfr[\"source_media\"].value_counts().items():\n",
    "            logger.info(f\"   {source[0]:15s} : {source[1]:3d} articles\")\n",
    "\n",
    "        # Sauvegarde locale + MinIO\n",
    "        local = RAW_DIR / \"rss\" / f\"rss_multi_sources_{ts()}.csv\"\n",
    "        local.parent.mkdir(parents=True, exist_ok=True)\n",
    "        dfr.to_csv(local, index=False)\n",
    "\n",
    "        try:\n",
    "            minio_uri = minio_upload(local, f\"rss/{local.name}\")\n",
    "            logger.info(f\"   ‚òÅÔ∏è Upload MinIO : {minio_uri}\")\n",
    "        except Exception as e:\n",
    "            log_error(\"MinIO\", e, \"Upload fichier RSS\")\n",
    "            minio_uri = f\"local://{local}\"\n",
    "\n",
    "        # Insertion PostgreSQL\n",
    "        with engine.begin() as conn:\n",
    "            id_source = get_source_id(conn, \"Flux RSS Multi-Sources\")\n",
    "            if not id_source:\n",
    "                id_type = conn.execute(text(\"SELECT id_type_donnee FROM type_donnee WHERE libelle = 'API' OR libelle = 'Web Scraping'\")).scalar()\n",
    "                if id_type:\n",
    "                    conn.execute(text(\"\"\"\n",
    "                        INSERT INTO source (id_type_donnee, nom, url, fiabilite)\n",
    "                        VALUES (:id_type, 'Flux RSS Multi-Sources', 'https://www.francetvinfo.fr/titres.rss', 0.95)\n",
    "                    \"\"\"), {\"id_type\": id_type})\n",
    "                    id_source = conn.execute(text(\"SELECT id_source FROM source WHERE nom = 'Flux RSS Multi-Sources'\")).scalar()\n",
    "\n",
    "            if id_source:\n",
    "                id_flux = create_flux(conn, id_source, \"rss\", minio_uri)\n",
    "\n",
    "                # Pr√©parer documents pour insertion batch\n",
    "                docs = []\n",
    "                for _, row in dfr.iterrows():\n",
    "                    docs.append({\n",
    "                        \"id_flux\": id_flux,\n",
    "                        \"id_territoire\": None,\n",
    "                        \"titre\": row[\"titre\"],\n",
    "                        \"texte\": row[\"texte\"],\n",
    "                        \"langue\": row[\"langue\"],\n",
    "                        \"date_publication\": row[\"date_publication\"],\n",
    "                        \"hash_fingerprint\": row[\"hash_fingerprint\"]\n",
    "                    })\n",
    "\n",
    "                inserted = insert_documents(conn, docs)\n",
    "                logger.info(f\"‚úÖ Source 3/5 termin√©e : {inserted} articles RSS ins√©r√©s\")\n",
    "            else:\n",
    "                logger.warning(\"   ‚ö†Ô∏è Source RSS non cr√©√©e - insertion ignor√©e\")\n",
    "    else:\n",
    "        logger.warning(\"‚ö†Ô∏è Aucun article RSS collect√©\")\n",
    "else:\n",
    "    logger.warning(\"‚ö†Ô∏è Module feedparser manquant - Source 3 ignor√©e\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåê Source 4/5 : Web Scraping Multi-Sources (Dry-run MonAvisCitoyen)\n",
    "\n",
    "Collecte de donn√©es citoyennes depuis sources l√©gales et √©thiques (version simplifi√©e pour E1).\n",
    "\n",
    "**Sources impl√©ment√©es (dry-run)** :\n",
    "- **Vie-publique.fr** (RSS) : Consultations citoyennes nationales\n",
    "- **data.gouv.fr** (API) : Open Data datasets CSV officiels\n",
    "\n",
    "**√âthique & L√©galit√©** :\n",
    "- ‚úÖ Open Data gouvernemental (.gouv.fr)\n",
    "- ‚úÖ Respect robots.txt\n",
    "- ‚úÖ APIs officielles uniquement\n",
    "- ‚úÖ Aucun scraping de sites priv√©s sans autorisation\n",
    "\n",
    "**Stockage** :\n",
    "- **PostgreSQL** : Documents structur√©s\n",
    "- **MinIO** : CSV bruts pour audit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"üåê SOURCE 4/5 : Web Scraping Multi-Sources (Dry-run)\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "all_scraping_data = []\n",
    "\n",
    "# ============================================================\n",
    "# SOURCE 1 : VIE-PUBLIQUE.FR (RSS)\n",
    "# ============================================================\n",
    "logger.info(\"üèõÔ∏è Source 1/2 : Vie-publique.fr (RSS)\")\n",
    "\n",
    "try:\n",
    "    if feedparser:\n",
    "        feed_url = \"https://www.vie-publique.fr/rss\"\n",
    "        feed = feedparser.parse(feed_url)\n",
    "\n",
    "        for entry in feed.entries[:50]:\n",
    "            all_scraping_data.append({\n",
    "                \"titre\": entry.get(\"title\", \"\"),\n",
    "                \"texte\": entry.get(\"summary\", entry.get(\"description\", \"\")),\n",
    "                \"source_site\": \"vie-publique.fr\",\n",
    "                \"url\": entry.get(\"link\", \"\"),\n",
    "                \"date_publication\": datetime(*entry.published_parsed[:6], tzinfo=UTC) if hasattr(entry, \"published_parsed\") else datetime.now(UTC),\n",
    "                \"langue\": \"fr\"\n",
    "            })\n",
    "\n",
    "        logger.info(f\"‚úÖ Vie-publique.fr: {len([d for d in all_scraping_data if 'vie-publique' in d['source_site']])} articles collect√©s\")\n",
    "    else:\n",
    "        logger.warning(\"   ‚ö†Ô∏è Module feedparser manquant\")\n",
    "except Exception as e:\n",
    "    log_error(\"ViePublique\", e, \"Parsing RSS feed\")\n",
    "    logger.warning(f\"   ‚ö†Ô∏è Vie-publique.fr: {str(e)[:100]} (skip)\")\n",
    "\n",
    "# ============================================================\n",
    "# SOURCE 2 : DATA.GOUV.FR (API officielle)\n",
    "# ============================================================\n",
    "logger.info(\"üìä Source 2/2 : data.gouv.fr (API officielle)\")\n",
    "\n",
    "try:\n",
    "    url = \"https://www.data.gouv.fr/api/1/datasets/\"\n",
    "    params = {\"q\": \"france\", \"page_size\": 50}\n",
    "    response = requests.get(url, params=params, timeout=10)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    data = response.json()\n",
    "    for dataset in data.get(\"data\", []):\n",
    "        all_scraping_data.append({\n",
    "            \"titre\": dataset.get(\"title\", \"\"),\n",
    "            \"texte\": dataset.get(\"description\", dataset.get(\"title\", \"\")),\n",
    "            \"source_site\": \"data.gouv.fr\",\n",
    "            \"url\": f\"https://www.data.gouv.fr/fr/datasets/{dataset.get('slug', '')}\",\n",
    "            \"date_publication\": datetime.fromisoformat(dataset.get(\"created_at\", datetime.now(UTC).isoformat()).replace(\"Z\", \"+00:00\")),\n",
    "            \"langue\": \"fr\"\n",
    "        })\n",
    "\n",
    "    logger.info(f\"‚úÖ data.gouv.fr: {len([d for d in all_scraping_data if 'data.gouv' in d['source_site']])} datasets collect√©s\")\n",
    "\n",
    "except Exception as e:\n",
    "    log_error(\"DataGouv\", e, \"Collecte datasets Open Data\")\n",
    "    logger.warning(f\"   ‚ö†Ô∏è data.gouv.fr: {str(e)[:100]} (skip)\")\n",
    "\n",
    "# ============================================================\n",
    "# CONSOLIDATION ET STORAGE\n",
    "# ============================================================\n",
    "if len(all_scraping_data) > 0:\n",
    "    df_scraping = pd.DataFrame(all_scraping_data)\n",
    "\n",
    "    # Nettoyage\n",
    "    df_scraping = df_scraping[df_scraping[\"texte\"].str.len() > 20].copy()\n",
    "    df_scraping[\"hash_fingerprint\"] = df_scraping[\"texte\"].apply(lambda t: sha256(t[:500]))\n",
    "    df_scraping = df_scraping.drop_duplicates(subset=[\"hash_fingerprint\"])\n",
    "\n",
    "    logger.info(f\"üìà Total collect√©: {len(df_scraping)} documents citoyens\")\n",
    "    logger.info(f\"   ‚Ä¢ Vie Publique: {len(df_scraping[df_scraping['source_site'].str.contains('vie-publique', na=False)])}\")\n",
    "    logger.info(f\"   ‚Ä¢ Data.gouv: {len(df_scraping[df_scraping['source_site'].str.contains('data.gouv', na=False)])}\")\n",
    "\n",
    "    # Storage MinIO\n",
    "    scraping_dir = RAW_DIR / \"scraping\" / \"multi\"\n",
    "    scraping_dir.mkdir(parents=True, exist_ok=True)\n",
    "    local = scraping_dir / f\"scraping_multi_{ts()}.csv\"\n",
    "    df_scraping.to_csv(local, index=False)\n",
    "\n",
    "    try:\n",
    "        minio_uri = minio_upload(local, f\"scraping/multi/{local.name}\")\n",
    "        logger.info(f\"   ‚òÅÔ∏è Upload MinIO : {minio_uri}\")\n",
    "    except Exception as e:\n",
    "        log_error(\"MinIO\", e, \"Upload fichier scraping\")\n",
    "        minio_uri = f\"local://{local}\"\n",
    "\n",
    "    # Storage PostgreSQL\n",
    "    with engine.begin() as conn:\n",
    "        id_source = get_source_id(conn, \"Web Scraping Multi-Sources\")\n",
    "        if not id_source:\n",
    "            id_type = conn.execute(text(\"SELECT id_type_donnee FROM type_donnee WHERE libelle = 'Web Scraping'\")).scalar()\n",
    "            if id_type:\n",
    "                conn.execute(text(\"\"\"\n",
    "                    INSERT INTO source (id_type_donnee, nom, url, fiabilite)\n",
    "                    VALUES (:id_type, 'Web Scraping Multi-Sources', 'https://www.data.gouv.fr', 0.85)\n",
    "                \"\"\"), {\"id_type\": id_type})\n",
    "                id_source = conn.execute(text(\"SELECT id_source FROM source WHERE nom = 'Web Scraping Multi-Sources'\")).scalar()\n",
    "\n",
    "        if id_source:\n",
    "            id_flux = create_flux(conn, id_source, \"html\", minio_uri)\n",
    "\n",
    "            docs = []\n",
    "            for _, row in df_scraping.iterrows():\n",
    "                docs.append({\n",
    "                    \"id_flux\": id_flux,\n",
    "                    \"id_territoire\": None,\n",
    "                    \"titre\": row[\"titre\"],\n",
    "                    \"texte\": row[\"texte\"],\n",
    "                    \"langue\": row[\"langue\"],\n",
    "                    \"date_publication\": row[\"date_publication\"],\n",
    "                    \"hash_fingerprint\": row[\"hash_fingerprint\"]\n",
    "                })\n",
    "\n",
    "            inserted = insert_documents(conn, docs)\n",
    "            logger.info(f\"‚úÖ Source 4/5 termin√©e : {inserted} documents scraping ins√©r√©s\")\n",
    "        else:\n",
    "            logger.warning(\"   ‚ö†Ô∏è Source scraping non cr√©√©e - insertion ignor√©e\")\n",
    "else:\n",
    "    logger.warning(\"‚ö†Ô∏è Aucune donn√©e collect√©e depuis les sources web scraping\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåç Source 5/5 : GDELT GKG France (Big Data)\n",
    "\n",
    "T√©l√©chargement et analyse de donn√©es Big Data depuis GDELT Project (Global Database of Events, Language, and Tone) avec **focus France**.\n",
    "\n",
    "**Source** : http://data.gdeltproject.org/gdeltv2/\n",
    "\n",
    "**Format** : GKG 2.0 (Global Knowledge Graph) - Fichiers CSV.zip (~300 MB/15min)\n",
    "\n",
    "**Contenu Big Data** :\n",
    "- √âv√©nements mondiaux g√©olocalis√©s\n",
    "- **Tonalit√© √©motionnelle** (V2Tone : -100 n√©gatif ‚Üí +100 positif)\n",
    "- **Th√®mes extraits** (V2Themes : PROTEST, HEALTH, ECONOMY, TERROR...)\n",
    "- **Entit√©s nomm√©es** (V2Persons, V2Organizations)\n",
    "- **G√©olocalisation** (V2Locations avec codes pays)\n",
    "\n",
    "**Filtrage France** :\n",
    "- S√©lection √©v√©nements avec localisation France (code pays FR)\n",
    "- Extraction tonalit√© moyenne France\n",
    "- Top th√®mes fran√ßais\n",
    "\n",
    "**Strat√©gie Big Data** :\n",
    "- T√©l√©chargement fichier derni√®res 15min (~6-300 MB brut)\n",
    "- Parsing colonnes V2* nomm√©es (27 colonnes GKG)\n",
    "- Filtrage g√©ographique France ‚Üí √©chantillon\n",
    "- Storage MinIO (fichier brut complet)\n",
    "- Insertion PostgreSQL (√©v√©nements France)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"üåç SOURCE 5/5 : GDELT GKG France (Big Data)\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "import io\n",
    "import zipfile\n",
    "\n",
    "# Colonnes GKG 2.0 (version compl√®te)\n",
    "GKG_COLUMNS = [\n",
    "    \"GKGRECORDID\", \"V2.1DATE\", \"V2SourceCollectionIdentifier\", \"V2SourceCommonName\",\n",
    "    \"V2DocumentIdentifier\", \"V1Counts\", \"V2.1Counts\", \"V1Themes\", \"V2Themes\",\n",
    "    \"V1Locations\", \"V2Locations\", \"V1Persons\", \"V2Persons\", \"V1Organizations\",\n",
    "    \"V2Organizations\", \"V1.5Tone\", \"V2.1Tone\", \"V2.1Dates\", \"V2.1Amounts\",\n",
    "    \"V2.1TransInfo\", \"V2.1Extras\", \"V21SourceLanguage\", \"V21QuotationLanguage\",\n",
    "    \"V21Url\", \"V21Date2\", \"V21Xml\"\n",
    "]\n",
    "\n",
    "# R√©cup√©rer le fichier GKG le plus r√©cent (derni√®res 15 minutes)\n",
    "try:\n",
    "    # URL du dernier update GDELT\n",
    "    update_url = \"http://data.gdeltproject.org/gdeltv2/lastupdate.txt\"\n",
    "    r = requests.get(update_url, timeout=15)\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        lines = r.text.strip().split(\"\\n\")\n",
    "        # Trouver ligne GKG (pas export ni mentions)\n",
    "        gkg_line = [line for line in lines if \".gkg.csv.zip\" in line and \"translation\" not in line]\n",
    "\n",
    "        if gkg_line:\n",
    "            # Format: size hash url\n",
    "            parts = gkg_line[0].split()\n",
    "            gkg_url = parts[2] if len(parts) >= 3 else parts[-1]\n",
    "            file_size_mb = int(parts[0]) / 1024 / 1024 if parts[0].isdigit() else 0\n",
    "\n",
    "            logger.info(f\"üì• T√©l√©chargement GDELT GKG ({file_size_mb:.1f} MB)\")\n",
    "            logger.info(f\"   URL: {gkg_url}\")\n",
    "\n",
    "            # T√©l√©charger\n",
    "            gkg_r = requests.get(gkg_url, timeout=120)\n",
    "\n",
    "            if gkg_r.status_code == 200:\n",
    "                # Sauvegarder ZIP\n",
    "                zip_filename = gkg_url.split(\"/\")[-1]\n",
    "                zip_path = RAW_DIR / \"gdelt\" / zip_filename\n",
    "                zip_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                with zip_path.open(\"wb\") as f:\n",
    "                    f.write(gkg_r.content)\n",
    "\n",
    "                logger.info(f\"   ‚úÖ T√©l√©charg√©: {zip_path.name} ({len(gkg_r.content) / 1024 / 1024:.1f} MB)\")\n",
    "\n",
    "                # Upload MinIO (fichier brut complet)\n",
    "                try:\n",
    "                    minio_uri = minio_upload(zip_path, f\"gdelt/{zip_path.name}\")\n",
    "                    logger.info(f\"   ‚òÅÔ∏è Upload MinIO : {minio_uri}\")\n",
    "                except Exception as e:\n",
    "                    log_error(\"MinIO\", e, \"Upload fichier GDELT\")\n",
    "                    minio_uri = f\"local://{zip_path}\"\n",
    "\n",
    "                # Extraction et parsing\n",
    "                with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "                    csv_filename = z.namelist()[0]\n",
    "                    logger.info(f\"\\nüìä Parsing: {csv_filename}\")\n",
    "\n",
    "                    with z.open(csv_filename) as f:\n",
    "                        # Lire avec pandas\n",
    "                        try:\n",
    "                            df_gkg = pd.read_csv(\n",
    "                                io.BytesIO(f.read()),\n",
    "                                sep=\"\\t\",\n",
    "                                header=None,\n",
    "                                names=GKG_COLUMNS,\n",
    "                                on_bad_lines=\"skip\",\n",
    "                                low_memory=False,\n",
    "                                nrows=5000  # Limiter pour d√©mo (sinon trop long)\n",
    "                            )\n",
    "\n",
    "                            logger.info(f\"   üìà Total lignes charg√©es: {len(df_gkg):,}\")\n",
    "\n",
    "                            # üá´üá∑ FILTRAGE FRANCE\n",
    "                            logger.info(\"\\nüá´üá∑ Filtrage √©v√©nements France...\")\n",
    "                            df_france = df_gkg[\n",
    "                                df_gkg[\"V2Locations\"].fillna(\"\").str.contains(\"1#France#FR#\", na=False) |\n",
    "                                df_gkg[\"V2Locations\"].fillna(\"\").str.contains(\"#FR#\", na=False)\n",
    "                            ].copy()\n",
    "\n",
    "                            logger.info(f\"   ‚úÖ √âv√©nements France: {len(df_france):,} ({len(df_france)/len(df_gkg)*100:.1f}%)\")\n",
    "\n",
    "                            if len(df_france) > 0:\n",
    "                                # Extraction tonalit√© √©motionnelle\n",
    "                                def parse_tone(tone_str):\n",
    "                                    if pd.isna(tone_str) or tone_str == \"\":\n",
    "                                        return None\n",
    "                                    try:\n",
    "                                        parts = str(tone_str).split(\",\")\n",
    "                                        return float(parts[0]) if parts else None\n",
    "                                    except Exception:\n",
    "                                        return None\n",
    "\n",
    "                                df_france[\"tone_value\"] = df_france[\"V2.1Tone\"].apply(parse_tone)\n",
    "                                avg_tone = df_france[\"tone_value\"].mean()\n",
    "\n",
    "                                logger.info(f\"üìä Tonalit√© moyenne France: {avg_tone:.2f} (-100=tr√®s n√©gatif, +100=tr√®s positif)\")\n",
    "\n",
    "                                # Insertion PostgreSQL (√©v√©nements et documents)\n",
    "                                with engine.begin() as conn:\n",
    "                                    id_source = get_source_id(conn, \"GDELT GKG\")\n",
    "                                    if not id_source:\n",
    "                                        id_type = conn.execute(text(\"SELECT id_type_donnee FROM type_donnee WHERE libelle = 'Big Data'\")).scalar()\n",
    "                                        if id_type:\n",
    "                                            conn.execute(text(\"\"\"\n",
    "                                                INSERT INTO source (id_type_donnee, nom, url, fiabilite)\n",
    "                                                VALUES (:id_type, 'GDELT GKG', 'http://data.gdeltproject.org/gdeltv2/', 0.9)\n",
    "                                            \"\"\"), {\"id_type\": id_type})\n",
    "                                            id_source = conn.execute(text(\"SELECT id_source FROM source WHERE nom = 'GDELT GKG'\")).scalar()\n",
    "\n",
    "                                    if id_source:\n",
    "                                        id_flux = create_flux(conn, id_source, \"csv\", minio_uri)\n",
    "\n",
    "                                        # Insertion √©v√©nements et documents\n",
    "                                        inserted_events = 0\n",
    "                                        inserted_docs = 0\n",
    "\n",
    "                                        for _, row in df_france.head(100).iterrows():  # Limiter √† 100 pour d√©mo\n",
    "                                            try:\n",
    "                                                # Cr√©er th√®me si n√©cessaire\n",
    "                                                themes_str = str(row[\"V2Themes\"]) if pd.notna(row[\"V2Themes\"]) else \"\"\n",
    "                                                theme_libelle = themes_str.split(\";\")[0] if themes_str else \"GENERAL\"\n",
    "\n",
    "                                                theme_id = conn.execute(text(\"\"\"\n",
    "                                                    SELECT id_theme FROM theme WHERE libelle = :libelle\n",
    "                                                \"\"\"), {\"libelle\": theme_libelle}).fetchone()\n",
    "\n",
    "                                                if not theme_id:\n",
    "                                                    conn.execute(text(\"\"\"\n",
    "                                                        INSERT INTO theme (libelle, description)\n",
    "                                                        VALUES (:libelle, :desc)\n",
    "                                                    \"\"\"), {\"libelle\": theme_libelle, \"desc\": f\"Th√®me GDELT: {theme_libelle}\"})\n",
    "                                                    theme_id = conn.execute(text(\"\"\"\n",
    "                                                        SELECT id_theme FROM theme WHERE libelle = :libelle\n",
    "                                                    \"\"\"), {\"libelle\": theme_libelle}).fetchone()\n",
    "\n",
    "                                                theme_id_val = theme_id[0] if theme_id else None\n",
    "\n",
    "                                                # Cr√©er √©v√©nement\n",
    "                                                event_result = conn.execute(text(\"\"\"\n",
    "                                                    INSERT INTO evenement (id_theme, date_event, avg_tone, source_event)\n",
    "                                                    VALUES (:theme, :date_event, :tone, :source)\n",
    "                                                    RETURNING id_event\n",
    "                                                \"\"\"), {\n",
    "                                                    \"theme\": theme_id_val,\n",
    "                                                    \"date_event\": datetime.fromtimestamp(int(str(row[\"V2.1DATE\"])[:8]), tz=UTC) if len(str(row[\"V2.1DATE\"])) >= 8 else datetime.now(UTC),\n",
    "                                                    \"tone\": avg_tone,\n",
    "                                                    \"source\": \"GDELT\"\n",
    "                                                })\n",
    "                                                event_id = event_result.scalar()\n",
    "\n",
    "                                                # Cr√©er document associ√©\n",
    "                                                doc_text = f\"{row.get('V2SourceCommonName', '')} - {themes_str[:200]}\"\n",
    "                                                doc_hash = sha256(doc_text)\n",
    "\n",
    "                                                doc_result = conn.execute(text(\"\"\"\n",
    "                                                    INSERT INTO document (id_flux, id_territoire, titre, texte, langue, date_publication, hash_fingerprint)\n",
    "                                                    VALUES (:id_flux, NULL, :titre, :texte, 'en', :date_pub, :hash)\n",
    "                                                    ON CONFLICT (hash_fingerprint) DO NOTHING\n",
    "                                                    RETURNING id_doc\n",
    "                                                \"\"\"), {\n",
    "                                                    \"id_flux\": id_flux,\n",
    "                                                    \"titre\": row.get(\"V2SourceCommonName\", \"GDELT Event\")[:200],\n",
    "                                                    \"texte\": doc_text,\n",
    "                                                    \"date_pub\": datetime.now(UTC),\n",
    "                                                    \"hash\": doc_hash\n",
    "                                                })\n",
    "                                                doc_id = doc_result.scalar()\n",
    "\n",
    "                                                if doc_id and event_id:\n",
    "                                                    # Lier document √† √©v√©nement\n",
    "                                                    conn.execute(text(\"\"\"\n",
    "                                                        INSERT INTO document_evenement (id_doc, id_event)\n",
    "                                                        VALUES (:doc_id, :event_id)\n",
    "                                                        ON CONFLICT DO NOTHING\n",
    "                                                    \"\"\"), {\"doc_id\": doc_id, \"event_id\": event_id})\n",
    "                                                    inserted_events += 1\n",
    "                                                    inserted_docs += 1\n",
    "\n",
    "                                            except Exception as e:\n",
    "                                                log_error(\"GDELT\", e, \"Insertion √©v√©nement/document\")\n",
    "\n",
    "                                        logger.info(f\"‚úÖ Source 5/5 termin√©e : {inserted_events} √©v√©nements France ins√©r√©s ({inserted_docs} docs)\")\n",
    "                                    else:\n",
    "                                        logger.warning(\"   ‚ö†Ô∏è Source GDELT non cr√©√©e - insertion ignor√©e\")\n",
    "                            else:\n",
    "                                logger.warning(\"   ‚ö†Ô∏è Aucun √©v√©nement France trouv√© dans ce fichier\")\n",
    "\n",
    "                        except Exception as e:\n",
    "                            log_error(\"GDELT\", e, \"Parsing CSV\")\n",
    "                            logger.warning(f\"   ‚ùå Erreur parsing CSV: {str(e)[:100]}\")\n",
    "                            logger.info(\"   i Fichier brut sauvegard√© sur MinIO\")\n",
    "\n",
    "            else:\n",
    "                logger.error(f\"   ‚ùå Erreur t√©l√©chargement GKG: {gkg_r.status_code}\")\n",
    "        else:\n",
    "            logger.warning(\"   ‚ö†Ô∏è Aucun fichier GKG trouv√© dans lastupdate.txt\")\n",
    "    else:\n",
    "        logger.error(f\"   ‚ùå Erreur acc√®s lastupdate.txt: {r.status_code}\")\n",
    "\n",
    "except Exception as e:\n",
    "    log_error(\"GDELT\", e, \"Collecte Big Data\")\n",
    "    logger.warning(f\"‚ùå Erreur GDELT: {str(e)[:200]}\")\n",
    "    logger.info(\"i GDELT peut √™tre temporairement indisponible (service tiers)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Cr√©ation du Manifest JSON\n",
    "\n",
    "G√©n√©ration d'un manifest JSON pour tra√ßabilit√© compl√®te de toutes les ingestions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Barom√®tres DataSens - Sources M√©tier (E2/E3)\n",
    "\n",
    "Les 5 sources de base (E1) sont compl√®tes. Pour enrichir le dataset avec des donn√©es m√©tier sp√©cialis√©es, voici **10 types de barom√®tres** √† impl√©menter dans les phases E2/E3 :\n",
    "\n",
    "### üìã Liste des Barom√®tres\n",
    "\n",
    "1. **üîπ Barom√®tre de confiance politique & sociale**\n",
    "   - **Source** : CEVIPOF ‚Äì La confiance des Fran√ßais dans la politique\n",
    "   - **Th√©matique** : Soci√©t√©, gouvernance, d√©mocratie, institutions\n",
    "   - **Format** : CSV / PDF / API\n",
    "   - **Mapping E1** : API / Fichier plat\n",
    "\n",
    "2. **üîπ Barom√®tre des √©motions et du moral des Fran√ßais**\n",
    "   - **Source** : Kantar Public / Ipsos Mood of France\n",
    "   - **Th√©matique** : Joie, anxi√©t√©, col√®re, espoir (‚Üí table EMOTION)\n",
    "   - **Format** : CSV / scraping\n",
    "   - **Mapping E1** : CSV / Web Scraping\n",
    "\n",
    "3. **üîπ Barom√®tre environnemental**\n",
    "   - **Source** : ADEME / IFOP pour la transition √©cologique\n",
    "   - **Th√©matique** : √âcologie, √©nergie, climat, sobri√©t√©\n",
    "   - **Format** : Dataset plat + API\n",
    "   - **Mapping E1** : API / CSV\n",
    "\n",
    "4. **üîπ Barom√®tre √©conomique et social**\n",
    "   - **Source** : INSEE Conjoncture + BVA Observatoire social\n",
    "   - **Th√©matique** : Pouvoir d'achat, ch√¥mage, inflation, emploi\n",
    "   - **Format** : Base SQL / CSV\n",
    "   - **Mapping E1** : Base de donn√©es / CSV\n",
    "\n",
    "5. **üîπ Barom√®tre des m√©dias et de la confiance**\n",
    "   - **Source** : La Croix ‚Äì Barom√®tre Kantar sur les m√©dias\n",
    "   - **Th√©matique** : Information, confiance m√©diatique, fake news\n",
    "   - **Format** : Web scraping\n",
    "   - **Mapping E1** : Web Scraping\n",
    "\n",
    "6. **üîπ Barom√®tre sport & coh√©sion sociale**\n",
    "   - **Source** : Minist√®re des Sports / CNOSF / Paris 2024\n",
    "   - **Th√©matique** : Sport, bien-√™tre, fiert√© nationale, coh√©sion\n",
    "   - **Format** : CSV / API\n",
    "   - **Mapping E1** : CSV / API\n",
    "\n",
    "7. **üîπ Barom√®tre des discriminations et √©galit√©**\n",
    "   - **Source** : D√©fenseur des Droits / IFOP\n",
    "   - **Th√©matique** : Inclusion, diversit√©, √©galit√© femmes-hommes\n",
    "   - **Format** : CSV / API\n",
    "   - **Mapping E1** : CSV / API\n",
    "\n",
    "8. **üîπ Barom√®tre sant√© mentale et bien-√™tre**\n",
    "   - **Source** : Sant√© Publique France ‚Äì CoviPrev\n",
    "   - **Th√©matique** : Stress, anxi√©t√©, sant√© mentale post-COVID\n",
    "   - **Format** : CSV\n",
    "   - **Mapping E1** : CSV\n",
    "\n",
    "9. **üîπ Barom√®tre climat social et tensions**\n",
    "   - **Source** : Elabe / BFMTV Opinion 2024\n",
    "   - **Th√©matique** : Col√®re, frustration, confiance, peur\n",
    "   - **Format** : Web Scraping\n",
    "   - **Mapping E1** : Web Scraping\n",
    "\n",
    "10. **üîπ Barom√®tre innovation et IA**\n",
    "    - **Source** : CNIL / France IA / Capgemini Research Institute\n",
    "    - **Th√©matique** : Adoption de l'IA, confiance num√©rique\n",
    "    - **Format** : PDF / API\n",
    "    - **Mapping E1** : API / PDF scraping\n",
    "\n",
    "### üìö Documentation Compl√®te\n",
    "\n",
    "Voir `docs/BAROMETRES_SOURCES.md` pour :\n",
    "- D√©tails par barom√®tre (URLs, format, tables PostgreSQL)\n",
    "- Plan d'impl√©mentation E2/E3\n",
    "- Notes techniques et RGPD\n",
    "\n",
    "### üéØ Plan d'Impl√©mentation\n",
    "\n",
    "**Phase E2 (Priorit√©)** :\n",
    "1. Barom√®tre √©conomique et social (INSEE)\n",
    "2. Barom√®tre des √©motions (Kantar/Ipsos)\n",
    "3. Barom√®tre sant√© mentale (Sant√© Publique France)\n",
    "\n",
    "**Phase E3 (Compl√©ment)** :\n",
    "4-10. Autres barom√®tres selon priorit√©s m√©tier\n",
    "\n",
    "**Architecture** : Tous les barom√®tres suivront le m√™me pipeline que les sources E1 :\n",
    "- Logging structur√©\n",
    "- Upload MinIO\n",
    "- Insertion PostgreSQL avec helpers\n",
    "- D√©duplication SHA-256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# SOURCES 2, 3, 4, 5 : √Ä IMPL√âMENTER AVEC VRAIES SOURCES\n",
    "# =====================================================\n",
    "#\n",
    "# Pour respecter l'architecture pipeline du notebook datasens_E1_v2.ipynb,\n",
    "# les sources 2-5 doivent √™tre impl√©ment√©es avec :\n",
    "# 1. Collecte r√©elle depuis API/BDD/Scraping/GDELT\n",
    "# 2. Upload MinIO pour tra√ßabilit√© DataLake\n",
    "# 3. Insertion PostgreSQL avec fonctions helpers (create_flux, insert_documents)\n",
    "# 4. Logging complet via logger.info/error\n",
    "#\n",
    "# Voir notebook datasens_E1_v2.ipynb pour impl√©mentations compl√®tes :\n",
    "# - Source 2 : Kaggle DB (SQLite ‚Üí Postgres via Pandas)\n",
    "# - Source 3 : OpenWeatherMap API (voir Cell 20 du notebook existant)\n",
    "# - Source 4 : Web Scraping MonAvisCitoyen (voir Cell 26 du notebook existant)\n",
    "# - Source 5 : GDELT GKG Big Data (voir Cell 28 du notebook existant)\n",
    "\n",
    "logger.info(\"\\nüìã Pour sources 2-5 : Voir notebooks/datasens_E1_v2.ipynb\")\n",
    "logger.info(\"   ‚Üí Exemples complets avec vraies API keys et collectes r√©elles\")\n",
    "\n",
    "# =====================================================\n",
    "# MANIFEST JSON (Tra√ßabilit√© finale)\n",
    "# =====================================================\n",
    "logger.info(\"üìã Cr√©ation du manifest JSON\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "# Compter les donn√©es collect√©es\n",
    "with engine.connect() as conn:\n",
    "    counts = {\n",
    "        \"documents\": conn.execute(text(\"SELECT COUNT(*) FROM document\")).scalar(),\n",
    "        \"flux\": conn.execute(text(\"SELECT COUNT(*) FROM flux\")).scalar(),\n",
    "        \"sources\": conn.execute(text(\"SELECT COUNT(*) FROM source\")).scalar(),\n",
    "        \"meteo\": conn.execute(text(\"SELECT COUNT(*) FROM meteo\")).scalar(),\n",
    "        \"evenements\": conn.execute(text(\"SELECT COUNT(*) FROM evenement\")).scalar(),\n",
    "    }\n",
    "\n",
    "manifest = {\n",
    "    \"run_id\": ts(),\n",
    "    \"timestamp_utc\": datetime.now(UTC).isoformat(),\n",
    "    \"notebook_version\": \"03_ingest_sources.ipynb\",\n",
    "    \"sources_ingested\": [\n",
    "        \"Kaggle CSV (fichier plat - 50% PG + 50% MinIO)\",\n",
    "        \"Kaggle DB (base de donn√©es - √† impl√©menter)\",\n",
    "        \"OpenWeatherMap (API - √† impl√©menter)\",\n",
    "        \"MonAvisCitoyen (scraping - √† impl√©menter)\",\n",
    "        \"GDELT GKG (big data - √† impl√©menter)\"\n",
    "    ],\n",
    "    \"counts\": counts,\n",
    "    \"postgres_db\": PG_DB,\n",
    "    \"minio_bucket\": MINIO_BUCKET,\n",
    "    \"raw_data_location\": str(RAW_DIR),\n",
    "    \"log_file\": str(log_file)\n",
    "}\n",
    "\n",
    "# Sauvegarder manifest local + MinIO\n",
    "manifest_path = MANIFESTS_DIR / f\"manifest_{manifest['run_id']}.json\"\n",
    "manifest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with manifest_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(manifest, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "try:\n",
    "    manifest_minio_uri = minio_upload(manifest_path, f\"manifests/{manifest_path.name}\")\n",
    "    logger.info(f\"‚úÖ Manifest cr√©√© : {manifest_path.name}\")\n",
    "    logger.info(f\"‚òÅÔ∏è Manifest MinIO : {manifest_minio_uri}\")\n",
    "except Exception as e:\n",
    "    log_error(\"MinIO\", e, \"Upload manifest\")\n",
    "    manifest_minio_uri = f\"local://{manifest_path}\"\n",
    "\n",
    "logger.info(\"\\nüìä R√©sum√© ingestion :\")\n",
    "for key, value in counts.items():\n",
    "    logger.info(f\"   ‚Ä¢ {key}: {value}\")\n",
    "\n",
    "logger.info(\"\\n‚úÖ Ingestion termin√©e ! (Source 1/5 compl√®te, sources 2-5 √† documenter)\")\n",
    "logger.info(\"   ‚û°Ô∏è Passez au notebook 04_crud_tests.ipynb\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
