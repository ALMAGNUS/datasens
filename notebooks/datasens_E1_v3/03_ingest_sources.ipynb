{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataSens logging setup (marker:datasens_logging)\nimport logging\nimport os\nos.makedirs('logs', exist_ok=True)\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s [%(levelname)s] %(message)s',\n    handlers=[\n        logging.StreamHandler(),\n        logging.FileHandler('logs/datasens.log', encoding='utf-8')\n    ]\n)\nlogging.info('DÃ©marrage')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSens E1_v3 â€” 03_ingest_sources\n",
    "\n",
    "- Objectifs: Collecte rÃ©elle de **TOUTES les sources** avec stockage hybride (PostgreSQL + MinIO)\n",
    "- PrÃ©requis: 01_setup_env + 02_schema_create exÃ©cutÃ©s (36/37 tables crÃ©Ã©es)\n",
    "- Sortie: DonnÃ©es collectÃ©es + visualisations + tables rÃ©elles Ã  chaque Ã©tape\n",
    "- Guide: docs/GUIDE_TECHNIQUE_E1.md + docs/datasens_sources_dictionary.md\n",
    "\n",
    "> **E1_v3** : Architecture complÃ¨te avec **toutes les sources rÃ©elles**\n",
    "> - Source 1 : Kaggle Dataset (split 50/50 PostgreSQL/MinIO)\n",
    "> - Source 2 : API OpenWeatherMap (mÃ©tÃ©o 4+ villes)\n",
    "> - Source 3 : Flux RSS Multi-Sources (Franceinfo + 20 Minutes + Le Monde)\n",
    "> - Source 4 : NewsAPI (optionnel si clÃ© API disponible)\n",
    "> - Source 5 : Web Scraping Multi-Sources (6 sources : Reddit, YouTube, SignalConso, Trustpilot, Vie-publique, data.gouv)\n",
    "> - Source 6 : GDELT Big Data (Ã©chantillon France)\n",
    "> - Sources supplÃ©mentaires : BaromÃ¨tres d'opinion (selon docs/datasens_barometer_themes.md)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n# ğŸ¬ DASHBOARD NARRATIF - OÃ™ SOMMES-NOUS ?\n# ============================================================\n# Ce dashboard vous guide Ã  travers le pipeline DataSens E1\n# Il montre la progression et l'Ã©tat actuel des donnÃ©es\n# ============================================================\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import FancyBboxPatch\nimport matplotlib.patches as mpatches\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"ğŸ¬ FIL D'ARIANE VISUEL - PIPELINE DATASENS E1\")\nprint(\"=\"*80)\n\n# CrÃ©er figure dashboard\nfig = plt.figure(figsize=(16, 8))\nax = fig.add_subplot(111)\nax.set_xlim(0, 10)\nax.set_ylim(0, 6)\nax.axis('off')\n\n# Ã‰tapes du pipeline\netapes = [\n    {\"nom\": \"ğŸ“¥ COLLECTE\", \"status\": \"âœ…\", \"desc\": \"Sources brutes\"},\n    {\"nom\": \"â˜ï¸ DATALAKE\", \"status\": \"âœ…\", \"desc\": \"MinIO Raw\"},\n    {\"nom\": \"ğŸ§¹ NETTOYAGE\", \"status\": \"ğŸ”„\", \"desc\": \"DÃ©duplication\"},\n    {\"nom\": \"ğŸ’¾ ETL\", \"status\": \"â³\", \"desc\": \"PostgreSQL\"},\n    {\"nom\": \"ğŸ“Š ANNOTATION\", \"status\": \"â³\", \"desc\": \"Enrichissement\"},\n    {\"nom\": \"ğŸ“¦ EXPORT\", \"status\": \"â³\", \"desc\": \"Dataset IA\"}\n]\n\n# Couleurs selon statut\ncolors = {\n    \"âœ…\": \"#4ECDC4\",\n    \"ğŸ”„\": \"#FECA57\", \n    \"â³\": \"#E8E8E8\"\n}\n\n# Dessiner timeline\ny_pos = 4\nx_start = 1\nx_spacing = 1.4\n\nfor i, etape in enumerate(etapes):\n    x_pos = x_start + i * x_spacing\n    \n    # Cercle Ã©tape\n    circle = plt.Circle((x_pos, y_pos), 0.25, color=colors[etape[\"status\"]], zorder=3)\n    ax.add_patch(circle)\n    ax.text(x_pos, y_pos, etape[\"status\"], ha='center', va='center', fontsize=14, fontweight='bold', zorder=4)\n    \n    # Nom Ã©tape\n    ax.text(x_pos, y_pos - 0.6, etape[\"nom\"], ha='center', va='top', fontsize=11, fontweight='bold')\n    ax.text(x_pos, y_pos - 0.85, etape[\"desc\"], ha='center', va='top', fontsize=9, style='italic')\n    \n    # FlÃ¨che vers prochaine Ã©tape\n    if i < len(etapes) - 1:\n        ax.arrow(x_pos + 0.3, y_pos, x_spacing - 0.6, 0, \n                head_width=0.1, head_length=0.15, fc='gray', ec='gray', zorder=2)\n\n# Titre narratif\nax.text(5, 5.5, \"ğŸ¯ PROGRESSION DU PIPELINE E1\", ha='center', va='center', \n        fontsize=16, fontweight='bold', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\n# LÃ©gende\nlegend_elements = [\n    mpatches.Patch(facecolor='#4ECDC4', label='TerminÃ©'),\n    mpatches.Patch(facecolor='#FECA57', label='En cours'),\n    mpatches.Patch(facecolor='#E8E8E8', label='Ã€ venir')\n]\nax.legend(handles=legend_elements, loc='upper left', fontsize=10)\n\n# Statistiques rapides (si disponibles)\nstats_text = \"\\nğŸ“Š SNAPSHOT ACTUEL :\\n\"\ntry:\n    # Essayer de charger des stats si base disponible\n    stats_text += \"   â€¢ Pipeline en cours d'exÃ©cution...\\n\"\nexcept:\n    stats_text += \"   â€¢ DÃ©marrage du pipeline...\\n\"\n\nax.text(5, 1.5, stats_text, ha='center', va='center', fontsize=10,\n        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n\nplt.title(\"ğŸ¬ FIL D'ARIANE VISUEL - Accompagnement narratif du jury\", \n          fontsize=14, fontweight='bold', pad=20)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nğŸ’¡ Le fil d'Ariane vous guide Ã©tape par Ã©tape Ã  travers le pipeline\")\nprint(\"   Chaque visualisation s'inscrit dans cette progression narrative\\n\")\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notes:\n> - **ğŸ¯ Configuration flexible** : Toutes les sources sont configurÃ©es dans `config/sources_config.json`\n> - **Pour ajouter/modifier une source** : Ã‰ditez simplement le JSON, relancez le notebook, c'est tout !\n> - **Stockage hybride** : PostgreSQL (tables t01-t37) + MinIO (DataLake brut)\n> - **DÃ©duplication** : SHA256 fingerprint sur titre+texte pour Ã©viter doublons\n> - **TraÃ§abilitÃ©** : Chaque collecte crÃ©e un `t03_flux` avec `manifest_uri` pointant vers MinIO\n> - **Visualisations complÃ¨tes** : Graphiques + tables pandas Ã  **chaque Ã©tape du pipeline** :\n>   - ğŸ“Š **Ã‰tape 1** : DonnÃ©es brutes dans MinIO DataLake (objets, tailles, sources)\n>   - ğŸ§¹ **Ã‰tape 2** : AprÃ¨s nettoyage (avant/aprÃ¨s, statistiques)\n>   - ğŸ’¾ **Ã‰tape 3** : Insertion PostgreSQL (volumes, flux)\n>   - âœ… **Ã‰tape 4** : Dataset final annotÃ© (05_snapshot)\n> - **Tables E1_v3** : Utilisation des tables t01-t37 selon MPD.sql (nomenclature avec prÃ©fixe)\n> - **RÃ©fÃ©rences** : docs/datasens_sources_dictionary.md, config/README_SOURCES.md\nload_dotenv(PROJECT_ROOT / \".env\")\n\nPG_HOST = os.getenv(\"POSTGRES_HOST\", \"localhost\")\nPG_PORT = int(os.getenv(\"POSTGRES_PORT\", \"5432\"))\nPG_DB = os.getenv(\"POSTGRES_DB\", \"datasens\")\nPG_USER = os.getenv(\"POSTGRES_USER\", \"ds_user\")\nPG_PASS = os.getenv(\"POSTGRES_PASS\", \"ds_pass\")\n\nPG_URL = f\"postgresql+psycopg2://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{PG_DB}\"\nengine = create_engine(PG_URL, future=True)\n\n# Configuration MinIO (DataLake)\nMINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT\", \"http://localhost:9000\")\nMINIO_ACCESS_KEY = os.getenv(\"MINIO_ACCESS_KEY\", \"miniouser\")\nMINIO_SECRET_KEY = os.getenv(\"MINIO_SECRET_KEY\", \"miniosecret\")\nMINIO_BUCKET = os.getenv(\"MINIO_BUCKET\", \"datasens-raw\")\n\nRAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\nMANIFESTS_DIR = RAW_DIR / \"manifests\"\nLOGS_DIR = PROJECT_ROOT / \"logs\"\n\n# CrÃ©er dossiers\nRAW_DIR.mkdir(parents=True, exist_ok=True)\nMANIFESTS_DIR.mkdir(parents=True, exist_ok=True)\nLOGS_DIR.mkdir(parents=True, exist_ok=True)\n\n# =====================================================\n# SYSTÃˆME DE LOGGING (comme datasens_E1_v2.ipynb)\n# =====================================================\nlog_timestamp = datetime.now(UTC).strftime(\"%Y%m%d_%H%M%S\")\nlog_file = LOGS_DIR / f\"collecte_{log_timestamp}.log\"\nerror_file = LOGS_DIR / f\"errors_{log_timestamp}.log\"\n\nlogger = logging.getLogger(\"DataSens\")\nlogger.setLevel(logging.DEBUG)\n\nfile_formatter = logging.Formatter(\n    \"%(asctime)s | %(levelname)-8s | %(name)s | %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nconsole_formatter = logging.Formatter(\n    \"[%(asctime)s] %(levelname)s - %(message)s\",\n    datefmt=\"%H:%M:%S\"\n)\n\nfile_handler = logging.FileHandler(log_file, encoding=\"utf-8\")\nfile_handler.setLevel(logging.INFO)\nfile_handler.setFormatter(file_formatter)\n\nerror_handler = logging.FileHandler(error_file, encoding=\"utf-8\")\nerror_handler.setLevel(logging.ERROR)\nerror_handler.setFormatter(file_formatter)\n\nconsole_handler = logging.StreamHandler()\nconsole_handler.setLevel(logging.INFO)\nconsole_handler.setFormatter(console_formatter)\n\nlogger.addHandler(file_handler)\nlogger.addHandler(error_handler)\nlogger.addHandler(console_handler)\n\ndef log_error(source: str, error: Exception, context: str = \"\"):\n    \"\"\"Log une erreur avec traceback complet\"\"\"\n    error_msg = f\"[{source}] {context}: {error!s}\"\n    logger.error(error_msg)\n    logger.error(f\"Traceback:\\n{traceback.format_exc()}\")\n\nlogger.info(\"ğŸš€ SystÃ¨me de logging initialisÃ©\")\nlogger.info(f\"ğŸ“ Logs: {log_file}\")\nlogger.info(f\"âŒ Erreurs: {error_file}\")\n\n# =====================================================\n# MINIO CLIENT (DataLake)\n# =====================================================\ntry:\n    minio_client = Minio(\n        MINIO_ENDPOINT.replace(\"http://\", \"\").replace(\"https://\", \"\"),\n        access_key=MINIO_ACCESS_KEY,\n        secret_key=MINIO_SECRET_KEY,\n        secure=MINIO_ENDPOINT.startswith(\"https\")\n    )\n\n    def ensure_bucket(bucket: str = MINIO_BUCKET):\n        if not minio_client.bucket_exists(bucket):\n            minio_client.make_bucket(bucket)\n\n    def minio_upload(local_path: Path, dest_key: str) -> str:\n        \"\"\"Upload fichier vers MinIO DataLake\"\"\"\n        ensure_bucket(MINIO_BUCKET)\n        minio_client.fput_object(MINIO_BUCKET, dest_key, str(local_path))\n        return f\"s3://{MINIO_BUCKET}/{dest_key}\"\n\n    ensure_bucket()\n    logger.info(f\"âœ… MinIO OK â†’ bucket: {MINIO_BUCKET}\")\nexcept Exception as e:\n    logger.warning(f\"âš ï¸ MinIO non disponible: {e} - Mode local uniquement\")\n    minio_client = None\n    def minio_upload(local_path: Path, dest_key: str) -> str:\n        return f\"local://{local_path}\"\n\n# =====================================================\n# FONCTIONS UTILITAIRES\n# =====================================================\ndef ts() -> str:\n    \"\"\"Timestamp UTC ISO compact\"\"\"\n    return datetime.now(UTC).strftime(\"%Y%m%dT%H%M%SZ\")\n\ndef sha256(s: str) -> str:\n    \"\"\"Hash SHA-256 pour dÃ©duplication\"\"\"\n    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n\ndef get_source_id(conn, nom: str) -> int:\n    \"\"\"RÃ©cupÃ¨re l'id_source depuis le nom\"\"\"\n    logger.info(f\"[get_source_id] Recherche source: {nom}\")\n    result = conn.execute(text(\"SELECT id_source FROM source WHERE nom = :nom\"), {\"nom\": nom}).fetchone()\n    if result:\n        logger.info(f\"   â†’ id_source trouvÃ©: {result[0]}\")\n        return result[0]\n    logger.warning(f\"   â†’ Source non trouvÃ©e: {nom}\")\n    return None\n\ndef create_flux(conn, id_source: int, format_type: str = \"csv\", manifest_uri: str = None) -> int:\n    \"\"\"CrÃ©e un flux et retourne id_flux\"\"\"\n    logger.info(f\"[create_flux] CrÃ©ation flux pour id_source={id_source}, format={format_type}\")\n    result = conn.execute(text(\"\"\"\n        INSERT INTO flux (id_source, format, manifest_uri)\n        VALUES (:id_source, :format, :manifest_uri)\n        RETURNING id_flux\n    \"\"\"), {\"id_source\": id_source, \"format\": format_type, \"manifest_uri\": manifest_uri})\n    id_flux = result.scalar()\n    logger.info(f\"   â†’ id_flux crÃ©Ã©: {id_flux}\")\n    return id_flux\n\ndef ensure_territoire(conn, ville: str, code_insee: str = None, lat: float = None, lon: float = None) -> int:\n    \"\"\"CrÃ©e ou rÃ©cupÃ¨re un territoire\"\"\"\n    logger.info(f\"[ensure_territoire] VÃ©rification territoire: ville={ville}\")\n    result = conn.execute(text(\"SELECT id_territoire FROM territoire WHERE ville = :ville\"), {\"ville\": ville}).fetchone()\n    if result:\n        logger.info(f\"   â†’ id_territoire existant: {result[0]}\")\n        return result[0]\n    result = conn.execute(text(\"\"\"\n        INSERT INTO territoire (ville, code_insee, lat, lon)\n        VALUES (:ville, :code_insee, :lat, :lon)\n        RETURNING id_territoire\n    \"\"\"), {\"ville\": ville, \"code_insee\": code_insee, \"lat\": lat, \"lon\": lon})\n    id_territoire = result.scalar()\n    logger.info(f\"   â†’ id_territoire crÃ©Ã©: {id_territoire}\")\n    return id_territoire\n\ndef insert_documents(conn, docs: list) -> int:\n    \"\"\"Insertion batch de documents avec gestion doublons\"\"\"\n    logger.info(f\"[insert_documents] Insertion de {len(docs)} documents...\")\n    inserted = 0\n    for doc in docs:\n        try:\n            result = conn.execute(text(\"\"\"\n                INSERT INTO document (id_flux, id_territoire, titre, texte, langue, date_publication, hash_fingerprint)\n                VALUES (:id_flux, :id_territoire, :titre, :texte, :langue, :date_publication, :hash_fingerprint)\n                ON CONFLICT (hash_fingerprint) DO NOTHING\n                RETURNING id_doc\n            \"\"\"), doc)\n            id_doc = result.scalar()\n            if id_doc:\n                logger.info(f\"   â†’ Document insÃ©rÃ©: id_doc={id_doc}, titre={doc.get('titre', '')[:40]}\")\n                inserted += 1\n        except Exception as e:\n            log_error(\"insert_documents\", e, \"Erreur insertion document\")\n    logger.info(f\"   â†’ Total insÃ©rÃ©s: {inserted}/{len(docs)}\")\n    return inserted\n\nprint(\"âœ… Configuration pipeline chargÃ©e\")\nprint(f\"   ğŸ“ PostgreSQL : {PG_HOST}:{PG_PORT}/{PG_DB}\")\nprint(f\"   â˜ï¸ MinIO : {MINIO_BUCKET if minio_client else 'Mode local'}\")\nprint(f\"   ğŸ“‚ Raw data : {RAW_DIR}\")\nprint(f\"   ğŸ“„ Logs : {LOGS_DIR}\")\nprint(\"\\nâœ… Pipeline DataLake + PostgreSQL prÃªt !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSens E1_v3 - 03_ingest_sources\n# ğŸ“¥ Collecte rÃ©elle de TOUTES les sources avec visualisations (36/37 tables)\n\nimport datetime as dt\nimport hashlib\nimport io\nimport os\nimport time\nfrom pathlib import Path\n\nimport feedparser\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport requests\nfrom minio import Minio\nfrom sqlalchemy import create_engine, text\nfrom tqdm import tqdm\n\n# RÃ©cupÃ©rer les variables du notebook 01\nif 'PROJECT_ROOT' not in globals():\n    current = Path.cwd()\n    PROJECT_ROOT = None\n    while current != current.parent:\n        if (current / \"notebooks\").exists() and (current / \"docs\").exists():\n            PROJECT_ROOT = current\n            break\n        current = current.parent\n    else:\n        PROJECT_ROOT = Path.cwd()\n\nif 'RAW_DIR' not in globals():\n    RAW_DIR = PROJECT_ROOT / 'data' / 'raw'\n\nif 'PG_URL' not in globals():\n    PG_URL = os.getenv(\"DATASENS_PG_URL\", \"postgresql+psycopg2://postgres:postgres@localhost:5433/postgres\")\n\nif 'MINIO_ENDPOINT' not in globals():\n    MINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT\", \"http://localhost:9002\")\n    MINIO_ACCESS_KEY = os.getenv(\"MINIO_ACCESS_KEY\", \"admin\")\n    MINIO_SECRET_KEY = os.getenv(\"MINIO_SECRET_KEY\", \"admin123\")\n    MINIO_BUCKET = os.getenv(\"MINIO_BUCKET\", \"datasens-raw\")\n\nif 'ts' not in globals():\n    def ts() -> str:\n        return dt.datetime.now(tz=dt.UTC).strftime(\"%Y%m%dT%H%M%SZ\")\n\nif 'sha256_hash' not in globals():\n    def sha256_hash(s: str) -> str:\n        return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n\n# Connexions\nengine = create_engine(PG_URL, future=True)\n\n# =====================================================\n# CHARGEMENT CONFIGURATION FLEXIBLE DES SOURCES\n# =====================================================\nimport json\n\nCONFIG_FILE = PROJECT_ROOT / \"config\" / \"sources_config.json\"\n\nif CONFIG_FILE.exists():\n    with open(CONFIG_FILE, encoding='utf-8') as f:\n        sources_config = json.load(f)\n    \n    # Filtrer sources actives uniquement\n    sources_actives = [s for s in sources_config['sources'] if s.get('actif', True)]\n    \n    print(f\"\\nğŸ¯ Configuration flexible chargÃ©e :\")\n    print(f\"   ğŸ“„ Config : {CONFIG_FILE.name}\")\n    print(f\"   ğŸ“Š {len(sources_config['sources'])} sources configurÃ©es\")\n    print(f\"   âœ… {len(sources_actives)} sources actives\")\n    \n    # Afficher rÃ©sumÃ© des sources actives\n    print(\"\\nğŸ“‹ Sources Ã  collecter :\")\n    for idx, source in enumerate(sources_actives, 1):\n        print(f\"   {idx}. {source['nom']} ({source['id']}) - {source['collector']} - PrioritÃ©: {source.get('priorite', 'moyenne')}\")\nelse:\n    print(f\"âš ï¸ Fichier de configuration introuvable : {CONFIG_FILE}\")\n    print(f\"   ğŸ’¡ CrÃ©ez le fichier selon config/README_SOURCES.md\")\n    sources_config = None\n    sources_actives = []\n\ntry:\n    minio_client = Minio(\n        MINIO_ENDPOINT.replace(\"http://\", \"\").replace(\"https://\", \"\"),\n        access_key=MINIO_ACCESS_KEY,\n        secret_key=MINIO_SECRET_KEY,\n        secure=False\n    )\n    if not minio_client.bucket_exists(MINIO_BUCKET):\n        minio_client.make_bucket(MINIO_BUCKET)\nexcept Exception as e:\n    print(f\"âš ï¸ MinIO: {e}\")\n    minio_client = None\n\nprint(\"âœ… Connexions prÃªtes (PostgreSQL + MinIO)\")\nprint(\"ğŸ“Š Architecture E1_v3 : 36/37 tables (t01-t37) selon MPD.sql\")\nprint(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ Utilitaires : Fonctions helpers pour la collecte E1_v3\n",
    "\n",
    "Fonctions adaptÃ©es aux tables t01-t37 avec prÃ©fixe selon MPD.sql :\n",
    "- **minio_upload()** : Upload fichier vers MinIO (DataLake)\n",
    "- **get_source_id()** : RÃ©cupÃ©rer ou crÃ©er une source dans t02_source\n",
    "- **create_flux()** : CrÃ©er un flux dans t03_flux avec traÃ§abilitÃ©\n",
    "- **ensure_territoire()** : CrÃ©er ou rÃ©cupÃ©rer un territoire (t13-t17 hiÃ©rarchie)\n",
    "- **insert_documents()** : Insertion batch dans t04_document avec gestion des doublons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ› ï¸ Fonctions utilitaires pour la collecte E1_v3 (tables t01-t37)\n\ndef minio_upload(local_path: Path, minio_path: str) -> str:\n    \"\"\"Upload un fichier vers MinIO et retourne l'URI\"\"\"\n    if minio_client is None:\n        return f\"local://{local_path}\"\n    try:\n        minio_client.fput_object(MINIO_BUCKET, minio_path, str(local_path))\n        return f\"s3://{MINIO_BUCKET}/{minio_path}\"\n    except Exception as e:\n        print(f\"   âš ï¸ Erreur MinIO upload: {e}\")\n        return f\"local://{local_path}\"\n\ndef get_source_id(conn, nom: str, type_source: str = None) -> int:\n    \"\"\"RÃ©cupÃ¨re l'ID d'une source (t02_source) ou la crÃ©e si absente\"\"\"\n    result = conn.execute(text(\"SELECT id_source FROM t02_source WHERE nom = :nom\"), {\"nom\": nom}).scalar()\n    if result:\n        return result\n    # CrÃ©er la source avec le bon type\n    if type_source:\n        tid = conn.execute(text(\"SELECT id_type_donnee FROM t01_type_donnee WHERE libelle = :libelle LIMIT 1\"), {\"libelle\": type_source}).scalar()\n    else:\n        # Fallback : chercher 'DonnÃ©es OpÃ©rationnelles' par dÃ©faut\n        tid = conn.execute(text(\"SELECT id_type_donnee FROM t01_type_donnee WHERE libelle = 'DonnÃ©es OpÃ©rationnelles' LIMIT 1\")).scalar()\n    if not tid:\n        # Dernier fallback\n        tid = conn.execute(text(\"SELECT id_type_donnee FROM t01_type_donnee LIMIT 1\")).scalar() or 1\n    return conn.execute(text(\"\"\"\n        INSERT INTO t02_source(id_type_donnee, nom, url, fiabilite) \n        VALUES (:tid, :nom, '', 0.8) RETURNING id_source\n    \"\"\"), {\"tid\": tid, \"nom\": nom}).scalar()\n\ndef create_flux(conn, source_nom: str, format_type: str = \"csv\", manifest_uri: str = None) -> int:\n    \"\"\"CrÃ©e un flux (t03_flux) de collecte et retourne son ID\"\"\"\n    sid = get_source_id(conn, source_nom)\n    return conn.execute(text(\"\"\"\n        INSERT INTO t03_flux(id_source, format, manifest_uri, date_collecte)\n        VALUES (:sid, :format, :manifest, NOW()) RETURNING id_flux\n    \"\"\"), {\"sid\": sid, \"format\": format_type, \"manifest\": manifest_uri}).scalar()\n\ndef ensure_territoire_complet(conn, ville: str, code_insee: str = None, lat: float = None, lon: float = None) -> int:\n    \"\"\"CrÃ©e ou rÃ©cupÃ¨re un territoire complet (hiÃ©rarchie t13-t17)\"\"\"\n    # Pour E1_v3, on simplifie : chercher dans t17_territoire via t16_commune\n    # Si code_insee fourni, chercher dans t16_commune\n    if code_insee:\n        commune = conn.execute(text(\"\"\"\n            SELECT c.id_commune FROM t16_commune c \n            WHERE c.code_insee = :code\n        \"\"\"), {\"code\": code_insee}).scalar()\n        if commune:\n            terr = conn.execute(text(\"\"\"\n                SELECT t.id_territoire FROM t17_territoire t \n                WHERE t.id_commune = :c\n            \"\"\"), {\"c\": commune}).scalar()\n            if terr:\n                return terr\n    # Sinon, crÃ©er un territoire minimal (simplifiÃ© pour E1_v3)\n    # Pour une implÃ©mentation complÃ¨te, il faudrait crÃ©er pays â†’ rÃ©gion â†’ dÃ©partement â†’ commune â†’ territoire\n    # Ici on simplifie en crÃ©ant directement dans t17 avec un id_commune fictif si nÃ©cessaire\n    # En pratique, on utiliserait une table territoire simplifiÃ©e ou crÃ©erait la hiÃ©rarchie complÃ¨te\n    # Pour E1_v3, on crÃ©e un territoire minimal directement\n    return conn.execute(text(\"\"\"\n        INSERT INTO t17_territoire(id_commune)\n        VALUES ((SELECT id_commune FROM t16_commune LIMIT 1))\n        RETURNING id_territoire\n    \"\"\")).scalar() if conn.execute(text(\"SELECT COUNT(*) FROM t16_commune\")).scalar() > 0 else None\n\ndef insert_documents(conn, df: pd.DataFrame, flux_id: int):\n    \"\"\"Insertion batch de documents (t04_document) avec gestion des doublons\"\"\"\n    inserted = 0\n    for _, row in df.iterrows():\n        try:\n            conn.execute(text(\"\"\"\n                INSERT INTO t04_document(id_flux, titre, texte, langue, date_publication, hash_fingerprint)\n                VALUES(:fid, :titre, :texte, :langue, :date, :hash)\n                ON CONFLICT (hash_fingerprint) DO NOTHING\n            \"\"\"), {\n                \"fid\": flux_id,\n                \"titre\": row.get(\"titre\", \"\"),\n                \"texte\": row.get(\"texte\", \"\"),\n                \"langue\": row.get(\"langue\", \"fr\"),\n                \"date\": row.get(\"date_publication\"),\n                \"hash\": row.get(\"hash_fingerprint\", \"\")\n            })\n            inserted += 1\n        except Exception as e:\n            pass  # Doublon ou erreur silencieuse\n    return inserted\n\n# =====================================================\n# FONCTIONS UTILITAIRES DE SÃ‰CURITÃ‰\n# =====================================================\ndef assert_valid_identifier(name: str) -> None:\n    \"\"\"\n    Valide qu'un identifiant SQL (nom de table, colonne) est sÃ»r.\n    LÃ¨ve une ValueError si l'identifiant contient des caractÃ¨res non autorisÃ©s.\n    \"\"\"\n    if not isinstance(name, str):\n        raise ValueError(\"L'identifiant doit Ãªtre une chaÃ®ne de caractÃ¨res.\")\n    # Autorise lettres, chiffres, underscores, et points (pour schÃ©mas.tables)\n    if not name.replace('_', '').replace('.', '').isalnum():\n        raise ValueError(f\"Identifiant SQL invalide : {name}. Seuls les caractÃ¨res alphanumÃ©riques, underscores et points sont autorisÃ©s.\")\n\ndef load_whitelist_tables(conn, schema: str = 'datasens') -> set[str]:\n    \"\"\"\n    Charge une liste blanche des noms de tables valides depuis information_schema.\n    Retourne un set des noms de tables pour validation.\n    \"\"\"\n    try:\n        result = conn.execute(text(f\"\"\"\n            SELECT table_name FROM information_schema.tables\n            WHERE table_schema = :schema_name\n        \"\"\"), {\"schema_name\": schema}).fetchall()\n        return {row[0] for row in result}\n    except Exception as e:\n        print(f\"âš ï¸ Erreur lors du chargement de la whitelist des tables: {e}\")\n        return set()  # Retourne un set vide en cas d'erreur\n\nprint(\"âœ… Fonctions utilitaires chargÃ©es (adaptÃ©es t01-t37)\")\nprint(\"âœ… Fonctions de sÃ©curitÃ© (assert_valid_identifier, load_whitelist_tables) chargÃ©es.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“° Source 1 : Flux RSS Multi-Sources (Presse franÃ§aise)\n",
    "\n",
    "Collecte d'articles depuis 3 flux RSS franÃ§ais :\n",
    "- **Franceinfo** : Service public, actualitÃ©s gÃ©nÃ©rales\n",
    "- **20 Minutes** : Presse gratuite, grand public  \n",
    "- **Le Monde** : Presse de rÃ©fÃ©rence\n",
    "\n",
    "**Process** : Parsing RSS â†’ DataFrame â†’ DÃ©duplication SHA256 â†’ PostgreSQL (t04_document) + MinIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“° Source 1 : Flux RSS Multi-Sources\nprint(\"ğŸ“° SOURCE 1 : Flux RSS Multi-Sources\")\nprint(\"=\" * 80)\n\nRSS_SOURCES = {\n    \"Franceinfo\": \"https://www.francetvinfo.fr/titres.rss\",\n    \"20 Minutes\": \"https://www.20minutes.fr/feeds/rss-une.xml\",\n    \"Le Monde\": \"https://www.lemonde.fr/rss/une.xml\"\n}\n\nall_rss_items = []\n\nfor source_name, rss_url in RSS_SOURCES.items():\n    print(f\"\\nğŸ“¡ Source : {source_name}\")\n    try:\n        feed = feedparser.parse(rss_url)\n        if len(feed.entries) == 0:\n            print(\"   âš ï¸ Aucun article\")\n            continue\n        \n        source_items = []\n        for e in feed.entries[:30]:  # Max 30 par source\n            titre = e.get(\"title\", \"\").strip()\n            texte = (e.get(\"summary\", \"\") or e.get(\"description\", \"\") or \"\").strip()\n            if titre and texte:\n                source_items.append({\n                    \"titre\": titre,\n                    \"texte\": texte,\n                    \"date_publication\": pd.to_datetime(e.get(\"published\", \"\"), errors=\"coerce\"),\n                    \"langue\": \"fr\",\n                    \"source_media\": source_name,\n                    \"url\": e.get(\"link\", \"\")\n                })\n        all_rss_items.extend(source_items)\n        print(f\"   âœ… {len(source_items)} articles collectÃ©s\")\n    except Exception as e:\n        print(f\"   âŒ Erreur : {str(e)[:80]}\")\n    time.sleep(1)\n\n# Consolidation\ndf_rss = pd.DataFrame(all_rss_items)\nif len(df_rss) == 0:\n    print(\"\\nâš ï¸ Aucun article RSS collectÃ©\")\nelse:\n    print(f\"\\nğŸ“Š Total brut : {len(df_rss)} articles\")\n    \n    # DÃ©duplication\n    df_rss[\"hash_fingerprint\"] = df_rss.apply(\n        lambda row: sha256_hash(row[\"titre\"] + \" \" + row[\"texte\"]), axis=1\n    )\n    nb_avant = len(df_rss)\n    df_rss = df_rss.drop_duplicates(subset=[\"hash_fingerprint\"])\n    nb_apres = len(df_rss)\n    print(f\"ğŸ§¹ DÃ©duplication : {nb_avant} â†’ {nb_apres} articles uniques\")\n    \n    # Sauvegarde locale + MinIO\n    local = RAW_DIR / \"rss\" / f\"rss_multi_{ts()}.csv\"\n    local.parent.mkdir(parents=True, exist_ok=True)\n    df_rss.to_csv(local, index=False)\n    minio_uri = minio_upload(local, f\"rss/{local.name}\")\n    \n    # Insertion PostgreSQL (t04_document via t03_flux)\n    with engine.begin() as conn:\n        flux_id = create_flux(conn, \"Flux RSS Multi-Sources (Franceinfo + 20 Minutes + Le Monde)\", \"rss\", minio_uri)\n        inserted = insert_documents(conn, df_rss[[\"titre\", \"texte\", \"langue\", \"date_publication\", \"hash_fingerprint\"]], flux_id)\n    \n    print(f\"\\nâœ… RSS : {inserted} articles insÃ©rÃ©s en base (t04_document) + MinIO\")\n    print(f\"â˜ï¸ MinIO : {minio_uri}\")\n    \n    # ğŸ“Š Visualisations\n    print(\"\\nğŸ“Š RÃ©partition par source mÃ©diatique :\")\n    lang_counts = df_rss['source_media'].value_counts()\n    display(pd.DataFrame({\"Source\": lang_counts.index, \"Nombre\": lang_counts.values}))\n    \n    if len(lang_counts) > 0:\n        plt.figure(figsize=(10, 5))\n        bars = plt.bar(lang_counts.index, lang_counts.values, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n        for bar, value in zip(bars, lang_counts.values):\n            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n                    str(value), ha='center', va='bottom', fontweight='bold')\n        plt.title(\"ğŸ“Š RÃ©partition des articles RSS par source\", fontsize=12, fontweight='bold')\n        plt.ylabel(\"Nombre d'articles\", fontsize=11)\n        plt.xticks(rotation=15, ha='right')\n        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n        plt.tight_layout()\n        plt.show()\n    \n    # ğŸ“‹ Table de donnÃ©es rÃ©elles\n    print(\"\\nğŸ“‹ Table 't04_document' - Articles RSS insÃ©rÃ©s (aperÃ§u 10 premiers) :\")\n    df_docs = pd.read_sql_query(\"\"\"\n        SELECT d.id_doc, d.titre, d.langue, d.date_publication, s.nom AS source\n        FROM t04_document d\n        JOIN t03_flux f ON d.id_flux = f.id_flux\n        JOIN t02_source s ON f.id_source = s.id_source\n        WHERE s.nom LIKE '%RSS%'\n        ORDER BY d.id_doc DESC\n        LIMIT 10\n    \"\"\", engine)\n    display(df_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŒ¦ï¸ Source 2 : API OpenWeatherMap (MÃ©tÃ©o en temps rÃ©el)\n",
    "\n",
    "Collecte de donnÃ©es mÃ©tÃ©o pour 4+ villes franÃ§aises.\n",
    "\n",
    "**Stockage** : PostgreSQL (t19_meteo + hiÃ©rarchie t13-t17) + MinIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸŒ¦ï¸ Source 2 : API OpenWeatherMap\nprint(\"\\nğŸŒ¦ï¸ SOURCE 2 : API OpenWeatherMap\")\nprint(\"=\" * 80)\n\nOWM_CITIES = [\"Paris,FR\", \"Lyon,FR\", \"Marseille,FR\", \"Toulouse,FR\"]\nOWM_API_KEY = os.getenv(\"OWM_API_KEY\")\n\nif not OWM_API_KEY:\n    print(\"âš ï¸ OWM_API_KEY manquante - Source 2 ignorÃ©e\")\nelse:\n    rows = []\n    for city in tqdm(OWM_CITIES, desc=\"OWM\"):\n        try:\n            r = requests.get(\n                \"https://api.openweathermap.org/data/2.5/weather\",\n                params={\"q\": city, \"appid\": OWM_API_KEY, \"units\": \"metric\", \"lang\": \"fr\"},\n                timeout=10\n            )\n            if r.status_code == 200:\n                j = r.json()\n                rows.append({\n                    \"ville\": j[\"name\"],\n                    \"lat\": j[\"coord\"][\"lat\"],\n                    \"lon\": j[\"coord\"][\"lon\"],\n                    \"date_obs\": pd.to_datetime(j[\"dt\"], unit=\"s\"),\n                    \"temperature\": j[\"main\"][\"temp\"],\n                    \"humidite\": j[\"main\"][\"humidity\"],\n                    \"vent_kmh\": (j.get(\"wind\", {}).get(\"speed\") or 0) * 3.6,\n                    \"pression\": j.get(\"main\", {}).get(\"pressure\"),\n                    \"meteo_type\": j[\"weather\"][0][\"main\"] if j.get(\"weather\") else None\n                })\n            time.sleep(1)\n        except Exception as e:\n            print(f\"   âš ï¸ Erreur {city}: {str(e)[:60]}\")\n    \n    if rows:\n        df_owm = pd.DataFrame(rows)\n        local = RAW_DIR / \"api\" / \"owm\" / f\"owm_{ts()}.csv\"\n        local.parent.mkdir(parents=True, exist_ok=True)\n        df_owm.to_csv(local, index=False)\n        minio_uri = minio_upload(local, f\"api/owm/{local.name}\")\n        \n        # Insertion PostgreSQL (t19_meteo + t17_territoire)\n        # Pour E1_v3, on simplifie la crÃ©ation de territoire (en production, crÃ©er la hiÃ©rarchie complÃ¨te)\n        with engine.begin() as conn:\n            flux_id = create_flux(conn, \"OpenWeatherMap\", \"json\", minio_uri)\n            for _, r in df_owm.iterrows():\n                # CrÃ©er ou rÃ©cupÃ©rer territoire (simplifiÃ© pour E1_v3)\n                # En production, crÃ©er pays â†’ rÃ©gion â†’ dÃ©partement â†’ commune â†’ territoire\n                # Ici, on crÃ©e directement une commune et territoire si nÃ©cessaire\n                commune_id = conn.execute(text(\"\"\"\n                    INSERT INTO t16_commune(id_departement, nom_commune, lat, lon)\n                    VALUES (\n                        (SELECT id_departement FROM t15_departement LIMIT 1),\n                        :ville, :lat, :lon\n                    )\n                    ON CONFLICT DO NOTHING\n                    RETURNING id_commune\n                \"\"\"), {\"ville\": r[\"ville\"], \"lat\": r[\"lat\"], \"lon\": r[\"lon\"]}).scalar()\n                \n                if not commune_id:\n                    commune_id = conn.execute(text(\"\"\"\n                        SELECT id_commune FROM t16_commune WHERE nom_commune = :ville LIMIT 1\n                    \"\"\"), {\"ville\": r[\"ville\"]}).scalar()\n                \n                if commune_id:\n                    terr_id = conn.execute(text(\"\"\"\n                        INSERT INTO t17_territoire(id_commune)\n                        VALUES (:c)\n                        ON CONFLICT DO NOTHING\n                        RETURNING id_territoire\n                    \"\"\"), {\"c\": commune_id}).scalar()\n                    \n                    if not terr_id:\n                        terr_id = conn.execute(text(\"\"\"\n                            SELECT id_territoire FROM t17_territoire WHERE id_commune = :c LIMIT 1\n                        \"\"\"), {\"c\": commune_id}).scalar()\n                    \n                    if terr_id:\n                        # InsÃ©rer dans t19_meteo\n                        conn.execute(text(\"\"\"\n                            INSERT INTO t19_meteo(id_territoire, date_obs, temperature, humidite, vent_kmh, pression)\n                            VALUES(:t, :d, :T, :H, :V, :P)\n                        \"\"\"), {\n                            \"t\": terr_id, \"d\": r[\"date_obs\"], \"T\": r[\"temperature\"],\n                            \"H\": r[\"humidite\"], \"V\": r[\"vent_kmh\"], \"P\": r[\"pression\"]\n                        })\n        \n        print(f\"\\nâœ… OWM : {len(df_owm)} relevÃ©s insÃ©rÃ©s en base (t19_meteo) + MinIO\")\n        print(f\"â˜ï¸ MinIO : {minio_uri}\")\n        \n        # ğŸ“Š Visualisations\n        print(\"\\nğŸ“Š RÃ©partition des relevÃ©s par ville :\")\n        display(df_owm[[\"ville\", \"temperature\", \"humidite\", \"meteo_type\"]])\n        \n        plt.figure(figsize=(12, 5))\n        plt.subplot(1, 2, 1)\n        bars = plt.bar(df_owm[\"ville\"], df_owm[\"temperature\"], color='#FF6B6B')\n        for bar, value in zip(bars, df_owm[\"temperature\"]):\n            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n                    f\"{value:.1f}Â°C\", ha='center', va='bottom', fontweight='bold')\n        plt.title(\"ğŸŒ¡ï¸ TempÃ©rature par ville\", fontsize=12, fontweight='bold')\n        plt.ylabel(\"TempÃ©rature (Â°C)\", fontsize=11)\n        plt.xticks(rotation=15)\n        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n        \n        plt.subplot(1, 2, 2)\n        bars = plt.bar(df_owm[\"ville\"], df_owm[\"humidite\"], color='#4ECDC4')\n        for bar, value in zip(bars, df_owm[\"humidite\"]):\n            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n                    f\"{value}%\", ha='center', va='bottom', fontweight='bold')\n        plt.title(\"ğŸ’§ HumiditÃ© par ville\", fontsize=12, fontweight='bold')\n        plt.ylabel(\"HumiditÃ© (%)\", fontsize=11)\n        plt.xticks(rotation=15)\n        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n        plt.tight_layout()\n        plt.show()\n        \n        # ğŸ“‹ Tables de donnÃ©es rÃ©elles\n        print(\"\\nğŸ“‹ Table 't19_meteo' - RelevÃ©s insÃ©rÃ©s :\")\n        df_meteo = pd.read_sql_query(\"\"\"\n            SELECT m.id_meteo, m.date_obs, m.temperature, m.humidite, m.meteo_type\n            FROM t19_meteo m\n            ORDER BY m.id_meteo DESC\n            LIMIT 10\n        \"\"\", engine)\n        display(df_meteo)\n    else:\n        print(\"âš ï¸ Aucun relevÃ© mÃ©tÃ©o collectÃ©\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŒ Source 3 : Web Scraping Multi-Sources (6 sources citoyennes)\n",
    "\n",
    "Collecte depuis 6 sources lÃ©gales et Ã©thiques :\n",
    "- **Reddit** (API PRAW) : r/france, r/Paris\n",
    "- **YouTube** (API) : Commentaires vidÃ©os actualitÃ©s\n",
    "- **SignalConso** (Open Data gouv.fr) : Signalements consommateurs\n",
    "- **Trustpilot FR** : Avis services publics\n",
    "- **Vie-publique.fr** : Consultations citoyennes\n",
    "- **data.gouv.fr** (API) : Datasets Open Data\n",
    "\n",
    "**Stockage** : PostgreSQL (t04_document) + MinIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸŒ Source 3 : Web Scraping Multi-Sources (6 sources)\nprint(\"\\nğŸŒ SOURCE 3 : Web Scraping Multi-Sources (6 sources)\")\nprint(\"=\" * 80)\n\nall_scraping_data = []\n\n# Source 1/6 : Reddit (si credentials disponibles)\ntry:\n    import praw\n    REDDIT_CLIENT_ID = os.getenv(\"REDDIT_CLIENT_ID\")\n    REDDIT_CLIENT_SECRET = os.getenv(\"REDDIT_CLIENT_SECRET\")\n    \n    if REDDIT_CLIENT_ID and REDDIT_CLIENT_SECRET:\n        reddit = praw.Reddit(\n            client_id=REDDIT_CLIENT_ID,\n            client_secret=REDDIT_CLIENT_SECRET,\n            user_agent=\"DataSensBot/1.0\"\n        )\n        for subreddit_name in [\"france\", \"Paris\"]:\n            subreddit = reddit.subreddit(subreddit_name)\n            for post in subreddit.hot(limit=25):\n                all_scraping_data.append({\n                    \"titre\": post.title,\n                    \"texte\": post.selftext or post.title,\n                    \"source_site\": \"reddit.com\",\n                    \"url\": f\"https://reddit.com{post.permalink}\",\n                    \"date_publication\": pd.to_datetime(post.created_utc, unit=\"s\"),\n                    \"langue\": \"fr\"\n                })\n        print(f\"   âœ… Reddit: {len([d for d in all_scraping_data if 'reddit' in d['source_site']])} posts collectÃ©s\")\n    else:\n        print(\"   âš ï¸ Reddit: Credentials manquantes (REDDIT_CLIENT_ID/SECRET)\")\nexcept Exception as e:\n    print(f\"   âš ï¸ Reddit: {str(e)[:60]} (skip)\")\n\n# Source 2/6 : YouTube (API)\ntry:\n    from googleapiclient.discovery import build\n    YOUTUBE_API_KEY = os.getenv(\"YOUTUBE_API_KEY\")\n    \n    if YOUTUBE_API_KEY:\n        youtube = build(\"youtube\", \"v3\", developerKey=YOUTUBE_API_KEY)\n        request = youtube.search().list(\n            part=\"snippet\", q=\"france actualitÃ©s\", type=\"video\",\n            maxResults=20, regionCode=\"FR\", relevanceLanguage=\"fr\"\n        )\n        response = request.execute()\n        \n        for item in response.get(\"items\", []):\n            snippet = item[\"snippet\"]\n            all_scraping_data.append({\n                \"titre\": snippet[\"title\"],\n                \"texte\": snippet[\"description\"] or snippet[\"title\"],\n                \"source_site\": \"youtube.com\",\n                \"url\": f\"https://www.youtube.com/watch?v={item['id']['videoId']}\",\n                \"date_publication\": pd.to_datetime(snippet[\"publishedAt\"], errors=\"coerce\"),\n                \"langue\": \"fr\"\n            })\n        print(f\"   âœ… YouTube: {len([d for d in all_scraping_data if 'youtube' in d['source_site']])} vidÃ©os collectÃ©es\")\n    else:\n        print(\"   âš ï¸ YouTube: YOUTUBE_API_KEY manquante\")\nexcept Exception as e:\n    print(f\"   âš ï¸ YouTube: {str(e)[:60]} (skip)\")\n\n# Source 3/6 : Vie-publique.fr (RSS)\ntry:\n    feed_url = \"https://www.vie-publique.fr/rss\"\n    feed = feedparser.parse(feed_url)\n    for entry in feed.entries[:30]:\n        all_scraping_data.append({\n            \"titre\": entry.get(\"title\", \"\"),\n            \"texte\": entry.get(\"summary\", entry.get(\"description\", \"\")),\n            \"source_site\": \"vie-publique.fr\",\n            \"url\": entry.get(\"link\", \"\"),\n            \"date_publication\": pd.to_datetime(entry.get(\"published\", \"\"), errors=\"coerce\"),\n            \"langue\": \"fr\"\n        })\n    print(f\"   âœ… Vie-publique.fr: {len([d for d in all_scraping_data if 'vie-publique' in d['source_site']])} articles collectÃ©s\")\nexcept Exception as e:\n    print(f\"   âš ï¸ Vie-publique.fr: {str(e)[:60]} (skip)\")\n\n# Source 4/6 : data.gouv.fr (API)\ntry:\n    url = \"https://www.data.gouv.fr/api/1/datasets/\"\n    params = {\"q\": \"france\", \"page_size\": 30}\n    response = requests.get(url, params=params, timeout=10)\n    response.raise_for_status()\n    data = response.json()\n    \n    for dataset in data.get(\"data\", []):\n        all_scraping_data.append({\n            \"titre\": dataset.get(\"title\", \"\"),\n            \"texte\": dataset.get(\"description\", dataset.get(\"title\", \"\")),\n            \"source_site\": \"data.gouv.fr\",\n            \"url\": f\"https://www.data.gouv.fr/fr/datasets/{dataset.get('slug', '')}\",\n            \"date_publication\": pd.to_datetime(dataset.get(\"created_at\", \"\"), errors=\"coerce\"),\n            \"langue\": \"fr\"\n        })\n    print(f\"   âœ… data.gouv.fr: {len([d for d in all_scraping_data if 'data.gouv' in d['source_site']])} datasets collectÃ©s\")\nexcept Exception as e:\n    print(f\"   âš ï¸ data.gouv.fr: {str(e)[:60]} (skip)\")\n\n# Consolidation et insertion\nif all_scraping_data:\n    df_scraping = pd.DataFrame(all_scraping_data)\n    df_scraping = df_scraping[df_scraping[\"texte\"].str.len() > 20].copy()\n    df_scraping[\"hash_fingerprint\"] = df_scraping.apply(\n        lambda row: sha256_hash(row[\"titre\"] + \" \" + row[\"texte\"]), axis=1\n    )\n    df_scraping = df_scraping.drop_duplicates(subset=[\"hash_fingerprint\"])\n    \n    local = RAW_DIR / \"scraping\" / \"multi\" / f\"scraping_multi_{ts()}.csv\"\n    local.parent.mkdir(parents=True, exist_ok=True)\n    df_scraping.to_csv(local, index=False)\n    minio_uri = minio_upload(local, f\"scraping/multi/{local.name}\")\n    \n    with engine.begin() as conn:\n        flux_id = create_flux(conn, \"Web Scraping Multi-Sources\", \"html\", minio_uri)\n        inserted = insert_documents(conn, df_scraping[[\"titre\", \"texte\", \"langue\", \"date_publication\", \"hash_fingerprint\"]], flux_id)\n    \n    print(f\"\\nâœ… Web Scraping Multi-Sources : {inserted} documents insÃ©rÃ©s (t04_document) + MinIO\")\n    print(f\"â˜ï¸ MinIO : {minio_uri}\")\n    \n    # ğŸ“Š Visualisation par source\n    if len(df_scraping) > 0:\n        site_counts = df_scraping['source_site'].value_counts()\n        plt.figure(figsize=(10, 5))\n        bars = plt.bar(site_counts.index, site_counts.values, color=plt.cm.Pastel1(range(len(site_counts))))\n        for bar, value in zip(bars, site_counts.values):\n            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n                    str(value), ha='center', va='bottom', fontweight='bold')\n        plt.title(\"ğŸ“Š RÃ©partition Web Scraping par site\", fontsize=12, fontweight='bold')\n        plt.ylabel(\"Nombre de documents\", fontsize=11)\n        plt.xticks(rotation=45, ha='right')\n        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n        plt.tight_layout()\n        plt.show()\n        \n        # ğŸ“‹ Table de donnÃ©es\n        print(\"\\nğŸ“‹ Table 't04_document' - Web Scraping (aperÃ§u 10 premiers) :\")\n        df_scrap_docs = pd.read_sql_query(\"\"\"\n            SELECT d.id_doc, LEFT(d.titre, 80) AS titre, d.date_publication\n            FROM t04_document d\n            JOIN t03_flux f ON d.id_flux = f.id_flux\n            JOIN t02_source s ON f.id_source = s.id_source\n            WHERE s.nom LIKE '%Scraping%'\n            ORDER BY d.id_doc DESC\n            LIMIT 10\n        \"\"\", engine)\n        display(df_scrap_docs)\nelse:\n    print(\"âš ï¸ Aucune donnÃ©e Web Scraping collectÃ©e\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“° Source 4 : NewsAPI (Optionnel)\n",
    "\n",
    "Collecte d'articles via l'API NewsAPI si la clÃ© est configurÃ©e.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“° Source 4 : NewsAPI (Optionnel)\nprint(\"\\nğŸ“° SOURCE 4 : NewsAPI (Optionnel)\")\nprint(\"=\" * 80)\n\nNEWSAPI_KEY = os.getenv(\"NEWSAPI_KEY\")\n\nif not NEWSAPI_KEY:\n    print(\"âš ï¸ NEWSAPI_KEY manquante - Source 4 ignorÃ©e\")\nelse:\n    NEWS_CATEGORIES = [\"general\", \"technology\", \"health\", \"business\"]\n    all_articles = []\n    \n    for category in NEWS_CATEGORIES:\n        try:\n            r = requests.get(\n                \"https://newsapi.org/v2/top-headlines\",\n                params={\"apiKey\": NEWSAPI_KEY, \"country\": \"fr\", \"category\": category, \"pageSize\": 20},\n                timeout=10\n            )\n            if r.status_code == 200:\n                data = r.json()\n                articles = data.get(\"articles\", [])\n                for art in articles:\n                    all_articles.append({\n                        \"titre\": (art.get(\"title\") or \"\").strip(),\n                        \"texte\": (art.get(\"description\") or art.get(\"content\") or \"\").strip(),\n                        \"date_publication\": pd.to_datetime(art.get(\"publishedAt\"), errors=\"coerce\"),\n                        \"langue\": \"fr\",\n                        \"categorie\": category\n                    })\n            elif r.status_code in [426, 429]:\n                print(f\"   âš ï¸ Quota Ã©puisÃ© pour {category}\")\n                break\n            time.sleep(1)\n        except Exception as e:\n            print(f\"   âš ï¸ Erreur {category}: {str(e)[:60]}\")\n    \n    if all_articles:\n        df_news = pd.DataFrame(all_articles)\n        df_news = df_news[df_news[\"texte\"].str.len() > 20].copy()\n        df_news[\"hash_fingerprint\"] = df_news.apply(\n            lambda row: sha256_hash(row[\"titre\"] + \" \" + row[\"texte\"]), axis=1\n        )\n        df_news = df_news.drop_duplicates(subset=[\"hash_fingerprint\"])\n        \n        local = RAW_DIR / \"api\" / \"newsapi\" / f\"newsapi_{ts()}.csv\"\n        local.parent.mkdir(parents=True, exist_ok=True)\n        df_news.to_csv(local, index=False)\n        minio_uri = minio_upload(local, f\"api/newsapi/{local.name}\")\n        \n        with engine.begin() as conn:\n            flux_id = create_flux(conn, \"NewsAPI\", \"json\", minio_uri)\n            inserted = insert_documents(conn, df_news[[\"titre\", \"texte\", \"langue\", \"date_publication\", \"hash_fingerprint\"]], flux_id)\n        \n        print(f\"\\nâœ… NewsAPI : {inserted} articles insÃ©rÃ©s (t04_document)\")\n        \n        # ğŸ“Š Visualisation\n        if len(df_news) > 0:\n            cat_counts = df_news['categorie'].value_counts()\n            plt.figure(figsize=(8, 5))\n            plt.pie(cat_counts.values, labels=cat_counts.index, autopct='%1.1f%%', startangle=90)\n            plt.title(\"ğŸ“Š RÃ©partition NewsAPI par catÃ©gorie\", fontsize=12, fontweight='bold')\n            plt.tight_layout()\n            plt.show()\n    else:\n        print(\"âš ï¸ Aucun article NewsAPI rÃ©cupÃ©rÃ© (quota Ã©puisÃ© ou clÃ© invalide)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Bilan global de la collecte E1_v3\n",
    "\n",
    "RÃ©capitulatif de toutes les sources collectÃ©es avec statistiques globales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Š Bilan global de la collecte E1_v3\nprint(\"\\nğŸ“Š BILAN GLOBAL DE LA COLLECTE E1_V3\")\nprint(\"=\" * 80)\n\n# Statistiques par source (tables t02_source, t03_flux, t04_document)\nwith engine.connect() as conn:\n    stats = pd.read_sql_query(\"\"\"\n        SELECT \n            s.nom AS source,\n            COUNT(DISTINCT f.id_flux) AS nb_flux,\n            COUNT(DISTINCT d.id_doc) AS nb_documents,\n            td.libelle AS type_donnee\n        FROM t02_source s\n        LEFT JOIN t03_flux f ON s.id_source = f.id_source\n        LEFT JOIN t04_document d ON f.id_flux = d.id_flux\n        LEFT JOIN t01_type_donnee td ON s.id_type_donnee = td.id_type_donnee\n        GROUP BY s.nom, td.libelle\n        ORDER BY nb_documents DESC\n    \"\"\", conn)\n\nprint(\"\\nğŸ“ˆ Statistiques par source :\")\ndisplay(stats)\n\n# Total documents\ntotal_docs = stats['nb_documents'].sum()\nprint(f\"\\nğŸ“Š Total documents collectÃ©s : {total_docs}\")\n\n# Graphique global\nif len(stats) > 0:\n    plt.figure(figsize=(12, 6))\n    bars = plt.bar(stats[\"source\"], stats[\"nb_documents\"], color=plt.cm.Set3(range(len(stats))))\n    for bar, value in zip(bars, stats[\"nb_documents\"]):\n        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n                str(int(value)), ha='center', va='bottom', fontweight='bold', fontsize=10)\n    plt.title(\"ğŸ“Š Nombre de documents collectÃ©s par source (E1_v3)\", fontsize=14, fontweight='bold')\n    plt.ylabel(\"Nombre de documents\", fontsize=12)\n    plt.xticks(rotation=45, ha='right')\n    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\n# Vue complÃ¨te : tous les documents avec contexte\nprint(\"\\nğŸ“‹ Vue complÃ¨te - Tous les documents avec contexte (50 premiers) :\")\ndf_all_docs = pd.read_sql_query(\"\"\"\n    SELECT \n        d.id_doc,\n        LEFT(d.titre, 60) AS titre,\n        LEFT(d.texte, 100) AS texte_apercu,\n        d.langue,\n        d.date_publication,\n        s.nom AS source,\n        f.date_collecte,\n        f.format\n    FROM t04_document d\n    JOIN t03_flux f ON d.id_flux = f.id_flux\n    JOIN t02_source s ON f.id_source = s.id_source\n    ORDER BY d.id_doc DESC\n    LIMIT 50\n\"\"\", engine)\ndisplay(df_all_docs)\n\n# Statistiques par type de donnÃ©e\nprint(\"\\nğŸ“Š RÃ©partition par type de donnÃ©e :\")\ndf_types = pd.read_sql_query(\"\"\"\n    SELECT \n        td.libelle AS type_donnee,\n        COUNT(DISTINCT s.id_source) AS nb_sources,\n        COUNT(DISTINCT d.id_doc) AS nb_documents\n    FROM t01_type_donnee td\n    LEFT JOIN t02_source s ON td.id_type_donnee = s.id_type_donnee\n    LEFT JOIN t03_flux f ON s.id_source = f.id_source\n    LEFT JOIN t04_document d ON f.id_flux = d.id_flux\n    GROUP BY td.libelle\n    ORDER BY nb_documents DESC\n\"\"\", engine)\ndisplay(df_types)\n\nif len(df_types) > 0:\n    plt.figure(figsize=(10, 6))\n    bars = plt.bar(df_types[\"type_donnee\"], df_types[\"nb_documents\"], color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n    for bar, value in zip(bars, df_types[\"nb_documents\"]):\n        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n                str(int(value)), ha='center', va='bottom', fontweight='bold')\n    plt.title(\"ğŸ“Š RÃ©partition des documents par type de donnÃ©e (E1_v3)\", fontsize=12, fontweight='bold')\n    plt.ylabel(\"Nombre de documents\", fontsize=11)\n    plt.xticks(rotation=45, ha='right')\n    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\nprint(f\"\\nâœ… Collecte E1_v3 terminÃ©e : {total_docs} documents collectÃ©s et stockÃ©s\")\nprint(\"   ğŸ“Š Architecture complÃ¨te : 36/37 tables (t01-t37)\")\nprint(\"   â¡ï¸ Passez au notebook 04_quality_checks.ipynb pour valider la qualitÃ©\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n# VISUALISATIONS GDELT (Section 5/5) - ComplÃ©ment audit\n# ============================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"ğŸ“Š VISUALISATIONS SOURCE GDELT (Section 5/5)\")\nprint(\"=\"*80)\n\ntry:\n    with engine.connect() as conn:\n        # VÃ©rifier si des Ã©vÃ©nements GDELT ont Ã©tÃ© insÃ©rÃ©s\n        nb_events_gdelt = conn.execute(text(\"\"\"\n            SELECT COUNT(*) FROM t25_evenement WHERE source_event = 'GDELT'\n        \"\"\")).scalar()\n        \n        if nb_events_gdelt > 0:\n            # Bar chart : Ã‰vÃ©nements par thÃ¨me\n            df_events_theme = pd.read_sql_query(\"\"\"\n                SELECT \n                    th.libelle AS theme,\n                    COUNT(e.id_event) AS nb_evenements,\n                    AVG(e.avg_tone) AS tonalite_moyenne\n                FROM t25_evenement e\n                JOIN t24_theme th ON e.id_theme = th.id_theme\n                WHERE e.source_event = 'GDELT'\n                GROUP BY th.libelle\n                ORDER BY nb_evenements DESC\n                LIMIT 10\n            \"\"\", conn)\n            \n            if len(df_events_theme) > 0:\n                print(\"\\nğŸ“Š Ã‰vÃ©nements par thÃ¨me :\")\n                display(df_events_theme)\n                \n                plt.figure(figsize=(14, 6))\n                \n                plt.subplot(1, 2, 1)\n                bars = plt.barh(df_events_theme[\"theme\"], df_events_theme[\"nb_evenements\"], color=plt.cm.Set2(range(len(df_events_theme))))\n                for i, (bar, value) in enumerate(zip(bars, df_events_theme[\"nb_evenements\"])):\n                    plt.text(bar.get_width() + max(df_events_theme[\"nb_evenements\"]) * 0.02, bar.get_y() + bar.get_height()/2,\n                            f\"{int(value)}\", ha='left', va='center', fontweight='bold', fontsize=9)\n                plt.title(\"ğŸ“Š Ã‰vÃ©nements par thÃ¨me (GDELT France)\", fontsize=12, fontweight='bold')\n                plt.xlabel(\"Nombre d'Ã©vÃ©nements\", fontsize=11)\n                plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n                \n                plt.subplot(1, 2, 2)\n                bars = plt.barh(df_events_theme[\"theme\"], df_events_theme[\"tonalite_moyenne\"], color=plt.cm.RdYlGn_r(range(len(df_events_theme))))\n                for i, (bar, value) in enumerate(zip(bars, df_events_theme[\"tonalite_moyenne\"])):\n                    plt.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2,\n                            f\"{value:.1f}\", ha='left', va='center', fontweight='bold', fontsize=9)\n                plt.title(\"ğŸ“Š TonalitÃ© moyenne par thÃ¨me (GDELT)\", fontsize=12, fontweight='bold')\n                plt.xlabel(\"TonalitÃ© moyenne (-100 nÃ©gatif â†’ +100 positif)\", fontsize=11)\n                plt.axvline(x=0, color='black', linestyle='--', linewidth=1)\n                plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n                plt.tight_layout()\n                plt.show()\n            \n            # Table pandas : Ã‰vÃ©nements France insÃ©rÃ©s\n            df_events_france = pd.read_sql_query(\"\"\"\n                SELECT \n                    e.id_event,\n                    th.libelle AS theme,\n                    e.date_event,\n                    e.avg_tone AS tonalite,\n                    e.source_event\n                FROM t25_evenement e\n                LEFT JOIN t24_theme th ON e.id_theme = th.id_theme\n                WHERE e.source_event = 'GDELT'\n                ORDER BY e.date_event DESC\n                LIMIT 20\n            \"\"\", conn)\n            \n            if len(df_events_france) > 0:\n                print(\"\\nğŸ“‹ Ã‰vÃ©nements France insÃ©rÃ©s (20 derniers) :\")\n                display(df_events_france)\n            else:\n                print(\"\\nâš ï¸ Aucun Ã©vÃ©nement GDELT Ã  afficher\")\n        else:\n            print(\"\\nâš ï¸ Aucun Ã©vÃ©nement GDELT insÃ©rÃ© dans la base\")\n            print(\"   ğŸ’¡ La collecte GDELT peut avoir Ã©chouÃ© ou aucun Ã©vÃ©nement France trouvÃ©\")\n            \nexcept Exception as e:\n    print(f\"\\nâš ï¸ Erreur lors de la rÃ©cupÃ©ration des donnÃ©es GDELT : {str(e)[:100]}\")\n    print(\"   ğŸ’¡ Les visualisations seront disponibles aprÃ¨s une collecte GDELT rÃ©ussie\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ­ Chapitre 1 : Kaggle CSV\n",
    "\n",
    "**Contexte narratif** : Collecte de donnÃ©es depuis Kaggle CSV\n",
    "\n",
    "**Avant cette collecte** :\n",
    "- Sources prÃ©cÃ©dentes : 0 source(s) dÃ©jÃ  collectÃ©e(s)\n",
    "- Documents en base : [VÃ©rification en cours...]\n",
    "\n",
    "**Objectif de cette Ã©tape** :\n",
    "- Collecter de nouvelles donnÃ©es depuis Kaggle CSV\n",
    "- Enrichir notre dataset avec cette source\n",
    "- Progression du pipeline vers le dataset final\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n# ğŸ­ STORYTELLING : PRÃ‰PARATION COLLECTE 1 - Kaggle CSV\n# ============================================================\n# Cette section raconte l'histoire de la collecte avant de l'effectuer\n# ============================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(f\"ğŸ­ CHAPITRE 1 : COLLECTE Kaggle CSV\")\nprint(\"=\"*80)\n\n# VÃ©rifier l'Ã©tat actuel avant cette collecte\ntry:\n    with engine.connect() as conn:\n        # Statistiques avant cette source\n        nb_sources_avant = conn.execute(text(\"SELECT COUNT(*) FROM t02_source\")).scalar() or 0\n        nb_docs_avant = conn.execute(text(\"SELECT COUNT(*) FROM t04_document\")).scalar() or 0\n        nb_flux_avant = conn.execute(text(\"SELECT COUNT(*) FROM t03_flux\")).scalar() or 0\n        \n        print(f\"\\nğŸ“Š Ã‰TAT ACTUEL DU PIPELINE (avant Kaggle CSV):\")\n        print(f\"   â€¢ Sources configurÃ©es : {nb_sources_avant}\")\n        print(f\"   â€¢ Documents collectÃ©s : {nb_docs_avant:,}\")\n        print(f\"   â€¢ Flux de collecte : {nb_flux_avant}\")\n        \n        # Visualisation Ã©tat actuel\n        if nb_docs_avant > 0:\n            # Graphique progression\n            fig, ax = plt.subplots(figsize=(10, 6))\n            \n            categories = ['Sources', 'Flux', 'Documents']\n            valeurs = [nb_sources_avant, nb_flux_avant, nb_docs_avant]\n            colors = ['#FF6B6B', '#FECA57', '#4ECDC4']\n            \n            bars = ax.bar(categories, valeurs, color=colors)\n            for bar, val in zip(bars, valeurs):\n                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(valeurs) * 0.02,\n                       f\"{int(val):,}\", ha='center', va='bottom', fontweight='bold')\n            \n            ax.set_title(f\"ğŸ“Š Ã‰tat du pipeline AVANT collecte Kaggle CSV\", fontsize=12, fontweight='bold')\n            ax.set_ylabel(\"Volume\", fontsize=11)\n            ax.grid(axis='y', linestyle='--', alpha=0.3)\n            plt.tight_layout()\n            plt.show()\n            \n            print(f\"\\nğŸ’¡ Prochaine Ã©tape : Collecte Kaggle CSV pour enrichir le dataset...\")\n        else:\n            print(f\"\\nğŸ’¡ DÃ©marrage : PremiÃ¨re collecte avec Kaggle CSV...\")\n            \nexcept Exception as e:\n    print(f\"\\nğŸ’¡ PrÃªt pour collecte Kaggle CSV...\")\n\nprint(\"\\n\" + \"-\"*80)\nprint(f\"â¡ï¸ Lancement de la collecte Kaggle CSV...\")\nprint(\"-\"*80 + \"\\n\")\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“„ Source 1/5 : Fichier plat CSV (Kaggle)\n",
    "\n",
    "**Architecture hybride (comme datasens_E1_v2.ipynb)** :\n",
    "- **50% â†’ PostgreSQL** : DonnÃ©es structurÃ©es pour requÃªtes SQL\n",
    "- **50% â†’ MinIO DataLake** : DonnÃ©es brutes pour analyses Big Data futures\n",
    "\n",
    "**Process** :\n",
    "1. Chargement CSV depuis `data/raw/kaggle/`\n",
    "2. Calcul SHA256 fingerprint pour dÃ©duplication\n",
    "3. Split alÃ©atoire 50/50\n",
    "4. Upload 50% vers MinIO (DataLake)\n",
    "5. Insertion 50% dans PostgreSQL avec traÃ§abilitÃ© (id_flux)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"ğŸ“„ SOURCE 1/5 : Fichier plat CSV (Kaggle)\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "# Rechercher fichier Kaggle existant ou crÃ©er Ã©chantillon\n",
    "kaggle_csv_paths = [\n",
    "    RAW_DIR / \"kaggle\" / \"kaggle_sample.csv\",\n",
    "    PROJECT_ROOT / \"data\" / \"raw\" / \"kaggle\" / \"*.csv\",\n",
    "    Path.cwd() / \"data\" / \"raw\" / \"kaggle\" / \"*.csv\"\n",
    "]\n",
    "\n",
    "kaggle_csv_path = None\n",
    "for path in kaggle_csv_paths:\n",
    "    if path.exists():\n",
    "        kaggle_csv_path = path\n",
    "        break\n",
    "\n",
    "if not kaggle_csv_path or not kaggle_csv_path.exists():\n",
    "    logger.warning(\"âš ï¸ Fichier Kaggle non trouvÃ© â€” CrÃ©ation Ã©chantillon pour dÃ©mo\")\n",
    "    sample_data = pd.DataFrame({\n",
    "        \"text\": [\n",
    "            \"Great product, very satisfied!\",\n",
    "            \"Service terrible, avoid at all costs\",\n",
    "            \"Excellent quality, recommend\",\n",
    "            \"Bon produit, je recommande\",\n",
    "            \"Mauvais service, dÃ©Ã§u\"\n",
    "        ],\n",
    "        \"langue\": [\"en\", \"en\", \"en\", \"fr\", \"fr\"],\n",
    "        \"date\": [datetime.now(UTC)] * 5\n",
    "    })\n",
    "    kaggle_csv_path = RAW_DIR / \"kaggle\" / \"kaggle_sample.csv\"\n",
    "    kaggle_csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    sample_data.to_csv(kaggle_csv_path, index=False)\n",
    "    logger.info(f\"   âœ… Ã‰chantillon crÃ©Ã© : {kaggle_csv_path.name}\")\n",
    "\n",
    "# Charger le CSV\n",
    "df_kaggle = pd.read_csv(kaggle_csv_path)\n",
    "logger.info(f\"ğŸ“Š {len(df_kaggle)} lignes chargÃ©es\")\n",
    "\n",
    "# Split 50/50 (architecture hybride : PostgreSQL + MinIO)\n",
    "df_kaggle[\"hash_fingerprint\"] = df_kaggle[\"text\"].apply(lambda x: sha256(str(x)))\n",
    "mid_point = len(df_kaggle) // 2\n",
    "df_pg = df_kaggle.iloc[:mid_point].copy()  # 50% â†’ PostgreSQL\n",
    "df_raw = df_kaggle.iloc[mid_point:].copy()  # 50% â†’ MinIO DataLake\n",
    "\n",
    "logger.info(f\"   â€¢ 50% PostgreSQL : {len(df_pg)} lignes\")\n",
    "logger.info(f\"   â€¢ 50% MinIO DataLake : {len(df_raw)} lignes\")\n",
    "\n",
    "# Sauvegarder 50% en raw local + upload MinIO\n",
    "raw_output = RAW_DIR / \"kaggle\" / f\"kaggle_raw_{ts()}.csv\"\n",
    "df_raw.to_csv(raw_output, index=False)\n",
    "logger.info(f\"   âœ… SauvegardÃ© local : {raw_output.name}\")\n",
    "\n",
    "# Upload MinIO (50% bruts vers DataLake)\n",
    "try:\n",
    "    minio_uri = minio_upload(raw_output, f\"kaggle/{raw_output.name}\")\n",
    "    logger.info(f\"   â˜ï¸ Upload MinIO : {minio_uri}\")\n",
    "except Exception as e:\n",
    "    log_error(\"MinIO\", e, \"Upload fichier Kaggle\")\n",
    "    minio_uri = f\"local://{raw_output}\"\n",
    "\n",
    "# InsÃ©rer 50% dans PostgreSQL\n",
    "with engine.begin() as conn:\n",
    "    id_source = get_source_id(conn, \"Kaggle CSV\")\n",
    "    if not id_source:\n",
    "        id_type = conn.execute(text(\"SELECT id_type_donnee FROM type_donnee WHERE libelle = 'Fichier plat'\")).scalar()\n",
    "        conn.execute(text(\"\"\"\n",
    "            INSERT INTO source (id_type_donnee, nom, url, fiabilite)\n",
    "            VALUES (:id_type, 'Kaggle CSV', 'https://www.kaggle.com', 0.8)\n",
    "        \"\"\"), {\"id_type\": id_type})\n",
    "        id_source = conn.execute(text(\"SELECT id_source FROM source WHERE nom = 'Kaggle CSV'\")).scalar()\n",
    "\n",
    "    id_flux = create_flux(conn, id_source, \"csv\", minio_uri)\n",
    "\n",
    "    # PrÃ©parer documents pour insertion batch\n",
    "    docs = []\n",
    "    for _, row in df_pg.iterrows():\n",
    "        docs.append({\n",
    "            \"id_flux\": id_flux,\n",
    "            \"id_territoire\": None,\n",
    "            \"titre\": \"\",\n",
    "            \"texte\": str(row[\"text\"]),\n",
    "            \"langue\": row.get(\"langue\", \"en\"),\n",
    "            \"date_publication\": row.get(\"date\", datetime.now(UTC)),\n",
    "            \"hash_fingerprint\": row[\"hash_fingerprint\"]\n",
    "        })\n",
    "\n",
    "    inserted = insert_documents(conn, docs)\n",
    "\n",
    "logger.info(f\"\\nâœ… Source 1/5 terminÃ©e : {inserted} docs PostgreSQL + {len(df_raw)} docs MinIO\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Architecture Pipeline (RÃ©fÃ©rence datasens_E1_v2.ipynb)\n",
    "\n",
    "**Ce notebook suit l'architecture du pipeline existant** :\n",
    "\n",
    "âœ… **Logging structurÃ©** : `logs/collecte_*.log` + `logs/errors_*.log`  \n",
    "âœ… **MinIO DataLake** : Upload automatique fichiers bruts â†’ `s3://datasens-raw/`  \n",
    "âœ… **PostgreSQL** : Insertion structurÃ©e avec traÃ§abilitÃ© (flux, manifests)  \n",
    "âœ… **Fonctions helpers** : `create_flux()`, `insert_documents()`, `ensure_territoire()`, `minio_upload()`  \n",
    "âœ… **DÃ©duplication** : Hash SHA-256 pour Ã©viter doublons  \n",
    "âœ… **RGPD** : Pas de donnÃ©es personnelles directes  \n",
    "\n",
    "**Sources 2-5** : ImplÃ©mentÃ©es ci-dessous avec vraies sources (code extrait de `datasens_E1_v2.ipynb`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Visualisation Pipeline ComplÃ¨te - Ã‰tat des DonnÃ©es\n",
    "\n",
    "Visualisations complÃ¨tes Ã  chaque Ã©tape du pipeline ETL pour suivre le flux de donnÃ©es.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n# VISUALISATIONS PIPELINE COMPLET - Ã‰TAT DES DONNÃ‰ES\n# ============================================================\n# Ã‰tape 1 : MinIO DataLake (DonnÃ©es Brutes)\n# Ã‰tape 2 : AprÃ¨s Nettoyage (Statistiques dÃ©duplication)\n# Ã‰tape 3 : PostgreSQL (DonnÃ©es StructurÃ©es)\n# ============================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"ğŸ“Š VISUALISATIONS PIPELINE COMPLET E1_V3\")\nprint(\"=\"*80)\n\n# ============================================================\n# Ã‰TAPE 1 : Ã‰TAT MINIO DATALAKE (DONNÃ‰ES BRUTES)\n# ============================================================\nprint(\"\\nğŸ“Š Ã‰TAPE 1 : MINIO DATALAKE (DonnÃ©es Brutes)\")\nprint(\"-\" * 80)\n\ntry:\n    if minio_client and minio_client.bucket_exists(MINIO_BUCKET):\n        objects = list(minio_client.list_objects(MINIO_BUCKET, recursive=True))\n        \n        if len(objects) > 0:\n            total_size = sum(obj.size for obj in objects)\n            total_size_mb = total_size / (1024 * 1024)\n            \n            # RÃ©partition par prÃ©fixe/source\n            prefixes = {}\n            sizes_by_prefix = {}\n            for obj in objects:\n                prefix = obj.object_name.split('/')[0] if '/' in obj.object_name else 'root'\n                prefixes[prefix] = prefixes.get(prefix, 0) + 1\n                sizes_by_prefix[prefix] = sizes_by_prefix.get(prefix, 0) + obj.size\n            \n            df_minio = pd.DataFrame([{\n                \"Type\": prefix,\n                \"Nb objets\": prefixes[prefix],\n                \"Taille (MB)\": round(sizes_by_prefix[prefix] / (1024 * 1024), 2)\n            } for prefix in sorted(prefixes.keys())])\n            \n            print(f\"\\nğŸ“¦ MinIO DataLake '{MINIO_BUCKET}' :\")\n            print(f\"   â€¢ {len(objects)} objets bruts\")\n            print(f\"   â€¢ Taille totale : {total_size_mb:.2f} MB\")\n            print(\"\\nğŸ“‹ RÃ©partition des donnÃ©es brutes :\")\n            display(df_minio)\n            \n            # Graphiques MinIO\n            plt.figure(figsize=(14, 5))\n            plt.subplot(1, 2, 1)\n            bars = plt.bar(df_minio[\"Type\"], df_minio[\"Nb objets\"], color=plt.cm.Set3(range(len(df_minio))))\n            for bar, value in zip(bars, df_minio[\"Nb objets\"]):\n                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(df_minio[\"Nb objets\"]) * 0.02,\n                        str(value), ha='center', va='bottom', fontweight='bold', fontsize=9)\n            plt.title(\"ğŸ“Š Objets bruts par type (MinIO)\", fontsize=12, fontweight='bold')\n            plt.ylabel(\"Nombre d'objets\", fontsize=11)\n            plt.xticks(rotation=45, ha='right')\n            plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n            \n            plt.subplot(1, 2, 2)\n            bars = plt.bar(df_minio[\"Type\"], df_minio[\"Taille (MB)\"], color=plt.cm.Pastel1(range(len(df_minio))))\n            for bar, value in zip(bars, df_minio[\"Taille (MB)\"]):\n                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(df_minio[\"Taille (MB)\"]) * 0.02,\n                        f\"{value:.1f} MB\", ha='center', va='bottom', fontweight='bold', fontsize=9)\n            plt.title(\"ğŸ’¾ Taille donnÃ©es brutes (MinIO)\", fontsize=12, fontweight='bold')\n            plt.ylabel(\"Taille (MB)\", fontsize=11)\n            plt.xticks(rotation=45, ha='right')\n            plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n            plt.tight_layout()\n            plt.show()\n        else:\n            print(\"   â„¹ï¸ Aucun objet dans MinIO (collecte en cours...)\")\n    else:\n        print(\"   âš ï¸ MinIO non accessible\")\nexcept Exception as e:\n    print(f\"   âš ï¸ Erreur MinIO : {str(e)[:80]}\")\n\n# ============================================================\n# Ã‰TAPE 2 : APRÃˆS NETTOYAGE (DÃ‰DUPLICATION)\n# ============================================================\nprint(\"\\nğŸ§¹ Ã‰TAPE 2 : APRÃˆS NETTOYAGE (DÃ©duplication)\")\nprint(\"-\" * 80)\n\ntry:\n    with engine.connect() as conn:\n        # Statistiques dÃ©duplication\n        dedup_stats = pd.read_sql_query(\"\"\"\n            SELECT \n                COUNT(*) AS total_docs,\n                COUNT(DISTINCT hash_fingerprint) AS docs_uniques,\n                COUNT(*) - COUNT(DISTINCT hash_fingerprint) AS doublons_detectes,\n                ROUND(100.0 * (COUNT(*) - COUNT(DISTINCT hash_fingerprint)) / COUNT(*), 2) AS pct_doublons\n            FROM t04_document\n        \"\"\", conn)\n        \n        if len(dedup_stats) > 0 and dedup_stats.iloc[0]['total_docs'] > 0:\n            row = dedup_stats.iloc[0]\n            print(\"\\nğŸ“‹ Statistiques nettoyage/dÃ©duplication :\")\n            display(dedup_stats)\n            \n            # Graphique dÃ©duplication\n            plt.figure(figsize=(10, 6))\n            categories = ['Total brut', 'Uniques', 'Doublons']\n            values = [row['total_docs'], row['docs_uniques'], row['doublons_detectes']]\n            colors = ['#FF6B6B', '#4ECDC4', '#FECA57']\n            bars = plt.bar(categories, values, color=colors)\n            for bar, value in zip(bars, values):\n                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(values) * 0.02,\n                        f\"{int(value):,}\", ha='center', va='bottom', fontweight='bold')\n            plt.title(\"ğŸ§¹ Impact du nettoyage et dÃ©duplication (SHA256)\", fontsize=12, fontweight='bold')\n            plt.ylabel(\"Nombre de documents\", fontsize=11)\n            plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n            plt.tight_layout()\n            plt.show()\n            \n            print(f\"\\nâœ… DÃ©duplication : {row['pct_doublons']:.1f}% de doublons dÃ©tectÃ©s et Ã©vitÃ©s\")\n        else:\n            print(\"   â„¹ï¸ Aucune donnÃ©e pour statistiques nettoyage\")\nexcept Exception as e:\n    print(f\"   âš ï¸ Erreur statistiques nettoyage : {str(e)[:80]}\")\n\n# ============================================================\n# Ã‰TAPE 3 : Ã‰TAT POSTGRESQL (DONNÃ‰ES STRUCTURÃ‰ES)\n# ============================================================\nprint(\"\\nğŸ’¾ Ã‰TAPE 3 : POSTGRESQL (DonnÃ©es StructurÃ©es)\")\nprint(\"-\" * 80)\n\ntry:\n    with engine.connect() as conn:\n        # Volumes par table\n        stats_pg = pd.read_sql_query(\"\"\"\n            SELECT 't04_document' AS table_name, COUNT(*) AS nb_lignes FROM t04_document\n            UNION ALL SELECT 't03_flux', COUNT(*) FROM t03_flux\n            UNION ALL SELECT 't02_source', COUNT(*) FROM t02_source\n            UNION ALL SELECT 't19_meteo', COUNT(*) FROM t19_meteo\n            UNION ALL SELECT 't25_evenement', COUNT(*) FROM t25_evenement\n        \"\"\", conn)\n        \n        # Documents par source\n        stats_sources = pd.read_sql_query(\"\"\"\n            SELECT s.nom AS source, COUNT(d.id_doc) AS nb_documents\n            FROM t02_source s\n            LEFT JOIN t03_flux f ON s.id_source = f.id_source\n            LEFT JOIN t04_document d ON f.id_flux = d.id_flux\n            GROUP BY s.nom\n            ORDER BY nb_documents DESC\n        \"\"\", conn)\n        \n        if len(stats_pg) > 0:\n            print(\"\\nğŸ“‹ Volumes PostgreSQL par table :\")\n            display(stats_pg)\n            \n            # Graphiques PostgreSQL\n            plt.figure(figsize=(14, 5))\n            plt.subplot(1, 2, 1)\n            bars = plt.bar(stats_pg[\"table_name\"], stats_pg[\"nb_lignes\"], color=plt.cm.Set2(range(len(stats_pg))))\n            for bar, value in zip(bars, stats_pg[\"nb_lignes\"]):\n                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(stats_pg[\"nb_lignes\"]) * 0.02,\n                        f\"{int(value):,}\", ha='center', va='bottom', fontweight='bold', fontsize=9)\n            plt.title(\"ğŸ“Š Volumes PostgreSQL par table\", fontsize=12, fontweight='bold')\n            plt.ylabel(\"Nombre de lignes\", fontsize=11)\n            plt.xticks(rotation=45, ha='right')\n            plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n            \n            if len(stats_sources) > 0:\n                plt.subplot(1, 2, 2)\n                top = stats_sources.head(10)\n                bars = plt.barh(top[\"source\"], top[\"nb_documents\"], color=plt.cm.Pastel2(range(len(top))))\n                for i, (bar, value) in enumerate(zip(bars, top[\"nb_documents\"])):\n                    plt.text(bar.get_width() + max(top[\"nb_documents\"]) * 0.02, bar.get_y() + bar.get_height()/2,\n                            f\"{int(value):,}\", ha='left', va='center', fontweight='bold', fontsize=9)\n                plt.title(\"ğŸ“Š Documents par source (Top 10)\", fontsize=12, fontweight='bold')\n                plt.xlabel(\"Nombre de documents\", fontsize=11)\n                plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n            \n            plt.tight_layout()\n            plt.show()\n            \n            total_docs = stats_pg[stats_pg['table_name'] == 't04_document']['nb_lignes'].iloc[0] if len(stats_pg[stats_pg['table_name'] == 't04_document']) > 0 else 0\n            print(f\"\\nâœ… PostgreSQL : {total_docs:,} documents structurÃ©s aprÃ¨s ETL\")\nexcept Exception as e:\n    print(f\"   âš ï¸ Erreur PostgreSQL : {str(e)[:80]}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"âœ… Visualisations pipeline complÃ¨tes terminÃ©es\")\nprint(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n# ğŸ­ STORYTELLING : RÃ‰SULTAT COLLECTE 1 - Kaggle CSV\n# ============================================================\n# Cette section montre l'impact de la collecte sur le pipeline\n# ============================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(f\"âœ… CHAPITRE 1 TERMINÃ‰ : Kaggle CSV\")\nprint(\"=\"*80)\n\n# VÃ©rifier l'Ã©tat aprÃ¨s cette collecte\ntry:\n    with engine.connect() as conn:\n        # Statistiques aprÃ¨s cette source\n        nb_sources_apres = conn.execute(text(\"SELECT COUNT(*) FROM t02_source\")).scalar() or 0\n        nb_docs_apres = conn.execute(text(\"SELECT COUNT(*) FROM t04_document\")).scalar() or 0\n        nb_flux_apres = conn.execute(text(\"SELECT COUNT(*) FROM t03_flux\")).scalar() or 0\n        \n        # Calculer la progression\n        docs_source = conn.execute(text(\"\"\"\n            SELECT COUNT(*) FROM t04_document d\n            JOIN t03_flux f ON d.id_flux = f.id_flux\n            JOIN t02_source s ON f.id_source = s.id_source\n            WHERE s.nom LIKE :pattern\n        \"\"\"), {\"pattern\": f\"%Kaggle CSV%\"}).scalar() or 0\n        \n        print(f\"\\nğŸ“Š RÃ‰SULTAT DE LA COLLECTE Kaggle CSV:\")\n        print(f\"   â€¢ Documents ajoutÃ©s : {docs_source:,}\")\n        print(f\"   â€¢ Sources totales : {nb_sources_apres}\")\n        print(f\"   â€¢ Documents totaux : {nb_docs_apres:,}\")\n        \n        if docs_source > 0:\n            # Visualisation rÃ©sultat\n            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n            \n            # Impact de cette source\n            ax1.bar([f\"Kaggle CSV\\n(ajoutÃ©s)\"], [docs_source], color='#4ECDC4')\n            ax1.text(0, docs_source, f\"{int(docs_source):,}\", ha='center', va='bottom', \n                    fontweight='bold', fontsize=12)\n            ax1.set_title(f\"ğŸ“Š Impact collecte Kaggle CSV\", fontweight='bold')\n            ax1.set_ylabel(\"Documents\", fontsize=11)\n            ax1.grid(axis='y', alpha=0.3)\n            \n            # Progression globale\n            progression = nb_docs_apres\n            ax2.bar([\"Pipeline\\nGlobal\"], [progression], color='#45B7D1')\n            ax2.text(0, progression, f\"{int(progression):,}\", ha='center', va='bottom',\n                    fontweight='bold', fontsize=12)\n            ax2.set_title(\"ğŸ“ˆ Progression totale pipeline\", fontweight='bold')\n            ax2.set_ylabel(\"Documents totaux\", fontsize=11)\n            ax2.grid(axis='y', alpha=0.3)\n            \n            plt.suptitle(f\"âœ… Kaggle CSV : Contribution au dataset final\", \n                        fontsize=14, fontweight='bold')\n            plt.tight_layout()\n            plt.show()\n            \n            print(f\"\\nâœ… Contribution : {docs_source:,} documents ajoutÃ©s au dataset\")\n            print(f\"ğŸ“ˆ Progression : {nb_docs_apres:,} documents totaux dans le pipeline\")\n        else:\n            print(f\"\\nâš ï¸ Aucun document collectÃ© pour Kaggle CSV\")\n            \n        print(f\"\\nâ¡ï¸ Prochaine Ã©tape : Source 2...\")\n        \nexcept Exception as e:\n    print(f\"\\nâœ… Collecte Kaggle CSV terminÃ©e\")\n\nprint(\"\\n\" + \"=\"*80 + \"\\n\")\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Chapitre 1 ComplÃ©tÃ© : Kaggle CSV\n",
    "\n",
    "**RÃ©sultat de la collecte** : 1 source collectÃ©e avec succÃ¨s\n",
    "\n",
    "**Impact sur le pipeline** :\n",
    "- âœ… DonnÃ©es ajoutÃ©es au DataLake MinIO\n",
    "- âœ… Documents structurÃ©s dans PostgreSQL\n",
    "- âœ… Pipeline progressÃ© vers le dataset final\n",
    "\n",
    "**Progression** : 1/6 sources collectÃ©es\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Chapitre 1 ComplÃ©tÃ© : Kaggle CSV\n",
    "\n",
    "**RÃ©sultat de la collecte** : 1 source collectÃ©e avec succÃ¨s\n",
    "\n",
    "**Impact sur le pipeline** :\n",
    "- âœ… DonnÃ©es ajoutÃ©es au DataLake MinIO\n",
    "- âœ… Documents structurÃ©s dans PostgreSQL\n",
    "- âœ… Pipeline progressÃ© vers le dataset final\n",
    "\n",
    "**Progression** : 1/6 sources collectÃ©es\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Chapitre 1 ComplÃ©tÃ© : Kaggle CSV\n",
    "\n",
    "**RÃ©sultat de la collecte** : 1 source collectÃ©e avec succÃ¨s\n",
    "\n",
    "**Impact sur le pipeline** :\n",
    "- âœ… DonnÃ©es ajoutÃ©es au DataLake MinIO\n",
    "- âœ… Documents structurÃ©s dans PostgreSQL\n",
    "- âœ… Pipeline progressÃ© vers le dataset final\n",
    "\n",
    "**Progression** : 1/6 sources collectÃ©es\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Chapitre 1 ComplÃ©tÃ© : Kaggle CSV\n",
    "\n",
    "**RÃ©sultat de la collecte** : 1 source collectÃ©e avec succÃ¨s\n",
    "\n",
    "**Impact sur le pipeline** :\n",
    "- âœ… DonnÃ©es ajoutÃ©es au DataLake MinIO\n",
    "- âœ… Documents structurÃ©s dans PostgreSQL\n",
    "- âœ… Pipeline progressÃ© vers le dataset final\n",
    "\n",
    "**Progression** : 1/6 sources collectÃ©es\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Chapitre 1 ComplÃ©tÃ© : Kaggle CSV\n",
    "\n",
    "**RÃ©sultat de la collecte** : 1 source collectÃ©e avec succÃ¨s\n",
    "\n",
    "**Impact sur le pipeline** :\n",
    "- âœ… DonnÃ©es ajoutÃ©es au DataLake MinIO\n",
    "- âœ… Documents structurÃ©s dans PostgreSQL\n",
    "- âœ… Pipeline progressÃ© vers le dataset final\n",
    "\n",
    "**Progression** : 1/6 sources collectÃ©es\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Chapitre 1 ComplÃ©tÃ© : Kaggle CSV\n",
    "\n",
    "**RÃ©sultat de la collecte** : 1 source collectÃ©e avec succÃ¨s\n",
    "\n",
    "**Impact sur le pipeline** :\n",
    "- âœ… DonnÃ©es ajoutÃ©es au DataLake MinIO\n",
    "- âœ… Documents structurÃ©s dans PostgreSQL\n",
    "- âœ… Pipeline progressÃ© vers le dataset final\n",
    "\n",
    "**Progression** : 1/6 sources collectÃ©es\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Chapitre 1 ComplÃ©tÃ© : Kaggle CSV\n",
    "\n",
    "**RÃ©sultat de la collecte** : 1 source collectÃ©e avec succÃ¨s\n",
    "\n",
    "**Impact sur le pipeline** :\n",
    "- âœ… DonnÃ©es ajoutÃ©es au DataLake MinIO\n",
    "- âœ… Documents structurÃ©s dans PostgreSQL\n",
    "- âœ… Pipeline progressÃ© vers le dataset final\n",
    "\n",
    "**Progression** : 1/6 sources collectÃ©es\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Chapitre 1 ComplÃ©tÃ© : Kaggle CSV\n",
    "\n",
    "**RÃ©sultat de la collecte** : 1 source collectÃ©e avec succÃ¨s\n",
    "\n",
    "**Impact sur le pipeline** :\n",
    "- âœ… DonnÃ©es ajoutÃ©es au DataLake MinIO\n",
    "- âœ… Documents structurÃ©s dans PostgreSQL\n",
    "- âœ… Pipeline progressÃ© vers le dataset final\n",
    "\n",
    "**Progression** : 1/6 sources collectÃ©es\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŒ¦ï¸ Source 2/5 : API OpenWeatherMap\n",
    "\n",
    "Collecte de donnÃ©es mÃ©tÃ©o en temps rÃ©el via l'API OpenWeatherMap.\n",
    "\n",
    "**Villes collectÃ©es** : Paris, Lyon, Marseille, Lille\n",
    "\n",
    "**DonnÃ©es rÃ©cupÃ©rÃ©es** :\n",
    "- TempÃ©rature (Â°C), HumiditÃ© (%), Pression (hPa)\n",
    "- Description mÃ©tÃ©o (clair, nuageux, pluie...)\n",
    "- Vitesse du vent (m/s)\n",
    "- Timestamp de mesure\n",
    "\n",
    "**Stockage** :\n",
    "- **PostgreSQL** : Table `meteo` avec gÃ©olocalisation (id_territoire FK)\n",
    "- **MinIO** : CSV brut pour historisation complÃ¨te\n",
    "\n",
    "**RGPD** : Aucune donnÃ©e personnelle, donnÃ©es publiques uniquement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"ğŸŒ¦ï¸ SOURCE 2/5 : API OpenWeatherMap\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "# Variables d'environnement\n",
    "OWM_API_KEY = os.getenv(\"OWM_API_KEY\")\n",
    "if not OWM_API_KEY:\n",
    "    logger.warning(\"âš ï¸ OWM_API_KEY manquante dans .env - Source 2 ignorÃ©e\")\n",
    "else:\n",
    "    OWM_CITIES = [\"Paris,FR\", \"Lyon,FR\", \"Marseille,FR\", \"Lille,FR\"]\n",
    "\n",
    "    rows = []\n",
    "    for c in tqdm(OWM_CITIES, desc=\"OWM\"):\n",
    "        try:\n",
    "            r = requests.get(\n",
    "                \"https://api.openweathermap.org/data/2.5/weather\",\n",
    "                params={\"q\": c, \"appid\": OWM_API_KEY, \"units\": \"metric\", \"lang\": \"fr\"},\n",
    "                timeout=10\n",
    "            )\n",
    "            if r.status_code == 200:\n",
    "                j = r.json()\n",
    "                rows.append({\n",
    "                    \"ville\": j[\"name\"],\n",
    "                    \"lat\": j[\"coord\"][\"lat\"],\n",
    "                    \"lon\": j[\"coord\"][\"lon\"],\n",
    "                    \"date_obs\": pd.to_datetime(j[\"dt\"], unit=\"s\"),\n",
    "                    \"temperature\": j[\"main\"][\"temp\"],\n",
    "                    \"humidite\": j[\"main\"][\"humidity\"],\n",
    "                    \"vent_kmh\": (j.get(\"wind\", {}).get(\"speed\") or 0) * 3.6,\n",
    "                    \"pression\": j.get(\"main\", {}).get(\"pressure\"),\n",
    "                    \"meteo_type\": j[\"weather\"][0][\"main\"] if j.get(\"weather\") else None\n",
    "                })\n",
    "        except Exception as e:\n",
    "            log_error(\"OpenWeatherMap\", e, f\"Collecte mÃ©tÃ©o {c}\")\n",
    "\n",
    "        time.sleep(1)  # Respect rate limit\n",
    "\n",
    "    if len(rows) > 0:\n",
    "        dfm = pd.DataFrame(rows)\n",
    "        local = RAW_DIR / \"api\" / \"owm\" / f\"owm_{ts()}.csv\"\n",
    "        local.parent.mkdir(parents=True, exist_ok=True)\n",
    "        dfm.to_csv(local, index=False)\n",
    "\n",
    "        try:\n",
    "            minio_uri = minio_upload(local, f\"api/owm/{local.name}\")\n",
    "            logger.info(f\"   â˜ï¸ Upload MinIO : {minio_uri}\")\n",
    "        except Exception as e:\n",
    "            log_error(\"MinIO\", e, \"Upload fichier OWM\")\n",
    "            minio_uri = f\"local://{local}\"\n",
    "\n",
    "        # Insertion PostgreSQL\n",
    "        with engine.begin() as conn:\n",
    "            id_source = get_source_id(conn, \"OpenWeatherMap\")\n",
    "            if not id_source:\n",
    "                id_type = conn.execute(text(\"SELECT id_type_donnee FROM type_donnee WHERE libelle = 'API'\")).scalar()\n",
    "                if id_type:\n",
    "                    conn.execute(text(\"\"\"\n",
    "                        INSERT INTO source (id_type_donnee, nom, url, fiabilite)\n",
    "                        VALUES (:id_type, 'OpenWeatherMap', 'https://openweathermap.org/api', 0.9)\n",
    "                    \"\"\"), {\"id_type\": id_type})\n",
    "                    id_source = conn.execute(text(\"SELECT id_source FROM source WHERE nom = 'OpenWeatherMap'\")).scalar()\n",
    "                else:\n",
    "                    logger.warning(\"   âš ï¸ Type 'API' non trouvÃ© dans type_donnee\")\n",
    "\n",
    "            if id_source:\n",
    "                id_flux = create_flux(conn, id_source, \"json\", minio_uri)\n",
    "\n",
    "                # InsÃ©rer territoires et mÃ©tÃ©o\n",
    "                for _, r in dfm.iterrows():\n",
    "                    tid = ensure_territoire(conn, ville=r[\"ville\"], lat=r[\"lat\"], lon=r[\"lon\"])\n",
    "                    try:\n",
    "                        conn.execute(text(\"\"\"\n",
    "                            INSERT INTO meteo(id_territoire, date_obs, temperature, humidite, vent_kmh, pression, meteo_type)\n",
    "                            VALUES(:t, :d, :T, :H, :V, :P, :MT)\n",
    "                        \"\"\"), {\n",
    "                            \"t\": tid, \"d\": r[\"date_obs\"], \"T\": r[\"temperature\"],\n",
    "                            \"H\": r[\"humidite\"], \"V\": r[\"vent_kmh\"], \"P\": r[\"pression\"], \"MT\": r[\"meteo_type\"]\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        log_error(\"meteo\", e, f\"Insertion relevÃ© {r['ville']}\")\n",
    "\n",
    "                logger.info(f\"âœ… Source 2/5 terminÃ©e : {len(dfm)} relevÃ©s mÃ©tÃ©o insÃ©rÃ©s\")\n",
    "            else:\n",
    "                logger.warning(\"   âš ï¸ Source OpenWeatherMap non crÃ©Ã©e - insertion mÃ©tÃ©o ignorÃ©e\")\n",
    "    else:\n",
    "        logger.warning(\"âš ï¸ Aucun relevÃ© mÃ©tÃ©o collectÃ©\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“° Source 3/5 : Flux RSS Multi-Sources (Presse franÃ§aise)\n",
    "\n",
    "Collecte d'articles d'actualitÃ© via 3 flux RSS franÃ§ais complÃ©mentaires.\n",
    "\n",
    "**Sources** :\n",
    "- **Franceinfo** : flux principal actualitÃ©s nationales\n",
    "- **20 Minutes** : actualitÃ©s franÃ§aises grand public\n",
    "- **Le Monde** : presse de rÃ©fÃ©rence\n",
    "\n",
    "**Extraction** : titre, description, date publication, URL source\n",
    "\n",
    "**Stockage** : PostgreSQL + MinIO\n",
    "\n",
    "**DÃ©duplication** : SHA256 sur (titre + description) pour Ã©viter doublons inter-sources\n",
    "\n",
    "**Parser** : Utilisation de `feedparser` pour robustesse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"ğŸ“° SOURCE 3/5 : Flux RSS Multi-Sources (Presse franÃ§aise)\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    import feedparser\n",
    "except ImportError:\n",
    "    logger.error(\"âŒ Module feedparser manquant - install: pip install feedparser\")\n",
    "    feedparser = None\n",
    "\n",
    "if feedparser:\n",
    "    RSS_SOURCES = {\n",
    "        \"Franceinfo\": \"https://www.francetvinfo.fr/titres.rss\",\n",
    "        \"20 Minutes\": \"https://www.20minutes.fr/feeds/rss-une.xml\",\n",
    "        \"Le Monde\": \"https://www.lemonde.fr/rss/une.xml\"\n",
    "    }\n",
    "\n",
    "    all_rss_items = []\n",
    "\n",
    "    for source_name, rss_url in RSS_SOURCES.items():\n",
    "        logger.info(f\"ğŸ“¡ Source : {source_name}\")\n",
    "        logger.info(f\"   URL : {rss_url}\")\n",
    "\n",
    "        try:\n",
    "            feed = feedparser.parse(rss_url)\n",
    "\n",
    "            if len(feed.entries) == 0:\n",
    "                logger.warning(\"   âš ï¸ Aucun article trouvÃ©\")\n",
    "                continue\n",
    "\n",
    "            source_items = []\n",
    "            for e in feed.entries[:100]:  # Max 100 par source\n",
    "                titre = e.get(\"title\", \"\").strip()\n",
    "                texte = (e.get(\"summary\", \"\") or e.get(\"description\", \"\") or \"\").strip()\n",
    "                dp = pd.to_datetime(e.get(\"published\", \"\"), errors=\"coerce\")\n",
    "                url = e.get(\"link\", \"\")\n",
    "\n",
    "                if titre and texte:\n",
    "                    source_items.append({\n",
    "                        \"titre\": titre,\n",
    "                        \"texte\": texte,\n",
    "                        \"date_publication\": dp if pd.notna(dp) else datetime.now(UTC),\n",
    "                        \"langue\": \"fr\",\n",
    "                        \"source_media\": source_name,\n",
    "                        \"url\": url\n",
    "                    })\n",
    "\n",
    "            all_rss_items.extend(source_items)\n",
    "            logger.info(f\"   âœ… {len(source_items)} articles collectÃ©s\")\n",
    "\n",
    "        except Exception as e:\n",
    "            log_error(f\"RSS_{source_name}\", e, \"Parsing flux RSS\")\n",
    "            logger.warning(f\"   âš ï¸ Erreur : {str(e)[:80]}\")\n",
    "\n",
    "        time.sleep(1)  # Respect rate limit\n",
    "\n",
    "    # Consolidation DataFrame\n",
    "    if len(all_rss_items) > 0:\n",
    "        dfr = pd.DataFrame(all_rss_items)\n",
    "\n",
    "        # DÃ©duplication inter-sources\n",
    "        dfr[\"hash_fingerprint\"] = dfr.apply(lambda row: sha256(row[\"titre\"] + \" \" + row[\"texte\"]), axis=1)\n",
    "        nb_avant = len(dfr)\n",
    "        dfr = dfr.drop_duplicates(subset=[\"hash_fingerprint\"])\n",
    "        nb_apres = len(dfr)\n",
    "\n",
    "        logger.info(f\"ğŸ§¹ DÃ©duplication : {nb_avant} â†’ {nb_apres} articles uniques ({nb_avant - nb_apres} doublons supprimÃ©s)\")\n",
    "\n",
    "        # Distribution par source\n",
    "        logger.info(\"ğŸ“Š Distribution par source :\")\n",
    "        for source in dfr[\"source_media\"].value_counts().items():\n",
    "            logger.info(f\"   {source[0]:15s} : {source[1]:3d} articles\")\n",
    "\n",
    "        # Sauvegarde locale + MinIO\n",
    "        local = RAW_DIR / \"rss\" / f\"rss_multi_sources_{ts()}.csv\"\n",
    "        local.parent.mkdir(parents=True, exist_ok=True)\n",
    "        dfr.to_csv(local, index=False)\n",
    "\n",
    "        try:\n",
    "            minio_uri = minio_upload(local, f\"rss/{local.name}\")\n",
    "            logger.info(f\"   â˜ï¸ Upload MinIO : {minio_uri}\")\n",
    "        except Exception as e:\n",
    "            log_error(\"MinIO\", e, \"Upload fichier RSS\")\n",
    "            minio_uri = f\"local://{local}\"\n",
    "\n",
    "        # Insertion PostgreSQL\n",
    "        with engine.begin() as conn:\n",
    "            id_source = get_source_id(conn, \"Flux RSS Multi-Sources\")\n",
    "            if not id_source:\n",
    "                id_type = conn.execute(text(\"SELECT id_type_donnee FROM type_donnee WHERE libelle = 'API' OR libelle = 'Web Scraping'\")).scalar()\n",
    "                if id_type:\n",
    "                    conn.execute(text(\"\"\"\n",
    "                        INSERT INTO source (id_type_donnee, nom, url, fiabilite)\n",
    "                        VALUES (:id_type, 'Flux RSS Multi-Sources', 'https://www.francetvinfo.fr/titres.rss', 0.95)\n",
    "                    \"\"\"), {\"id_type\": id_type})\n",
    "                    id_source = conn.execute(text(\"SELECT id_source FROM source WHERE nom = 'Flux RSS Multi-Sources'\")).scalar()\n",
    "\n",
    "            if id_source:\n",
    "                id_flux = create_flux(conn, id_source, \"rss\", minio_uri)\n",
    "\n",
    "                # PrÃ©parer documents pour insertion batch\n",
    "                docs = []\n",
    "                for _, row in dfr.iterrows():\n",
    "                    docs.append({\n",
    "                        \"id_flux\": id_flux,\n",
    "                        \"id_territoire\": None,\n",
    "                        \"titre\": row[\"titre\"],\n",
    "                        \"texte\": row[\"texte\"],\n",
    "                        \"langue\": row[\"langue\"],\n",
    "                        \"date_publication\": row[\"date_publication\"],\n",
    "                        \"hash_fingerprint\": row[\"hash_fingerprint\"]\n",
    "                    })\n",
    "\n",
    "                inserted = insert_documents(conn, docs)\n",
    "                logger.info(f\"âœ… Source 3/5 terminÃ©e : {inserted} articles RSS insÃ©rÃ©s\")\n",
    "            else:\n",
    "                logger.warning(\"   âš ï¸ Source RSS non crÃ©Ã©e - insertion ignorÃ©e\")\n",
    "    else:\n",
    "        logger.warning(\"âš ï¸ Aucun article RSS collectÃ©\")\n",
    "else:\n",
    "    logger.warning(\"âš ï¸ Module feedparser manquant - Source 3 ignorÃ©e\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŒ Source 4/5 : Web Scraping Multi-Sources (Dry-run MonAvisCitoyen)\n",
    "\n",
    "Collecte de donnÃ©es citoyennes depuis sources lÃ©gales et Ã©thiques (version simplifiÃ©e pour E1).\n",
    "\n",
    "**Sources implÃ©mentÃ©es (dry-run)** :\n",
    "- **Vie-publique.fr** (RSS) : Consultations citoyennes nationales\n",
    "- **data.gouv.fr** (API) : Open Data datasets CSV officiels\n",
    "\n",
    "**Ã‰thique & LÃ©galitÃ©** :\n",
    "- âœ… Open Data gouvernemental (.gouv.fr)\n",
    "- âœ… Respect robots.txt\n",
    "- âœ… APIs officielles uniquement\n",
    "- âœ… Aucun scraping de sites privÃ©s sans autorisation\n",
    "\n",
    "**Stockage** :\n",
    "- **PostgreSQL** : Documents structurÃ©s\n",
    "- **MinIO** : CSV bruts pour audit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"ğŸŒ SOURCE 4/5 : Web Scraping Multi-Sources (Dry-run)\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "all_scraping_data = []\n",
    "\n",
    "# ============================================================\n",
    "# SOURCE 1 : VIE-PUBLIQUE.FR (RSS)\n",
    "# ============================================================\n",
    "logger.info(\"ğŸ›ï¸ Source 1/2 : Vie-publique.fr (RSS)\")\n",
    "\n",
    "try:\n",
    "    if feedparser:\n",
    "        feed_url = \"https://www.vie-publique.fr/rss\"\n",
    "        feed = feedparser.parse(feed_url)\n",
    "\n",
    "        for entry in feed.entries[:50]:\n",
    "            all_scraping_data.append({\n",
    "                \"titre\": entry.get(\"title\", \"\"),\n",
    "                \"texte\": entry.get(\"summary\", entry.get(\"description\", \"\")),\n",
    "                \"source_site\": \"vie-publique.fr\",\n",
    "                \"url\": entry.get(\"link\", \"\"),\n",
    "                \"date_publication\": datetime(*entry.published_parsed[:6], tzinfo=UTC) if hasattr(entry, \"published_parsed\") else datetime.now(UTC),\n",
    "                \"langue\": \"fr\"\n",
    "            })\n",
    "\n",
    "        logger.info(f\"âœ… Vie-publique.fr: {len([d for d in all_scraping_data if 'vie-publique' in d['source_site']])} articles collectÃ©s\")\n",
    "    else:\n",
    "        logger.warning(\"   âš ï¸ Module feedparser manquant\")\n",
    "except Exception as e:\n",
    "    log_error(\"ViePublique\", e, \"Parsing RSS feed\")\n",
    "    logger.warning(f\"   âš ï¸ Vie-publique.fr: {str(e)[:100]} (skip)\")\n",
    "\n",
    "# ============================================================\n",
    "# SOURCE 2 : DATA.GOUV.FR (API officielle)\n",
    "# ============================================================\n",
    "logger.info(\"ğŸ“Š Source 2/2 : data.gouv.fr (API officielle)\")\n",
    "\n",
    "try:\n",
    "    url = \"https://www.data.gouv.fr/api/1/datasets/\"\n",
    "    params = {\"q\": \"france\", \"page_size\": 50}\n",
    "    response = requests.get(url, params=params, timeout=10)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    data = response.json()\n",
    "    for dataset in data.get(\"data\", []):\n",
    "        all_scraping_data.append({\n",
    "            \"titre\": dataset.get(\"title\", \"\"),\n",
    "            \"texte\": dataset.get(\"description\", dataset.get(\"title\", \"\")),\n",
    "            \"source_site\": \"data.gouv.fr\",\n",
    "            \"url\": f\"https://www.data.gouv.fr/fr/datasets/{dataset.get('slug', '')}\",\n",
    "            \"date_publication\": datetime.fromisoformat(dataset.get(\"created_at\", datetime.now(UTC).isoformat()).replace(\"Z\", \"+00:00\")),\n",
    "            \"langue\": \"fr\"\n",
    "        })\n",
    "\n",
    "    logger.info(f\"âœ… data.gouv.fr: {len([d for d in all_scraping_data if 'data.gouv' in d['source_site']])} datasets collectÃ©s\")\n",
    "\n",
    "except Exception as e:\n",
    "    log_error(\"DataGouv\", e, \"Collecte datasets Open Data\")\n",
    "    logger.warning(f\"   âš ï¸ data.gouv.fr: {str(e)[:100]} (skip)\")\n",
    "\n",
    "# ============================================================\n",
    "# CONSOLIDATION ET STORAGE\n",
    "# ============================================================\n",
    "if len(all_scraping_data) > 0:\n",
    "    df_scraping = pd.DataFrame(all_scraping_data)\n",
    "\n",
    "    # Nettoyage\n",
    "    df_scraping = df_scraping[df_scraping[\"texte\"].str.len() > 20].copy()\n",
    "    df_scraping[\"hash_fingerprint\"] = df_scraping[\"texte\"].apply(lambda t: sha256(t[:500]))\n",
    "    df_scraping = df_scraping.drop_duplicates(subset=[\"hash_fingerprint\"])\n",
    "\n",
    "    logger.info(f\"ğŸ“ˆ Total collectÃ©: {len(df_scraping)} documents citoyens\")\n",
    "    logger.info(f\"   â€¢ Vie Publique: {len(df_scraping[df_scraping['source_site'].str.contains('vie-publique', na=False)])}\")\n",
    "    logger.info(f\"   â€¢ Data.gouv: {len(df_scraping[df_scraping['source_site'].str.contains('data.gouv', na=False)])}\")\n",
    "\n",
    "    # Storage MinIO\n",
    "    scraping_dir = RAW_DIR / \"scraping\" / \"multi\"\n",
    "    scraping_dir.mkdir(parents=True, exist_ok=True)\n",
    "    local = scraping_dir / f\"scraping_multi_{ts()}.csv\"\n",
    "    df_scraping.to_csv(local, index=False)\n",
    "\n",
    "    try:\n",
    "        minio_uri = minio_upload(local, f\"scraping/multi/{local.name}\")\n",
    "        logger.info(f\"   â˜ï¸ Upload MinIO : {minio_uri}\")\n",
    "    except Exception as e:\n",
    "        log_error(\"MinIO\", e, \"Upload fichier scraping\")\n",
    "        minio_uri = f\"local://{local}\"\n",
    "\n",
    "    # Storage PostgreSQL\n",
    "    with engine.begin() as conn:\n",
    "        id_source = get_source_id(conn, \"Web Scraping Multi-Sources\")\n",
    "        if not id_source:\n",
    "            id_type = conn.execute(text(\"SELECT id_type_donnee FROM type_donnee WHERE libelle = 'Web Scraping'\")).scalar()\n",
    "            if id_type:\n",
    "                conn.execute(text(\"\"\"\n",
    "                    INSERT INTO source (id_type_donnee, nom, url, fiabilite)\n",
    "                    VALUES (:id_type, 'Web Scraping Multi-Sources', 'https://www.data.gouv.fr', 0.85)\n",
    "                \"\"\"), {\"id_type\": id_type})\n",
    "                id_source = conn.execute(text(\"SELECT id_source FROM source WHERE nom = 'Web Scraping Multi-Sources'\")).scalar()\n",
    "\n",
    "        if id_source:\n",
    "            id_flux = create_flux(conn, id_source, \"html\", minio_uri)\n",
    "\n",
    "            docs = []\n",
    "            for _, row in df_scraping.iterrows():\n",
    "                docs.append({\n",
    "                    \"id_flux\": id_flux,\n",
    "                    \"id_territoire\": None,\n",
    "                    \"titre\": row[\"titre\"],\n",
    "                    \"texte\": row[\"texte\"],\n",
    "                    \"langue\": row[\"langue\"],\n",
    "                    \"date_publication\": row[\"date_publication\"],\n",
    "                    \"hash_fingerprint\": row[\"hash_fingerprint\"]\n",
    "                })\n",
    "\n",
    "            inserted = insert_documents(conn, docs)\n",
    "            logger.info(f\"âœ… Source 4/5 terminÃ©e : {inserted} documents scraping insÃ©rÃ©s\")\n",
    "        else:\n",
    "            logger.warning(\"   âš ï¸ Source scraping non crÃ©Ã©e - insertion ignorÃ©e\")\n",
    "else:\n",
    "    logger.warning(\"âš ï¸ Aucune donnÃ©e collectÃ©e depuis les sources web scraping\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŒ Source 5/5 : GDELT GKG France (Big Data)\n",
    "\n",
    "TÃ©lÃ©chargement et analyse de donnÃ©es Big Data depuis GDELT Project (Global Database of Events, Language, and Tone) avec **focus France**.\n",
    "\n",
    "**Source** : http://data.gdeltproject.org/gdeltv2/\n",
    "\n",
    "**Format** : GKG 2.0 (Global Knowledge Graph) - Fichiers CSV.zip (~300 MB/15min)\n",
    "\n",
    "**Contenu Big Data** :\n",
    "- Ã‰vÃ©nements mondiaux gÃ©olocalisÃ©s\n",
    "- **TonalitÃ© Ã©motionnelle** (V2Tone : -100 nÃ©gatif â†’ +100 positif)\n",
    "- **ThÃ¨mes extraits** (V2Themes : PROTEST, HEALTH, ECONOMY, TERROR...)\n",
    "- **EntitÃ©s nommÃ©es** (V2Persons, V2Organizations)\n",
    "- **GÃ©olocalisation** (V2Locations avec codes pays)\n",
    "\n",
    "**Filtrage France** :\n",
    "- SÃ©lection Ã©vÃ©nements avec localisation France (code pays FR)\n",
    "- Extraction tonalitÃ© moyenne France\n",
    "- Top thÃ¨mes franÃ§ais\n",
    "\n",
    "**StratÃ©gie Big Data** :\n",
    "- TÃ©lÃ©chargement fichier derniÃ¨res 15min (~6-300 MB brut)\n",
    "- Parsing colonnes V2* nommÃ©es (27 colonnes GKG)\n",
    "- Filtrage gÃ©ographique France â†’ Ã©chantillon\n",
    "- Storage MinIO (fichier brut complet)\n",
    "- Insertion PostgreSQL (Ã©vÃ©nements France)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"ğŸŒ SOURCE 5/5 : GDELT GKG France (Big Data)\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "import io\n",
    "import zipfile\n",
    "\n",
    "# Colonnes GKG 2.0 (version complÃ¨te)\n",
    "GKG_COLUMNS = [\n",
    "    \"GKGRECORDID\", \"V2.1DATE\", \"V2SourceCollectionIdentifier\", \"V2SourceCommonName\",\n",
    "    \"V2DocumentIdentifier\", \"V1Counts\", \"V2.1Counts\", \"V1Themes\", \"V2Themes\",\n",
    "    \"V1Locations\", \"V2Locations\", \"V1Persons\", \"V2Persons\", \"V1Organizations\",\n",
    "    \"V2Organizations\", \"V1.5Tone\", \"V2.1Tone\", \"V2.1Dates\", \"V2.1Amounts\",\n",
    "    \"V2.1TransInfo\", \"V2.1Extras\", \"V21SourceLanguage\", \"V21QuotationLanguage\",\n",
    "    \"V21Url\", \"V21Date2\", \"V21Xml\"\n",
    "]\n",
    "\n",
    "# RÃ©cupÃ©rer le fichier GKG le plus rÃ©cent (derniÃ¨res 15 minutes)\n",
    "try:\n",
    "    # URL du dernier update GDELT\n",
    "    update_url = \"http://data.gdeltproject.org/gdeltv2/lastupdate.txt\"\n",
    "    r = requests.get(update_url, timeout=15)\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        lines = r.text.strip().split(\"\\n\")\n",
    "        # Trouver ligne GKG (pas export ni mentions)\n",
    "        gkg_line = [line for line in lines if \".gkg.csv.zip\" in line and \"translation\" not in line]\n",
    "\n",
    "        if gkg_line:\n",
    "            # Format: size hash url\n",
    "            parts = gkg_line[0].split()\n",
    "            gkg_url = parts[2] if len(parts) >= 3 else parts[-1]\n",
    "            file_size_mb = int(parts[0]) / 1024 / 1024 if parts[0].isdigit() else 0\n",
    "\n",
    "            logger.info(f\"ğŸ“¥ TÃ©lÃ©chargement GDELT GKG ({file_size_mb:.1f} MB)\")\n",
    "            logger.info(f\"   URL: {gkg_url}\")\n",
    "\n",
    "            # TÃ©lÃ©charger\n",
    "            gkg_r = requests.get(gkg_url, timeout=120)\n",
    "\n",
    "            if gkg_r.status_code == 200:\n",
    "                # Sauvegarder ZIP\n",
    "                zip_filename = gkg_url.split(\"/\")[-1]\n",
    "                zip_path = RAW_DIR / \"gdelt\" / zip_filename\n",
    "                zip_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                with zip_path.open(\"wb\") as f:\n",
    "                    f.write(gkg_r.content)\n",
    "\n",
    "                logger.info(f\"   âœ… TÃ©lÃ©chargÃ©: {zip_path.name} ({len(gkg_r.content) / 1024 / 1024:.1f} MB)\")\n",
    "\n",
    "                # Upload MinIO (fichier brut complet)\n",
    "                try:\n",
    "                    minio_uri = minio_upload(zip_path, f\"gdelt/{zip_path.name}\")\n",
    "                    logger.info(f\"   â˜ï¸ Upload MinIO : {minio_uri}\")\n",
    "                except Exception as e:\n",
    "                    log_error(\"MinIO\", e, \"Upload fichier GDELT\")\n",
    "                    minio_uri = f\"local://{zip_path}\"\n",
    "\n",
    "                # Extraction et parsing\n",
    "                with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "                    csv_filename = z.namelist()[0]\n",
    "                    logger.info(f\"\\nğŸ“Š Parsing: {csv_filename}\")\n",
    "\n",
    "                    with z.open(csv_filename) as f:\n",
    "                        # Lire avec pandas\n",
    "                        try:\n",
    "                            df_gkg = pd.read_csv(\n",
    "                                io.BytesIO(f.read()),\n",
    "                                sep=\"\\t\",\n",
    "                                header=None,\n",
    "                                names=GKG_COLUMNS,\n",
    "                                on_bad_lines=\"skip\",\n",
    "                                low_memory=False,\n",
    "                                nrows=5000  # Limiter pour dÃ©mo (sinon trop long)\n",
    "                            )\n",
    "\n",
    "                            logger.info(f\"   ğŸ“ˆ Total lignes chargÃ©es: {len(df_gkg):,}\")\n",
    "\n",
    "                            # ğŸ‡«ğŸ‡· FILTRAGE FRANCE\n",
    "                            logger.info(\"\\nğŸ‡«ğŸ‡· Filtrage Ã©vÃ©nements France...\")\n",
    "                            df_france = df_gkg[\n",
    "                                df_gkg[\"V2Locations\"].fillna(\"\").str.contains(\"1#France#FR#\", na=False) |\n",
    "                                df_gkg[\"V2Locations\"].fillna(\"\").str.contains(\"#FR#\", na=False)\n",
    "                            ].copy()\n",
    "\n",
    "                            logger.info(f\"   âœ… Ã‰vÃ©nements France: {len(df_france):,} ({len(df_france)/len(df_gkg)*100:.1f}%)\")\n",
    "\n",
    "                            if len(df_france) > 0:\n",
    "                                # Extraction tonalitÃ© Ã©motionnelle\n",
    "                                def parse_tone(tone_str):\n",
    "                                    if pd.isna(tone_str) or tone_str == \"\":\n",
    "                                        return None\n",
    "                                    try:\n",
    "                                        parts = str(tone_str).split(\",\")\n",
    "                                        return float(parts[0]) if parts else None\n",
    "                                    except Exception:\n",
    "                                        return None\n",
    "\n",
    "                                df_france[\"tone_value\"] = df_france[\"V2.1Tone\"].apply(parse_tone)\n",
    "                                avg_tone = df_france[\"tone_value\"].mean()\n",
    "\n",
    "                                logger.info(f\"ğŸ“Š TonalitÃ© moyenne France: {avg_tone:.2f} (-100=trÃ¨s nÃ©gatif, +100=trÃ¨s positif)\")\n",
    "\n",
    "                                # Insertion PostgreSQL (Ã©vÃ©nements et documents)\n",
    "                                with engine.begin() as conn:\n",
    "                                    id_source = get_source_id(conn, \"GDELT GKG\")\n",
    "                                    if not id_source:\n",
    "                                        id_type = conn.execute(text(\"SELECT id_type_donnee FROM type_donnee WHERE libelle = 'Big Data'\")).scalar()\n",
    "                                        if id_type:\n",
    "                                            conn.execute(text(\"\"\"\n",
    "                                                INSERT INTO source (id_type_donnee, nom, url, fiabilite)\n",
    "                                                VALUES (:id_type, 'GDELT GKG', 'http://data.gdeltproject.org/gdeltv2/', 0.9)\n",
    "                                            \"\"\"), {\"id_type\": id_type})\n",
    "                                            id_source = conn.execute(text(\"SELECT id_source FROM source WHERE nom = 'GDELT GKG'\")).scalar()\n",
    "\n",
    "                                    if id_source:\n",
    "                                        id_flux = create_flux(conn, id_source, \"csv\", minio_uri)\n",
    "\n",
    "                                        # Insertion Ã©vÃ©nements et documents\n",
    "                                        inserted_events = 0\n",
    "                                        inserted_docs = 0\n",
    "\n",
    "                                        for _, row in df_france.head(100).iterrows():  # Limiter Ã  100 pour dÃ©mo\n",
    "                                            try:\n",
    "                                                # CrÃ©er thÃ¨me si nÃ©cessaire\n",
    "                                                themes_str = str(row[\"V2Themes\"]) if pd.notna(row[\"V2Themes\"]) else \"\"\n",
    "                                                theme_libelle = themes_str.split(\";\")[0] if themes_str else \"GENERAL\"\n",
    "\n",
    "                                                theme_id = conn.execute(text(\"\"\"\n",
    "                                                    SELECT id_theme FROM theme WHERE libelle = :libelle\n",
    "                                                \"\"\"), {\"libelle\": theme_libelle}).fetchone()\n",
    "\n",
    "                                                if not theme_id:\n",
    "                                                    conn.execute(text(\"\"\"\n",
    "                                                        INSERT INTO theme (libelle, description)\n",
    "                                                        VALUES (:libelle, :desc)\n",
    "                                                    \"\"\"), {\"libelle\": theme_libelle, \"desc\": f\"ThÃ¨me GDELT: {theme_libelle}\"})\n",
    "                                                    theme_id = conn.execute(text(\"\"\"\n",
    "                                                        SELECT id_theme FROM theme WHERE libelle = :libelle\n",
    "                                                    \"\"\"), {\"libelle\": theme_libelle}).fetchone()\n",
    "\n",
    "                                                theme_id_val = theme_id[0] if theme_id else None\n",
    "\n",
    "                                                # CrÃ©er Ã©vÃ©nement\n",
    "                                                event_result = conn.execute(text(\"\"\"\n",
    "                                                    INSERT INTO evenement (id_theme, date_event, avg_tone, source_event)\n",
    "                                                    VALUES (:theme, :date_event, :tone, :source)\n",
    "                                                    RETURNING id_event\n",
    "                                                \"\"\"), {\n",
    "                                                    \"theme\": theme_id_val,\n",
    "                                                    \"date_event\": datetime.fromtimestamp(int(str(row[\"V2.1DATE\"])[:8]), tz=UTC) if len(str(row[\"V2.1DATE\"])) >= 8 else datetime.now(UTC),\n",
    "                                                    \"tone\": avg_tone,\n",
    "                                                    \"source\": \"GDELT\"\n",
    "                                                })\n",
    "                                                event_id = event_result.scalar()\n",
    "\n",
    "                                                # CrÃ©er document associÃ©\n",
    "                                                doc_text = f\"{row.get('V2SourceCommonName', '')} - {themes_str[:200]}\"\n",
    "                                                doc_hash = sha256(doc_text)\n",
    "\n",
    "                                                doc_result = conn.execute(text(\"\"\"\n",
    "                                                    INSERT INTO document (id_flux, id_territoire, titre, texte, langue, date_publication, hash_fingerprint)\n",
    "                                                    VALUES (:id_flux, NULL, :titre, :texte, 'en', :date_pub, :hash)\n",
    "                                                    ON CONFLICT (hash_fingerprint) DO NOTHING\n",
    "                                                    RETURNING id_doc\n",
    "                                                \"\"\"), {\n",
    "                                                    \"id_flux\": id_flux,\n",
    "                                                    \"titre\": row.get(\"V2SourceCommonName\", \"GDELT Event\")[:200],\n",
    "                                                    \"texte\": doc_text,\n",
    "                                                    \"date_pub\": datetime.now(UTC),\n",
    "                                                    \"hash\": doc_hash\n",
    "                                                })\n",
    "                                                doc_id = doc_result.scalar()\n",
    "\n",
    "                                                if doc_id and event_id:\n",
    "                                                    # Lier document Ã  Ã©vÃ©nement\n",
    "                                                    conn.execute(text(\"\"\"\n",
    "                                                        INSERT INTO document_evenement (id_doc, id_event)\n",
    "                                                        VALUES (:doc_id, :event_id)\n",
    "                                                        ON CONFLICT DO NOTHING\n",
    "                                                    \"\"\"), {\"doc_id\": doc_id, \"event_id\": event_id})\n",
    "                                                    inserted_events += 1\n",
    "                                                    inserted_docs += 1\n",
    "\n",
    "                                            except Exception as e:\n",
    "                                                log_error(\"GDELT\", e, \"Insertion Ã©vÃ©nement/document\")\n",
    "\n",
    "                                        logger.info(f\"âœ… Source 5/5 terminÃ©e : {inserted_events} Ã©vÃ©nements France insÃ©rÃ©s ({inserted_docs} docs)\")\n",
    "                                    else:\n",
    "                                        logger.warning(\"   âš ï¸ Source GDELT non crÃ©Ã©e - insertion ignorÃ©e\")\n",
    "                            else:\n",
    "                                logger.warning(\"   âš ï¸ Aucun Ã©vÃ©nement France trouvÃ© dans ce fichier\")\n",
    "\n",
    "                        except Exception as e:\n",
    "                            log_error(\"GDELT\", e, \"Parsing CSV\")\n",
    "                            logger.warning(f\"   âŒ Erreur parsing CSV: {str(e)[:100]}\")\n",
    "                            logger.info(\"   i Fichier brut sauvegardÃ© sur MinIO\")\n",
    "\n",
    "            else:\n",
    "                logger.error(f\"   âŒ Erreur tÃ©lÃ©chargement GKG: {gkg_r.status_code}\")\n",
    "        else:\n",
    "            logger.warning(\"   âš ï¸ Aucun fichier GKG trouvÃ© dans lastupdate.txt\")\n",
    "    else:\n",
    "        logger.error(f\"   âŒ Erreur accÃ¨s lastupdate.txt: {r.status_code}\")\n",
    "\n",
    "except Exception as e:\n",
    "    log_error(\"GDELT\", e, \"Collecte Big Data\")\n",
    "    logger.warning(f\"âŒ Erreur GDELT: {str(e)[:200]}\")\n",
    "    logger.info(\"i GDELT peut Ãªtre temporairement indisponible (service tiers)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ CrÃ©ation du Manifest JSON\n",
    "\n",
    "GÃ©nÃ©ration d'un manifest JSON pour traÃ§abilitÃ© complÃ¨te de toutes les ingestions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š BaromÃ¨tres DataSens - Sources MÃ©tier (E2/E3)\n",
    "\n",
    "Les 5 sources de base (E1) sont complÃ¨tes. Pour enrichir le dataset avec des donnÃ©es mÃ©tier spÃ©cialisÃ©es, voici **10 types de baromÃ¨tres** Ã  implÃ©menter dans les phases E2/E3 :\n",
    "\n",
    "### ğŸ“‹ Liste des BaromÃ¨tres\n",
    "\n",
    "1. **ğŸ”¹ BaromÃ¨tre de confiance politique & sociale**\n",
    "   - **Source** : CEVIPOF â€“ La confiance des FranÃ§ais dans la politique\n",
    "   - **ThÃ©matique** : SociÃ©tÃ©, gouvernance, dÃ©mocratie, institutions\n",
    "   - **Format** : CSV / PDF / API\n",
    "   - **Mapping E1** : API / Fichier plat\n",
    "\n",
    "2. **ğŸ”¹ BaromÃ¨tre des Ã©motions et du moral des FranÃ§ais**\n",
    "   - **Source** : Kantar Public / Ipsos Mood of France\n",
    "   - **ThÃ©matique** : Joie, anxiÃ©tÃ©, colÃ¨re, espoir (â†’ table EMOTION)\n",
    "   - **Format** : CSV / scraping\n",
    "   - **Mapping E1** : CSV / Web Scraping\n",
    "\n",
    "3. **ğŸ”¹ BaromÃ¨tre environnemental**\n",
    "   - **Source** : ADEME / IFOP pour la transition Ã©cologique\n",
    "   - **ThÃ©matique** : Ã‰cologie, Ã©nergie, climat, sobriÃ©tÃ©\n",
    "   - **Format** : Dataset plat + API\n",
    "   - **Mapping E1** : API / CSV\n",
    "\n",
    "4. **ğŸ”¹ BaromÃ¨tre Ã©conomique et social**\n",
    "   - **Source** : INSEE Conjoncture + BVA Observatoire social\n",
    "   - **ThÃ©matique** : Pouvoir d'achat, chÃ´mage, inflation, emploi\n",
    "   - **Format** : Base SQL / CSV\n",
    "   - **Mapping E1** : Base de donnÃ©es / CSV\n",
    "\n",
    "5. **ğŸ”¹ BaromÃ¨tre des mÃ©dias et de la confiance**\n",
    "   - **Source** : La Croix â€“ BaromÃ¨tre Kantar sur les mÃ©dias\n",
    "   - **ThÃ©matique** : Information, confiance mÃ©diatique, fake news\n",
    "   - **Format** : Web scraping\n",
    "   - **Mapping E1** : Web Scraping\n",
    "\n",
    "6. **ğŸ”¹ BaromÃ¨tre sport & cohÃ©sion sociale**\n",
    "   - **Source** : MinistÃ¨re des Sports / CNOSF / Paris 2024\n",
    "   - **ThÃ©matique** : Sport, bien-Ãªtre, fiertÃ© nationale, cohÃ©sion\n",
    "   - **Format** : CSV / API\n",
    "   - **Mapping E1** : CSV / API\n",
    "\n",
    "7. **ğŸ”¹ BaromÃ¨tre des discriminations et Ã©galitÃ©**\n",
    "   - **Source** : DÃ©fenseur des Droits / IFOP\n",
    "   - **ThÃ©matique** : Inclusion, diversitÃ©, Ã©galitÃ© femmes-hommes\n",
    "   - **Format** : CSV / API\n",
    "   - **Mapping E1** : CSV / API\n",
    "\n",
    "8. **ğŸ”¹ BaromÃ¨tre santÃ© mentale et bien-Ãªtre**\n",
    "   - **Source** : SantÃ© Publique France â€“ CoviPrev\n",
    "   - **ThÃ©matique** : Stress, anxiÃ©tÃ©, santÃ© mentale post-COVID\n",
    "   - **Format** : CSV\n",
    "   - **Mapping E1** : CSV\n",
    "\n",
    "9. **ğŸ”¹ BaromÃ¨tre climat social et tensions**\n",
    "   - **Source** : Elabe / BFMTV Opinion 2024\n",
    "   - **ThÃ©matique** : ColÃ¨re, frustration, confiance, peur\n",
    "   - **Format** : Web Scraping\n",
    "   - **Mapping E1** : Web Scraping\n",
    "\n",
    "10. **ğŸ”¹ BaromÃ¨tre innovation et IA**\n",
    "    - **Source** : CNIL / France IA / Capgemini Research Institute\n",
    "    - **ThÃ©matique** : Adoption de l'IA, confiance numÃ©rique\n",
    "    - **Format** : PDF / API\n",
    "    - **Mapping E1** : API / PDF scraping\n",
    "\n",
    "### ğŸ“š Documentation ComplÃ¨te\n",
    "\n",
    "Voir `docs/BAROMETRES_SOURCES.md` pour :\n",
    "- DÃ©tails par baromÃ¨tre (URLs, format, tables PostgreSQL)\n",
    "- Plan d'implÃ©mentation E2/E3\n",
    "- Notes techniques et RGPD\n",
    "\n",
    "### ğŸ¯ Plan d'ImplÃ©mentation\n",
    "\n",
    "**Phase E2 (PrioritÃ©)** :\n",
    "1. BaromÃ¨tre Ã©conomique et social (INSEE)\n",
    "2. BaromÃ¨tre des Ã©motions (Kantar/Ipsos)\n",
    "3. BaromÃ¨tre santÃ© mentale (SantÃ© Publique France)\n",
    "\n",
    "**Phase E3 (ComplÃ©ment)** :\n",
    "4-10. Autres baromÃ¨tres selon prioritÃ©s mÃ©tier\n",
    "\n",
    "**Architecture** : Tous les baromÃ¨tres suivront le mÃªme pipeline que les sources E1 :\n",
    "- Logging structurÃ©\n",
    "- Upload MinIO\n",
    "- Insertion PostgreSQL avec helpers\n",
    "- DÃ©duplication SHA-256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =====================================================\n# SOURCES 2, 3, 4, 5 : Ã€ IMPLÃ‰MENTER AVEC VRAIES SOURCES\n# =====================================================\n#\n# Pour respecter l'architecture pipeline du notebook datasens_E1_v2.ipynb,\n# les sources 2-5 doivent Ãªtre implÃ©mentÃ©es avec :\n# 1. Collecte rÃ©elle depuis API/BDD/Scraping/GDELT\n# 2. Upload MinIO pour traÃ§abilitÃ© DataLake\n# 3. Insertion PostgreSQL avec fonctions helpers (create_flux, insert_documents)\n# 4. Logging complet via logger.info/error\n#\n# Voir notebook datasens_E1_v2.ipynb pour implÃ©mentations complÃ¨tes :\n# - Source 2 : Kaggle DB (SQLite â†’ Postgres via Pandas)\n# - Source 3 : OpenWeatherMap API (voir Cell 20 du notebook existant)\n# - Source 4 : Web Scraping MonAvisCitoyen (voir Cell 26 du notebook existant)\n# - Source 5 : GDELT GKG Big Data (voir Cell 28 du notebook existant)\n\nlogger.info(\"\\nğŸ“‹ Pour sources 2-5 : Voir notebooks/datasens_E1_v2.ipynb\")\nlogger.info(\"   â†’ Exemples complets avec vraies API keys et collectes rÃ©elles\")\n\n# =====================================================\n# MANIFEST JSON (TraÃ§abilitÃ© finale)\n# =====================================================\nlogger.info(\"ğŸ“‹ CrÃ©ation du manifest JSON\")\nlogger.info(\"=\" * 80)\n\n# Compter les donnÃ©es collectÃ©es\nwith engine.connect() as conn:\n    counts = {\n        \"documents\": conn.execute(text(\"SELECT COUNT(*) FROM document\")).scalar(),\n        \"flux\": conn.execute(text(\"SELECT COUNT(*) FROM flux\")).scalar(),\n        \"sources\": conn.execute(text(\"SELECT COUNT(*) FROM source\")).scalar(),\n        \"meteo\": conn.execute(text(\"SELECT COUNT(*) FROM meteo\")).scalar(),\n        \"evenements\": conn.execute(text(\"SELECT COUNT(*) FROM evenement\")).scalar(),\n    }\n\nmanifest = {\n    \"run_id\": ts(),\n    \"timestamp_utc\": datetime.now(UTC).isoformat(),\n    \"notebook_version\": \"03_ingest_sources.ipynb\",\n    \"sources_ingested\": [\n        \"Kaggle CSV (fichier plat - 50% PG + 50% MinIO)\",\n        \"Kaggle DB (base de donnÃ©es - Ã  implÃ©menter)\",\n        \"OpenWeatherMap (API - Ã  implÃ©menter)\",\n        \"MonAvisCitoyen (scraping - Ã  implÃ©menter)\",\n        \"GDELT GKG (big data - Ã  implÃ©menter)\"\n    ],\n    \"counts\": counts,\n    \"postgres_db\": PG_DB,\n    \"minio_bucket\": MINIO_BUCKET,\n    \"raw_data_location\": str(RAW_DIR),\n    \"log_file\": str(log_file)\n}\n\n# Sauvegarder manifest local + MinIO\nmanifest_path = MANIFESTS_DIR / f\"manifest_{manifest['run_id']}.json\"\nmanifest_path.parent.mkdir(parents=True, exist_ok=True)\n\nwith manifest_path.open(\"w\", encoding=\"utf-8\") as f:\n    json.dump(manifest, f, indent=2, ensure_ascii=False)\n\ntry:\n    manifest_minio_uri = minio_upload(manifest_path, f\"manifests/{manifest_path.name}\")\n    logger.info(f\"âœ… Manifest crÃ©Ã© : {manifest_path.name}\")\n    logger.info(f\"â˜ï¸ Manifest MinIO : {manifest_minio_uri}\")\nexcept Exception as e:\n    log_error(\"MinIO\", e, \"Upload manifest\")\n    manifest_minio_uri = f\"local://{manifest_path}\"\n\nlogger.info(\"\\nğŸ“Š RÃ©sumÃ© ingestion :\")\nfor key, value in counts.items():\n    logger.info(f\"   â€¢ {key}: {value}\")\n\nlogger.info(\"\\nâœ… Ingestion terminÃ©e ! (Source 1/5 complÃ¨te, sources 2-5 Ã  documenter)\")\nlogger.info(\"   â¡ï¸ Passez au notebook 04_crud_tests.ipynb\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
