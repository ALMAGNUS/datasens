{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📥 DataSens E1 — Notebook 3 : Ingestion des 5 Sources\n",
    "\n",
    "**🎯 Objectif** : Ingérer réellement les 5 types de sources avec traçabilité complète\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 Plan d'ingestion\n",
    "\n",
    "1. **Fichier plat CSV** : Kaggle (50% → Postgres, 50% → raw)\n",
    "2. **Base de données** : Kaggle SQLite → Postgres\n",
    "3. **API** : OpenWeatherMap → meteo + flux\n",
    "4. **Web Scraping** : MonAvisCitoyen (dry-run) → document\n",
    "5. **Big Data** : GDELT GKG → evenement + document_evenement\n",
    "\n",
    "**Traçabilité** : Manifest JSON par run avec chemins, compteurs, horodatages\n",
    "\n",
    "---\n",
    "\n",
    "## 🔒 RGPD & Gouvernance\n",
    "\n",
    "⚠️ **Rappel** : Pas de données personnelles directes (hash SHA-256), respect robots.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration et imports (architecture pipeline complète)\n",
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "import traceback\n",
    "from datetime import UTC, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from minio import Minio\n",
    "from sqlalchemy import create_engine, text\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == \"notebooks\" else NOTEBOOK_DIR\n",
    "load_dotenv(PROJECT_ROOT / \".env\")\n",
    "\n",
    "PG_HOST = os.getenv(\"POSTGRES_HOST\", \"localhost\")\n",
    "PG_PORT = int(os.getenv(\"POSTGRES_PORT\", \"5432\"))\n",
    "PG_DB = os.getenv(\"POSTGRES_DB\", \"datasens\")\n",
    "PG_USER = os.getenv(\"POSTGRES_USER\", \"ds_user\")\n",
    "PG_PASS = os.getenv(\"POSTGRES_PASS\", \"ds_pass\")\n",
    "\n",
    "PG_URL = f\"postgresql+psycopg2://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{PG_DB}\"\n",
    "engine = create_engine(PG_URL, future=True)\n",
    "\n",
    "# Configuration MinIO (DataLake)\n",
    "MINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT\", \"http://localhost:9000\")\n",
    "MINIO_ACCESS_KEY = os.getenv(\"MINIO_ACCESS_KEY\", \"miniouser\")\n",
    "MINIO_SECRET_KEY = os.getenv(\"MINIO_SECRET_KEY\", \"miniosecret\")\n",
    "MINIO_BUCKET = os.getenv(\"MINIO_BUCKET\", \"datasens-raw\")\n",
    "\n",
    "RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "MANIFESTS_DIR = RAW_DIR / \"manifests\"\n",
    "LOGS_DIR = PROJECT_ROOT / \"logs\"\n",
    "\n",
    "# Créer dossiers\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MANIFESTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =====================================================\n",
    "# SYSTÈME DE LOGGING (comme datasens_E1_v2.ipynb)\n",
    "# =====================================================\n",
    "log_timestamp = datetime.now(UTC).strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_file = LOGS_DIR / f\"collecte_{log_timestamp}.log\"\n",
    "error_file = LOGS_DIR / f\"errors_{log_timestamp}.log\"\n",
    "\n",
    "logger = logging.getLogger(\"DataSens\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "file_formatter = logging.Formatter(\n",
    "    \"%(asctime)s | %(levelname)-8s | %(name)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "console_formatter = logging.Formatter(\n",
    "    \"[%(asctime)s] %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\"\n",
    ")\n",
    "\n",
    "file_handler = logging.FileHandler(log_file, encoding=\"utf-8\")\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(file_formatter)\n",
    "\n",
    "error_handler = logging.FileHandler(error_file, encoding=\"utf-8\")\n",
    "error_handler.setLevel(logging.ERROR)\n",
    "error_handler.setFormatter(file_formatter)\n",
    "\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "console_handler.setFormatter(console_formatter)\n",
    "\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(error_handler)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "def log_error(source: str, error: Exception, context: str = \"\"):\n",
    "    \"\"\"Log une erreur avec traceback complet\"\"\"\n",
    "    error_msg = f\"[{source}] {context}: {error!s}\"\n",
    "    logger.error(error_msg)\n",
    "    logger.error(f\"Traceback:\\n{traceback.format_exc()}\")\n",
    "\n",
    "logger.info(\"🚀 Système de logging initialisé\")\n",
    "logger.info(f\"📁 Logs: {log_file}\")\n",
    "logger.info(f\"❌ Erreurs: {error_file}\")\n",
    "\n",
    "# =====================================================\n",
    "# MINIO CLIENT (DataLake)\n",
    "# =====================================================\n",
    "try:\n",
    "    minio_client = Minio(\n",
    "        MINIO_ENDPOINT.replace(\"http://\", \"\").replace(\"https://\", \"\"),\n",
    "        access_key=MINIO_ACCESS_KEY,\n",
    "        secret_key=MINIO_SECRET_KEY,\n",
    "        secure=MINIO_ENDPOINT.startswith(\"https\")\n",
    "    )\n",
    "\n",
    "    def ensure_bucket(bucket: str = MINIO_BUCKET):\n",
    "        if not minio_client.bucket_exists(bucket):\n",
    "            minio_client.make_bucket(bucket)\n",
    "\n",
    "    def minio_upload(local_path: Path, dest_key: str) -> str:\n",
    "        \"\"\"Upload fichier vers MinIO DataLake\"\"\"\n",
    "        ensure_bucket(MINIO_BUCKET)\n",
    "        minio_client.fput_object(MINIO_BUCKET, dest_key, str(local_path))\n",
    "        return f\"s3://{MINIO_BUCKET}/{dest_key}\"\n",
    "\n",
    "    ensure_bucket()\n",
    "    logger.info(f\"✅ MinIO OK → bucket: {MINIO_BUCKET}\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"⚠️ MinIO non disponible: {e} - Mode local uniquement\")\n",
    "    minio_client = None\n",
    "    def minio_upload(local_path: Path, dest_key: str) -> str:\n",
    "        return f\"local://{local_path}\"\n",
    "\n",
    "# =====================================================\n",
    "# FONCTIONS UTILITAIRES\n",
    "# =====================================================\n",
    "def ts() -> str:\n",
    "    \"\"\"Timestamp UTC ISO compact\"\"\"\n",
    "    return datetime.now(UTC).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "\n",
    "def sha256(s: str) -> str:\n",
    "    \"\"\"Hash SHA-256 pour déduplication\"\"\"\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def get_source_id(conn, nom: str) -> int:\n",
    "    \"\"\"Récupère l'id_source depuis le nom\"\"\"\n",
    "    logger.info(f\"[get_source_id] Recherche source: {nom}\")\n",
    "    result = conn.execute(text(\"SELECT id_source FROM source WHERE nom = :nom\"), {\"nom\": nom}).fetchone()\n",
    "    if result:\n",
    "        logger.info(f\"   → id_source trouvé: {result[0]}\")\n",
    "        return result[0]\n",
    "    logger.warning(f\"   → Source non trouvée: {nom}\")\n",
    "    return None\n",
    "\n",
    "def create_flux(conn, id_source: int, format_type: str = \"csv\", manifest_uri: str = None) -> int:\n",
    "    \"\"\"Crée un flux et retourne id_flux\"\"\"\n",
    "    logger.info(f\"[create_flux] Création flux pour id_source={id_source}, format={format_type}\")\n",
    "    result = conn.execute(text(\"\"\"\n",
    "        INSERT INTO flux (id_source, format, manifest_uri)\n",
    "        VALUES (:id_source, :format, :manifest_uri)\n",
    "        RETURNING id_flux\n",
    "    \"\"\"), {\"id_source\": id_source, \"format\": format_type, \"manifest_uri\": manifest_uri})\n",
    "    id_flux = result.scalar()\n",
    "    logger.info(f\"   → id_flux créé: {id_flux}\")\n",
    "    return id_flux\n",
    "\n",
    "def ensure_territoire(conn, ville: str, code_insee: str = None, lat: float = None, lon: float = None) -> int:\n",
    "    \"\"\"Crée ou récupère un territoire\"\"\"\n",
    "    logger.info(f\"[ensure_territoire] Vérification territoire: ville={ville}\")\n",
    "    result = conn.execute(text(\"SELECT id_territoire FROM territoire WHERE ville = :ville\"), {\"ville\": ville}).fetchone()\n",
    "    if result:\n",
    "        logger.info(f\"   → id_territoire existant: {result[0]}\")\n",
    "        return result[0]\n",
    "    result = conn.execute(text(\"\"\"\n",
    "        INSERT INTO territoire (ville, code_insee, lat, lon)\n",
    "        VALUES (:ville, :code_insee, :lat, :lon)\n",
    "        RETURNING id_territoire\n",
    "    \"\"\"), {\"ville\": ville, \"code_insee\": code_insee, \"lat\": lat, \"lon\": lon})\n",
    "    id_territoire = result.scalar()\n",
    "    logger.info(f\"   → id_territoire créé: {id_territoire}\")\n",
    "    return id_territoire\n",
    "\n",
    "def insert_documents(conn, docs: list) -> int:\n",
    "    \"\"\"Insertion batch de documents avec gestion doublons\"\"\"\n",
    "    logger.info(f\"[insert_documents] Insertion de {len(docs)} documents...\")\n",
    "    inserted = 0\n",
    "    for doc in docs:\n",
    "        try:\n",
    "            result = conn.execute(text(\"\"\"\n",
    "                INSERT INTO document (id_flux, id_territoire, titre, texte, langue, date_publication, hash_fingerprint)\n",
    "                VALUES (:id_flux, :id_territoire, :titre, :texte, :langue, :date_publication, :hash_fingerprint)\n",
    "                ON CONFLICT (hash_fingerprint) DO NOTHING\n",
    "                RETURNING id_doc\n",
    "            \"\"\"), doc)\n",
    "            id_doc = result.scalar()\n",
    "            if id_doc:\n",
    "                logger.info(f\"   → Document inséré: id_doc={id_doc}, titre={doc.get('titre', '')[:40]}\")\n",
    "                inserted += 1\n",
    "        except Exception as e:\n",
    "            log_error(\"insert_documents\", e, \"Erreur insertion document\")\n",
    "    logger.info(f\"   → Total insérés: {inserted}/{len(docs)}\")\n",
    "    return inserted\n",
    "\n",
    "print(\"✅ Configuration pipeline chargée\")\n",
    "print(f\"   📍 PostgreSQL : {PG_HOST}:{PG_PORT}/{PG_DB}\")\n",
    "print(f\"   ☁️ MinIO : {MINIO_BUCKET if minio_client else 'Mode local'}\")\n",
    "print(f\"   📂 Raw data : {RAW_DIR}\")\n",
    "print(f\"   📄 Logs : {LOGS_DIR}\")\n",
    "print(\"\\n✅ Pipeline DataLake + PostgreSQL prêt !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📄 Source 1/5 : Fichier plat CSV (Kaggle)\n",
    "\n",
    "**Architecture hybride (comme datasens_E1_v2.ipynb)** :\n",
    "- **50% → PostgreSQL** : Données structurées pour requêtes SQL\n",
    "- **50% → MinIO DataLake** : Données brutes pour analyses Big Data futures\n",
    "\n",
    "**Process** :\n",
    "1. Chargement CSV depuis `data/raw/kaggle/`\n",
    "2. Calcul SHA256 fingerprint pour déduplication\n",
    "3. Split aléatoire 50/50\n",
    "4. Upload 50% vers MinIO (DataLake)\n",
    "5. Insertion 50% dans PostgreSQL avec traçabilité (id_flux)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"📄 SOURCE 1/5 : Fichier plat CSV (Kaggle)\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "# Rechercher fichier Kaggle existant ou créer échantillon\n",
    "kaggle_csv_paths = [\n",
    "    RAW_DIR / \"kaggle\" / \"kaggle_sample.csv\",\n",
    "    PROJECT_ROOT / \"data\" / \"raw\" / \"kaggle\" / \"*.csv\",\n",
    "    Path.cwd() / \"data\" / \"raw\" / \"kaggle\" / \"*.csv\"\n",
    "]\n",
    "\n",
    "kaggle_csv_path = None\n",
    "for path in kaggle_csv_paths:\n",
    "    if path.exists():\n",
    "        kaggle_csv_path = path\n",
    "        break\n",
    "\n",
    "if not kaggle_csv_path or not kaggle_csv_path.exists():\n",
    "    logger.warning(\"⚠️ Fichier Kaggle non trouvé — Création échantillon pour démo\")\n",
    "    sample_data = pd.DataFrame({\n",
    "        \"text\": [\n",
    "            \"Great product, very satisfied!\",\n",
    "            \"Service terrible, avoid at all costs\",\n",
    "            \"Excellent quality, recommend\",\n",
    "            \"Bon produit, je recommande\",\n",
    "            \"Mauvais service, déçu\"\n",
    "        ],\n",
    "        \"langue\": [\"en\", \"en\", \"en\", \"fr\", \"fr\"],\n",
    "        \"date\": [datetime.now(UTC)] * 5\n",
    "    })\n",
    "    kaggle_csv_path = RAW_DIR / \"kaggle\" / \"kaggle_sample.csv\"\n",
    "    kaggle_csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    sample_data.to_csv(kaggle_csv_path, index=False)\n",
    "    logger.info(f\"   ✅ Échantillon créé : {kaggle_csv_path.name}\")\n",
    "\n",
    "# Charger le CSV\n",
    "df_kaggle = pd.read_csv(kaggle_csv_path)\n",
    "logger.info(f\"📊 {len(df_kaggle)} lignes chargées\")\n",
    "\n",
    "# Split 50/50 (architecture hybride : PostgreSQL + MinIO)\n",
    "df_kaggle[\"hash_fingerprint\"] = df_kaggle[\"text\"].apply(lambda x: sha256(str(x)))\n",
    "mid_point = len(df_kaggle) // 2\n",
    "df_pg = df_kaggle.iloc[:mid_point].copy()  # 50% → PostgreSQL\n",
    "df_raw = df_kaggle.iloc[mid_point:].copy()  # 50% → MinIO DataLake\n",
    "\n",
    "logger.info(f\"   • 50% PostgreSQL : {len(df_pg)} lignes\")\n",
    "logger.info(f\"   • 50% MinIO DataLake : {len(df_raw)} lignes\")\n",
    "\n",
    "# Sauvegarder 50% en raw local + upload MinIO\n",
    "raw_output = RAW_DIR / \"kaggle\" / f\"kaggle_raw_{ts()}.csv\"\n",
    "df_raw.to_csv(raw_output, index=False)\n",
    "logger.info(f\"   ✅ Sauvegardé local : {raw_output.name}\")\n",
    "\n",
    "# Upload MinIO (50% bruts vers DataLake)\n",
    "try:\n",
    "    minio_uri = minio_upload(raw_output, f\"kaggle/{raw_output.name}\")\n",
    "    logger.info(f\"   ☁️ Upload MinIO : {minio_uri}\")\n",
    "except Exception as e:\n",
    "    log_error(\"MinIO\", e, \"Upload fichier Kaggle\")\n",
    "    minio_uri = f\"local://{raw_output}\"\n",
    "\n",
    "# Insérer 50% dans PostgreSQL\n",
    "with engine.begin() as conn:\n",
    "    id_source = get_source_id(conn, \"Kaggle CSV\")\n",
    "    if not id_source:\n",
    "        id_type = conn.execute(text(\"SELECT id_type_donnee FROM type_donnee WHERE libelle = 'Fichier plat'\")).scalar()\n",
    "        conn.execute(text(\"\"\"\n",
    "            INSERT INTO source (id_type_donnee, nom, url, fiabilite)\n",
    "            VALUES (:id_type, 'Kaggle CSV', 'https://www.kaggle.com', 0.8)\n",
    "        \"\"\"), {\"id_type\": id_type})\n",
    "        id_source = conn.execute(text(\"SELECT id_source FROM source WHERE nom = 'Kaggle CSV'\")).scalar()\n",
    "\n",
    "    id_flux = create_flux(conn, id_source, \"csv\", minio_uri)\n",
    "\n",
    "    # Préparer documents pour insertion batch\n",
    "    docs = []\n",
    "    for _, row in df_pg.iterrows():\n",
    "        docs.append({\n",
    "            \"id_flux\": id_flux,\n",
    "            \"id_territoire\": None,\n",
    "            \"titre\": \"\",\n",
    "            \"texte\": str(row[\"text\"]),\n",
    "            \"langue\": row.get(\"langue\", \"en\"),\n",
    "            \"date_publication\": row.get(\"date\", datetime.now(UTC)),\n",
    "            \"hash_fingerprint\": row[\"hash_fingerprint\"]\n",
    "        })\n",
    "\n",
    "    inserted = insert_documents(conn, docs)\n",
    "\n",
    "logger.info(f\"\\n✅ Source 1/5 terminée : {inserted} docs PostgreSQL + {len(df_raw)} docs MinIO\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Architecture Pipeline (Référence datasens_E1_v2.ipynb)\n",
    "\n",
    "**Ce notebook suit l'architecture du pipeline existant** :\n",
    "\n",
    "✅ **Logging structuré** : `logs/collecte_*.log` + `logs/errors_*.log`  \n",
    "✅ **MinIO DataLake** : Upload automatique fichiers bruts → `s3://datasens-raw/`  \n",
    "✅ **PostgreSQL** : Insertion structurée avec traçabilité (flux, manifests)  \n",
    "✅ **Fonctions helpers** : `create_flux()`, `insert_documents()`, `ensure_territoire()`, `minio_upload()`  \n",
    "✅ **Déduplication** : Hash SHA-256 pour éviter doublons  \n",
    "✅ **RGPD** : Pas de données personnelles directes  \n",
    "\n",
    "**Sources 2-5** : Implémentées ci-dessous avec vraies sources (code extrait de `datasens_E1_v2.ipynb`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌦️ Source 2/5 : API OpenWeatherMap\n",
    "\n",
    "Collecte de données météo en temps réel via l'API OpenWeatherMap.\n",
    "\n",
    "**Villes collectées** : Paris, Lyon, Marseille, Lille\n",
    "\n",
    "**Données récupérées** :\n",
    "- Température (°C), Humidité (%), Pression (hPa)\n",
    "- Description météo (clair, nuageux, pluie...)\n",
    "- Vitesse du vent (m/s)\n",
    "- Timestamp de mesure\n",
    "\n",
    "**Stockage** :\n",
    "- **PostgreSQL** : Table `meteo` avec géolocalisation (id_territoire FK)\n",
    "- **MinIO** : CSV brut pour historisation complète\n",
    "\n",
    "**RGPD** : Aucune donnée personnelle, données publiques uniquement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"🌦️ SOURCE 2/5 : API OpenWeatherMap\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "# Variables d'environnement\n",
    "OWM_API_KEY = os.getenv(\"OWM_API_KEY\")\n",
    "if not OWM_API_KEY:\n",
    "    logger.warning(\"⚠️ OWM_API_KEY manquante dans .env - Source 2 ignorée\")\n",
    "else:\n",
    "    OWM_CITIES = [\"Paris,FR\", \"Lyon,FR\", \"Marseille,FR\", \"Lille,FR\"]\n",
    "\n",
    "    rows = []\n",
    "    for c in tqdm(OWM_CITIES, desc=\"OWM\"):\n",
    "        try:\n",
    "            r = requests.get(\n",
    "                \"https://api.openweathermap.org/data/2.5/weather\",\n",
    "                params={\"q\": c, \"appid\": OWM_API_KEY, \"units\": \"metric\", \"lang\": \"fr\"},\n",
    "                timeout=10\n",
    "            )\n",
    "            if r.status_code == 200:\n",
    "                j = r.json()\n",
    "                rows.append({\n",
    "                    \"ville\": j[\"name\"],\n",
    "                    \"lat\": j[\"coord\"][\"lat\"],\n",
    "                    \"lon\": j[\"coord\"][\"lon\"],\n",
    "                    \"date_obs\": pd.to_datetime(j[\"dt\"], unit=\"s\"),\n",
    "                    \"temperature\": j[\"main\"][\"temp\"],\n",
    "                    \"humidite\": j[\"main\"][\"humidity\"],\n",
    "                    \"vent_kmh\": (j.get(\"wind\", {}).get(\"speed\") or 0) * 3.6,\n",
    "                    \"pression\": j.get(\"main\", {}).get(\"pressure\"),\n",
    "                    \"meteo_type\": j[\"weather\"][0][\"main\"] if j.get(\"weather\") else None\n",
    "                })\n",
    "        except Exception as e:\n",
    "            log_error(\"OpenWeatherMap\", e, f\"Collecte météo {c}\")\n",
    "\n",
    "        time.sleep(1)  # Respect rate limit\n",
    "\n",
    "    if len(rows) > 0:\n",
    "        dfm = pd.DataFrame(rows)\n",
    "        local = RAW_DIR / \"api\" / \"owm\" / f\"owm_{ts()}.csv\"\n",
    "        local.parent.mkdir(parents=True, exist_ok=True)\n",
    "        dfm.to_csv(local, index=False)\n",
    "\n",
    "        try:\n",
    "            minio_uri = minio_upload(local, f\"api/owm/{local.name}\")\n",
    "            logger.info(f\"   ☁️ Upload MinIO : {minio_uri}\")\n",
    "        except Exception as e:\n",
    "            log_error(\"MinIO\", e, \"Upload fichier OWM\")\n",
    "            minio_uri = f\"local://{local}\"\n",
    "\n",
    "        # Insertion PostgreSQL\n",
    "        with engine.begin() as conn:\n",
    "            id_source = get_source_id(conn, \"OpenWeatherMap\")\n",
    "            if not id_source:\n",
    "                id_type = conn.execute(text(\"SELECT id_type_donnee FROM type_donnee WHERE libelle = 'API'\")).scalar()\n",
    "                if id_type:\n",
    "                    conn.execute(text(\"\"\"\n",
    "                        INSERT INTO source (id_type_donnee, nom, url, fiabilite)\n",
    "                        VALUES (:id_type, 'OpenWeatherMap', 'https://openweathermap.org/api', 0.9)\n",
    "                    \"\"\"), {\"id_type\": id_type})\n",
    "                    id_source = conn.execute(text(\"SELECT id_source FROM source WHERE nom = 'OpenWeatherMap'\")).scalar()\n",
    "                else:\n",
    "                    logger.warning(\"   ⚠️ Type 'API' non trouvé dans type_donnee\")\n",
    "\n",
    "            if id_source:\n",
    "                id_flux = create_flux(conn, id_source, \"json\", minio_uri)\n",
    "\n",
    "                # Insérer territoires et météo\n",
    "                for _, r in dfm.iterrows():\n",
    "                    tid = ensure_territoire(conn, ville=r[\"ville\"], lat=r[\"lat\"], lon=r[\"lon\"])\n",
    "                    try:\n",
    "                        conn.execute(text(\"\"\"\n",
    "                            INSERT INTO meteo(id_territoire, date_obs, temperature, humidite, vent_kmh, pression, meteo_type)\n",
    "                            VALUES(:t, :d, :T, :H, :V, :P, :MT)\n",
    "                        \"\"\"), {\n",
    "                            \"t\": tid, \"d\": r[\"date_obs\"], \"T\": r[\"temperature\"],\n",
    "                            \"H\": r[\"humidite\"], \"V\": r[\"vent_kmh\"], \"P\": r[\"pression\"], \"MT\": r[\"meteo_type\"]\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        log_error(\"meteo\", e, f\"Insertion relevé {r['ville']}\")\n",
    "\n",
    "                logger.info(f\"✅ Source 2/5 terminée : {len(dfm)} relevés météo insérés\")\n",
    "            else:\n",
    "                logger.warning(\"   ⚠️ Source OpenWeatherMap non créée - insertion météo ignorée\")\n",
    "    else:\n",
    "        logger.warning(\"⚠️ Aucun relevé météo collecté\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📰 Source 3/5 : Flux RSS Multi-Sources (Presse française)\n",
    "\n",
    "Collecte d'articles d'actualité via 3 flux RSS français complémentaires.\n",
    "\n",
    "**Sources** :\n",
    "- **Franceinfo** : flux principal actualités nationales\n",
    "- **20 Minutes** : actualités françaises grand public\n",
    "- **Le Monde** : presse de référence\n",
    "\n",
    "**Extraction** : titre, description, date publication, URL source\n",
    "\n",
    "**Stockage** : PostgreSQL + MinIO\n",
    "\n",
    "**Déduplication** : SHA256 sur (titre + description) pour éviter doublons inter-sources\n",
    "\n",
    "**Parser** : Utilisation de `feedparser` pour robustesse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"📰 SOURCE 3/5 : Flux RSS Multi-Sources (Presse française)\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    import feedparser\n",
    "except ImportError:\n",
    "    logger.error(\"❌ Module feedparser manquant - install: pip install feedparser\")\n",
    "    feedparser = None\n",
    "\n",
    "if feedparser:\n",
    "    RSS_SOURCES = {\n",
    "        \"Franceinfo\": \"https://www.francetvinfo.fr/titres.rss\",\n",
    "        \"20 Minutes\": \"https://www.20minutes.fr/feeds/rss-une.xml\",\n",
    "        \"Le Monde\": \"https://www.lemonde.fr/rss/une.xml\"\n",
    "    }\n",
    "\n",
    "    all_rss_items = []\n",
    "\n",
    "    for source_name, rss_url in RSS_SOURCES.items():\n",
    "        logger.info(f\"📡 Source : {source_name}\")\n",
    "        logger.info(f\"   URL : {rss_url}\")\n",
    "\n",
    "        try:\n",
    "            feed = feedparser.parse(rss_url)\n",
    "\n",
    "            if len(feed.entries) == 0:\n",
    "                logger.warning(\"   ⚠️ Aucun article trouvé\")\n",
    "                continue\n",
    "\n",
    "            source_items = []\n",
    "            for e in feed.entries[:100]:  # Max 100 par source\n",
    "                titre = e.get(\"title\", \"\").strip()\n",
    "                texte = (e.get(\"summary\", \"\") or e.get(\"description\", \"\") or \"\").strip()\n",
    "                dp = pd.to_datetime(e.get(\"published\", \"\"), errors=\"coerce\")\n",
    "                url = e.get(\"link\", \"\")\n",
    "\n",
    "                if titre and texte:\n",
    "                    source_items.append({\n",
    "                        \"titre\": titre,\n",
    "                        \"texte\": texte,\n",
    "                        \"date_publication\": dp if pd.notna(dp) else datetime.now(UTC),\n",
    "                        \"langue\": \"fr\",\n",
    "                        \"source_media\": source_name,\n",
    "                        \"url\": url\n",
    "                    })\n",
    "\n",
    "            all_rss_items.extend(source_items)\n",
    "            logger.info(f\"   ✅ {len(source_items)} articles collectés\")\n",
    "\n",
    "        except Exception as e:\n",
    "            log_error(f\"RSS_{source_name}\", e, \"Parsing flux RSS\")\n",
    "            logger.warning(f\"   ⚠️ Erreur : {str(e)[:80]}\")\n",
    "\n",
    "        time.sleep(1)  # Respect rate limit\n",
    "\n",
    "    # Consolidation DataFrame\n",
    "    if len(all_rss_items) > 0:\n",
    "        dfr = pd.DataFrame(all_rss_items)\n",
    "\n",
    "        # Déduplication inter-sources\n",
    "        dfr[\"hash_fingerprint\"] = dfr.apply(lambda row: sha256(row[\"titre\"] + \" \" + row[\"texte\"]), axis=1)\n",
    "        nb_avant = len(dfr)\n",
    "        dfr = dfr.drop_duplicates(subset=[\"hash_fingerprint\"])\n",
    "        nb_apres = len(dfr)\n",
    "\n",
    "        logger.info(f\"🧹 Déduplication : {nb_avant} → {nb_apres} articles uniques ({nb_avant - nb_apres} doublons supprimés)\")\n",
    "\n",
    "        # Distribution par source\n",
    "        logger.info(\"📊 Distribution par source :\")\n",
    "        for source in dfr[\"source_media\"].value_counts().items():\n",
    "            logger.info(f\"   {source[0]:15s} : {source[1]:3d} articles\")\n",
    "\n",
    "        # Sauvegarde locale + MinIO\n",
    "        local = RAW_DIR / \"rss\" / f\"rss_multi_sources_{ts()}.csv\"\n",
    "        local.parent.mkdir(parents=True, exist_ok=True)\n",
    "        dfr.to_csv(local, index=False)\n",
    "\n",
    "        try:\n",
    "            minio_uri = minio_upload(local, f\"rss/{local.name}\")\n",
    "            logger.info(f\"   ☁️ Upload MinIO : {minio_uri}\")\n",
    "        except Exception as e:\n",
    "            log_error(\"MinIO\", e, \"Upload fichier RSS\")\n",
    "            minio_uri = f\"local://{local}\"\n",
    "\n",
    "        # Insertion PostgreSQL\n",
    "        with engine.begin() as conn:\n",
    "            id_source = get_source_id(conn, \"Flux RSS Multi-Sources\")\n",
    "            if not id_source:\n",
    "                id_type = conn.execute(text(\"SELECT id_type_donnee FROM type_donnee WHERE libelle = 'API' OR libelle = 'Web Scraping'\")).scalar()\n",
    "                if id_type:\n",
    "                    conn.execute(text(\"\"\"\n",
    "                        INSERT INTO source (id_type_donnee, nom, url, fiabilite)\n",
    "                        VALUES (:id_type, 'Flux RSS Multi-Sources', 'https://www.francetvinfo.fr/titres.rss', 0.95)\n",
    "                    \"\"\"), {\"id_type\": id_type})\n",
    "                    id_source = conn.execute(text(\"SELECT id_source FROM source WHERE nom = 'Flux RSS Multi-Sources'\")).scalar()\n",
    "\n",
    "            if id_source:\n",
    "                id_flux = create_flux(conn, id_source, \"rss\", minio_uri)\n",
    "\n",
    "                # Préparer documents pour insertion batch\n",
    "                docs = []\n",
    "                for _, row in dfr.iterrows():\n",
    "                    docs.append({\n",
    "                        \"id_flux\": id_flux,\n",
    "                        \"id_territoire\": None,\n",
    "                        \"titre\": row[\"titre\"],\n",
    "                        \"texte\": row[\"texte\"],\n",
    "                        \"langue\": row[\"langue\"],\n",
    "                        \"date_publication\": row[\"date_publication\"],\n",
    "                        \"hash_fingerprint\": row[\"hash_fingerprint\"]\n",
    "                    })\n",
    "\n",
    "                inserted = insert_documents(conn, docs)\n",
    "                logger.info(f\"✅ Source 3/5 terminée : {inserted} articles RSS insérés\")\n",
    "            else:\n",
    "                logger.warning(\"   ⚠️ Source RSS non créée - insertion ignorée\")\n",
    "    else:\n",
    "        logger.warning(\"⚠️ Aucun article RSS collecté\")\n",
    "else:\n",
    "    logger.warning(\"⚠️ Module feedparser manquant - Source 3 ignorée\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌐 Source 4/5 : Web Scraping Multi-Sources (Dry-run MonAvisCitoyen)\n",
    "\n",
    "Collecte de données citoyennes depuis sources légales et éthiques (version simplifiée pour E1).\n",
    "\n",
    "**Sources implémentées (dry-run)** :\n",
    "- **Vie-publique.fr** (RSS) : Consultations citoyennes nationales\n",
    "- **data.gouv.fr** (API) : Open Data datasets CSV officiels\n",
    "\n",
    "**Éthique & Légalité** :\n",
    "- ✅ Open Data gouvernemental (.gouv.fr)\n",
    "- ✅ Respect robots.txt\n",
    "- ✅ APIs officielles uniquement\n",
    "- ✅ Aucun scraping de sites privés sans autorisation\n",
    "\n",
    "**Stockage** :\n",
    "- **PostgreSQL** : Documents structurés\n",
    "- **MinIO** : CSV bruts pour audit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"🌐 SOURCE 4/5 : Web Scraping Multi-Sources (Dry-run)\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "all_scraping_data = []\n",
    "\n",
    "# ============================================================\n",
    "# SOURCE 1 : VIE-PUBLIQUE.FR (RSS)\n",
    "# ============================================================\n",
    "logger.info(\"🏛️ Source 1/2 : Vie-publique.fr (RSS)\")\n",
    "\n",
    "try:\n",
    "    if feedparser:\n",
    "        feed_url = \"https://www.vie-publique.fr/rss\"\n",
    "        feed = feedparser.parse(feed_url)\n",
    "\n",
    "        for entry in feed.entries[:50]:\n",
    "            all_scraping_data.append({\n",
    "                \"titre\": entry.get(\"title\", \"\"),\n",
    "                \"texte\": entry.get(\"summary\", entry.get(\"description\", \"\")),\n",
    "                \"source_site\": \"vie-publique.fr\",\n",
    "                \"url\": entry.get(\"link\", \"\"),\n",
    "                \"date_publication\": datetime(*entry.published_parsed[:6], tzinfo=UTC) if hasattr(entry, \"published_parsed\") else datetime.now(UTC),\n",
    "                \"langue\": \"fr\"\n",
    "            })\n",
    "\n",
    "        logger.info(f\"✅ Vie-publique.fr: {len([d for d in all_scraping_data if 'vie-publique' in d['source_site']])} articles collectés\")\n",
    "    else:\n",
    "        logger.warning(\"   ⚠️ Module feedparser manquant\")\n",
    "except Exception as e:\n",
    "    log_error(\"ViePublique\", e, \"Parsing RSS feed\")\n",
    "    logger.warning(f\"   ⚠️ Vie-publique.fr: {str(e)[:100]} (skip)\")\n",
    "\n",
    "# ============================================================\n",
    "# SOURCE 2 : DATA.GOUV.FR (API officielle)\n",
    "# ============================================================\n",
    "logger.info(\"📊 Source 2/2 : data.gouv.fr (API officielle)\")\n",
    "\n",
    "try:\n",
    "    url = \"https://www.data.gouv.fr/api/1/datasets/\"\n",
    "    params = {\"q\": \"france\", \"page_size\": 50}\n",
    "    response = requests.get(url, params=params, timeout=10)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    data = response.json()\n",
    "    for dataset in data.get(\"data\", []):\n",
    "        all_scraping_data.append({\n",
    "            \"titre\": dataset.get(\"title\", \"\"),\n",
    "            \"texte\": dataset.get(\"description\", dataset.get(\"title\", \"\")),\n",
    "            \"source_site\": \"data.gouv.fr\",\n",
    "            \"url\": f\"https://www.data.gouv.fr/fr/datasets/{dataset.get('slug', '')}\",\n",
    "            \"date_publication\": datetime.fromisoformat(dataset.get(\"created_at\", datetime.now(UTC).isoformat()).replace(\"Z\", \"+00:00\")),\n",
    "            \"langue\": \"fr\"\n",
    "        })\n",
    "\n",
    "    logger.info(f\"✅ data.gouv.fr: {len([d for d in all_scraping_data if 'data.gouv' in d['source_site']])} datasets collectés\")\n",
    "\n",
    "except Exception as e:\n",
    "    log_error(\"DataGouv\", e, \"Collecte datasets Open Data\")\n",
    "    logger.warning(f\"   ⚠️ data.gouv.fr: {str(e)[:100]} (skip)\")\n",
    "\n",
    "# ============================================================\n",
    "# CONSOLIDATION ET STORAGE\n",
    "# ============================================================\n",
    "if len(all_scraping_data) > 0:\n",
    "    df_scraping = pd.DataFrame(all_scraping_data)\n",
    "\n",
    "    # Nettoyage\n",
    "    df_scraping = df_scraping[df_scraping[\"texte\"].str.len() > 20].copy()\n",
    "    df_scraping[\"hash_fingerprint\"] = df_scraping[\"texte\"].apply(lambda t: sha256(t[:500]))\n",
    "    df_scraping = df_scraping.drop_duplicates(subset=[\"hash_fingerprint\"])\n",
    "\n",
    "    logger.info(f\"📈 Total collecté: {len(df_scraping)} documents citoyens\")\n",
    "    logger.info(f\"   • Vie Publique: {len(df_scraping[df_scraping['source_site'].str.contains('vie-publique', na=False)])}\")\n",
    "    logger.info(f\"   • Data.gouv: {len(df_scraping[df_scraping['source_site'].str.contains('data.gouv', na=False)])}\")\n",
    "\n",
    "    # Storage MinIO\n",
    "    scraping_dir = RAW_DIR / \"scraping\" / \"multi\"\n",
    "    scraping_dir.mkdir(parents=True, exist_ok=True)\n",
    "    local = scraping_dir / f\"scraping_multi_{ts()}.csv\"\n",
    "    df_scraping.to_csv(local, index=False)\n",
    "\n",
    "    try:\n",
    "        minio_uri = minio_upload(local, f\"scraping/multi/{local.name}\")\n",
    "        logger.info(f\"   ☁️ Upload MinIO : {minio_uri}\")\n",
    "    except Exception as e:\n",
    "        log_error(\"MinIO\", e, \"Upload fichier scraping\")\n",
    "        minio_uri = f\"local://{local}\"\n",
    "\n",
    "    # Storage PostgreSQL\n",
    "    with engine.begin() as conn:\n",
    "        id_source = get_source_id(conn, \"Web Scraping Multi-Sources\")\n",
    "        if not id_source:\n",
    "            id_type = conn.execute(text(\"SELECT id_type_donnee FROM type_donnee WHERE libelle = 'Web Scraping'\")).scalar()\n",
    "            if id_type:\n",
    "                conn.execute(text(\"\"\"\n",
    "                    INSERT INTO source (id_type_donnee, nom, url, fiabilite)\n",
    "                    VALUES (:id_type, 'Web Scraping Multi-Sources', 'https://www.data.gouv.fr', 0.85)\n",
    "                \"\"\"), {\"id_type\": id_type})\n",
    "                id_source = conn.execute(text(\"SELECT id_source FROM source WHERE nom = 'Web Scraping Multi-Sources'\")).scalar()\n",
    "\n",
    "        if id_source:\n",
    "            id_flux = create_flux(conn, id_source, \"html\", minio_uri)\n",
    "\n",
    "            docs = []\n",
    "            for _, row in df_scraping.iterrows():\n",
    "                docs.append({\n",
    "                    \"id_flux\": id_flux,\n",
    "                    \"id_territoire\": None,\n",
    "                    \"titre\": row[\"titre\"],\n",
    "                    \"texte\": row[\"texte\"],\n",
    "                    \"langue\": row[\"langue\"],\n",
    "                    \"date_publication\": row[\"date_publication\"],\n",
    "                    \"hash_fingerprint\": row[\"hash_fingerprint\"]\n",
    "                })\n",
    "\n",
    "            inserted = insert_documents(conn, docs)\n",
    "            logger.info(f\"✅ Source 4/5 terminée : {inserted} documents scraping insérés\")\n",
    "        else:\n",
    "            logger.warning(\"   ⚠️ Source scraping non créée - insertion ignorée\")\n",
    "else:\n",
    "    logger.warning(\"⚠️ Aucune donnée collectée depuis les sources web scraping\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌍 Source 5/5 : GDELT GKG France (Big Data)\n",
    "\n",
    "Téléchargement et analyse de données Big Data depuis GDELT Project (Global Database of Events, Language, and Tone) avec **focus France**.\n",
    "\n",
    "**Source** : http://data.gdeltproject.org/gdeltv2/\n",
    "\n",
    "**Format** : GKG 2.0 (Global Knowledge Graph) - Fichiers CSV.zip (~300 MB/15min)\n",
    "\n",
    "**Contenu Big Data** :\n",
    "- Événements mondiaux géolocalisés\n",
    "- **Tonalité émotionnelle** (V2Tone : -100 négatif → +100 positif)\n",
    "- **Thèmes extraits** (V2Themes : PROTEST, HEALTH, ECONOMY, TERROR...)\n",
    "- **Entités nommées** (V2Persons, V2Organizations)\n",
    "- **Géolocalisation** (V2Locations avec codes pays)\n",
    "\n",
    "**Filtrage France** :\n",
    "- Sélection événements avec localisation France (code pays FR)\n",
    "- Extraction tonalité moyenne France\n",
    "- Top thèmes français\n",
    "\n",
    "**Stratégie Big Data** :\n",
    "- Téléchargement fichier dernières 15min (~6-300 MB brut)\n",
    "- Parsing colonnes V2* nommées (27 colonnes GKG)\n",
    "- Filtrage géographique France → échantillon\n",
    "- Storage MinIO (fichier brut complet)\n",
    "- Insertion PostgreSQL (événements France)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"🌍 SOURCE 5/5 : GDELT GKG France (Big Data)\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "import io\n",
    "import zipfile\n",
    "\n",
    "# Colonnes GKG 2.0 (version complète)\n",
    "GKG_COLUMNS = [\n",
    "    \"GKGRECORDID\", \"V2.1DATE\", \"V2SourceCollectionIdentifier\", \"V2SourceCommonName\",\n",
    "    \"V2DocumentIdentifier\", \"V1Counts\", \"V2.1Counts\", \"V1Themes\", \"V2Themes\",\n",
    "    \"V1Locations\", \"V2Locations\", \"V1Persons\", \"V2Persons\", \"V1Organizations\",\n",
    "    \"V2Organizations\", \"V1.5Tone\", \"V2.1Tone\", \"V2.1Dates\", \"V2.1Amounts\",\n",
    "    \"V2.1TransInfo\", \"V2.1Extras\", \"V21SourceLanguage\", \"V21QuotationLanguage\",\n",
    "    \"V21Url\", \"V21Date2\", \"V21Xml\"\n",
    "]\n",
    "\n",
    "# Récupérer le fichier GKG le plus récent (dernières 15 minutes)\n",
    "try:\n",
    "    # URL du dernier update GDELT\n",
    "    update_url = \"http://data.gdeltproject.org/gdeltv2/lastupdate.txt\"\n",
    "    r = requests.get(update_url, timeout=15)\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        lines = r.text.strip().split(\"\\n\")\n",
    "        # Trouver ligne GKG (pas export ni mentions)\n",
    "        gkg_line = [line for line in lines if \".gkg.csv.zip\" in line and \"translation\" not in line]\n",
    "\n",
    "        if gkg_line:\n",
    "            # Format: size hash url\n",
    "            parts = gkg_line[0].split()\n",
    "            gkg_url = parts[2] if len(parts) >= 3 else parts[-1]\n",
    "            file_size_mb = int(parts[0]) / 1024 / 1024 if parts[0].isdigit() else 0\n",
    "\n",
    "            logger.info(f\"📥 Téléchargement GDELT GKG ({file_size_mb:.1f} MB)\")\n",
    "            logger.info(f\"   URL: {gkg_url}\")\n",
    "\n",
    "            # Télécharger\n",
    "            gkg_r = requests.get(gkg_url, timeout=120)\n",
    "\n",
    "            if gkg_r.status_code == 200:\n",
    "                # Sauvegarder ZIP\n",
    "                zip_filename = gkg_url.split(\"/\")[-1]\n",
    "                zip_path = RAW_DIR / \"gdelt\" / zip_filename\n",
    "                zip_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                with zip_path.open(\"wb\") as f:\n",
    "                    f.write(gkg_r.content)\n",
    "\n",
    "                logger.info(f\"   ✅ Téléchargé: {zip_path.name} ({len(gkg_r.content) / 1024 / 1024:.1f} MB)\")\n",
    "\n",
    "                # Upload MinIO (fichier brut complet)\n",
    "                try:\n",
    "                    minio_uri = minio_upload(zip_path, f\"gdelt/{zip_path.name}\")\n",
    "                    logger.info(f\"   ☁️ Upload MinIO : {minio_uri}\")\n",
    "                except Exception as e:\n",
    "                    log_error(\"MinIO\", e, \"Upload fichier GDELT\")\n",
    "                    minio_uri = f\"local://{zip_path}\"\n",
    "\n",
    "                # Extraction et parsing\n",
    "                with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "                    csv_filename = z.namelist()[0]\n",
    "                    logger.info(f\"\\n📊 Parsing: {csv_filename}\")\n",
    "\n",
    "                    with z.open(csv_filename) as f:\n",
    "                        # Lire avec pandas\n",
    "                        try:\n",
    "                            df_gkg = pd.read_csv(\n",
    "                                io.BytesIO(f.read()),\n",
    "                                sep=\"\\t\",\n",
    "                                header=None,\n",
    "                                names=GKG_COLUMNS,\n",
    "                                on_bad_lines=\"skip\",\n",
    "                                low_memory=False,\n",
    "                                nrows=5000  # Limiter pour démo (sinon trop long)\n",
    "                            )\n",
    "\n",
    "                            logger.info(f\"   📈 Total lignes chargées: {len(df_gkg):,}\")\n",
    "\n",
    "                            # 🇫🇷 FILTRAGE FRANCE\n",
    "                            logger.info(\"\\n🇫🇷 Filtrage événements France...\")\n",
    "                            df_france = df_gkg[\n",
    "                                df_gkg[\"V2Locations\"].fillna(\"\").str.contains(\"1#France#FR#\", na=False) |\n",
    "                                df_gkg[\"V2Locations\"].fillna(\"\").str.contains(\"#FR#\", na=False)\n",
    "                            ].copy()\n",
    "\n",
    "                            logger.info(f\"   ✅ Événements France: {len(df_france):,} ({len(df_france)/len(df_gkg)*100:.1f}%)\")\n",
    "\n",
    "                            if len(df_france) > 0:\n",
    "                                # Extraction tonalité émotionnelle\n",
    "                                def parse_tone(tone_str):\n",
    "                                    if pd.isna(tone_str) or tone_str == \"\":\n",
    "                                        return None\n",
    "                                    try:\n",
    "                                        parts = str(tone_str).split(\",\")\n",
    "                                        return float(parts[0]) if parts else None\n",
    "                                    except Exception:\n",
    "                                        return None\n",
    "\n",
    "                                df_france[\"tone_value\"] = df_france[\"V2.1Tone\"].apply(parse_tone)\n",
    "                                avg_tone = df_france[\"tone_value\"].mean()\n",
    "\n",
    "                                logger.info(f\"📊 Tonalité moyenne France: {avg_tone:.2f} (-100=très négatif, +100=très positif)\")\n",
    "\n",
    "                                # Insertion PostgreSQL (événements et documents)\n",
    "                                with engine.begin() as conn:\n",
    "                                    id_source = get_source_id(conn, \"GDELT GKG\")\n",
    "                                    if not id_source:\n",
    "                                        id_type = conn.execute(text(\"SELECT id_type_donnee FROM type_donnee WHERE libelle = 'Big Data'\")).scalar()\n",
    "                                        if id_type:\n",
    "                                            conn.execute(text(\"\"\"\n",
    "                                                INSERT INTO source (id_type_donnee, nom, url, fiabilite)\n",
    "                                                VALUES (:id_type, 'GDELT GKG', 'http://data.gdeltproject.org/gdeltv2/', 0.9)\n",
    "                                            \"\"\"), {\"id_type\": id_type})\n",
    "                                            id_source = conn.execute(text(\"SELECT id_source FROM source WHERE nom = 'GDELT GKG'\")).scalar()\n",
    "\n",
    "                                    if id_source:\n",
    "                                        id_flux = create_flux(conn, id_source, \"csv\", minio_uri)\n",
    "\n",
    "                                        # Insertion événements et documents\n",
    "                                        inserted_events = 0\n",
    "                                        inserted_docs = 0\n",
    "\n",
    "                                        for _, row in df_france.head(100).iterrows():  # Limiter à 100 pour démo\n",
    "                                            try:\n",
    "                                                # Créer thème si nécessaire\n",
    "                                                themes_str = str(row[\"V2Themes\"]) if pd.notna(row[\"V2Themes\"]) else \"\"\n",
    "                                                theme_libelle = themes_str.split(\";\")[0] if themes_str else \"GENERAL\"\n",
    "\n",
    "                                                theme_id = conn.execute(text(\"\"\"\n",
    "                                                    SELECT id_theme FROM theme WHERE libelle = :libelle\n",
    "                                                \"\"\"), {\"libelle\": theme_libelle}).fetchone()\n",
    "\n",
    "                                                if not theme_id:\n",
    "                                                    conn.execute(text(\"\"\"\n",
    "                                                        INSERT INTO theme (libelle, description)\n",
    "                                                        VALUES (:libelle, :desc)\n",
    "                                                    \"\"\"), {\"libelle\": theme_libelle, \"desc\": f\"Thème GDELT: {theme_libelle}\"})\n",
    "                                                    theme_id = conn.execute(text(\"\"\"\n",
    "                                                        SELECT id_theme FROM theme WHERE libelle = :libelle\n",
    "                                                    \"\"\"), {\"libelle\": theme_libelle}).fetchone()\n",
    "\n",
    "                                                theme_id_val = theme_id[0] if theme_id else None\n",
    "\n",
    "                                                # Créer événement\n",
    "                                                event_result = conn.execute(text(\"\"\"\n",
    "                                                    INSERT INTO evenement (id_theme, date_event, avg_tone, source_event)\n",
    "                                                    VALUES (:theme, :date_event, :tone, :source)\n",
    "                                                    RETURNING id_event\n",
    "                                                \"\"\"), {\n",
    "                                                    \"theme\": theme_id_val,\n",
    "                                                    \"date_event\": datetime.fromtimestamp(int(str(row[\"V2.1DATE\"])[:8]), tz=UTC) if len(str(row[\"V2.1DATE\"])) >= 8 else datetime.now(UTC),\n",
    "                                                    \"tone\": avg_tone,\n",
    "                                                    \"source\": \"GDELT\"\n",
    "                                                })\n",
    "                                                event_id = event_result.scalar()\n",
    "\n",
    "                                                # Créer document associé\n",
    "                                                doc_text = f\"{row.get('V2SourceCommonName', '')} - {themes_str[:200]}\"\n",
    "                                                doc_hash = sha256(doc_text)\n",
    "\n",
    "                                                doc_result = conn.execute(text(\"\"\"\n",
    "                                                    INSERT INTO document (id_flux, id_territoire, titre, texte, langue, date_publication, hash_fingerprint)\n",
    "                                                    VALUES (:id_flux, NULL, :titre, :texte, 'en', :date_pub, :hash)\n",
    "                                                    ON CONFLICT (hash_fingerprint) DO NOTHING\n",
    "                                                    RETURNING id_doc\n",
    "                                                \"\"\"), {\n",
    "                                                    \"id_flux\": id_flux,\n",
    "                                                    \"titre\": row.get(\"V2SourceCommonName\", \"GDELT Event\")[:200],\n",
    "                                                    \"texte\": doc_text,\n",
    "                                                    \"date_pub\": datetime.now(UTC),\n",
    "                                                    \"hash\": doc_hash\n",
    "                                                })\n",
    "                                                doc_id = doc_result.scalar()\n",
    "\n",
    "                                                if doc_id and event_id:\n",
    "                                                    # Lier document à événement\n",
    "                                                    conn.execute(text(\"\"\"\n",
    "                                                        INSERT INTO document_evenement (id_doc, id_event)\n",
    "                                                        VALUES (:doc_id, :event_id)\n",
    "                                                        ON CONFLICT DO NOTHING\n",
    "                                                    \"\"\"), {\"doc_id\": doc_id, \"event_id\": event_id})\n",
    "                                                    inserted_events += 1\n",
    "                                                    inserted_docs += 1\n",
    "\n",
    "                                            except Exception as e:\n",
    "                                                log_error(\"GDELT\", e, \"Insertion événement/document\")\n",
    "\n",
    "                                        logger.info(f\"✅ Source 5/5 terminée : {inserted_events} événements France insérés ({inserted_docs} docs)\")\n",
    "                                    else:\n",
    "                                        logger.warning(\"   ⚠️ Source GDELT non créée - insertion ignorée\")\n",
    "                            else:\n",
    "                                logger.warning(\"   ⚠️ Aucun événement France trouvé dans ce fichier\")\n",
    "\n",
    "                        except Exception as e:\n",
    "                            log_error(\"GDELT\", e, \"Parsing CSV\")\n",
    "                            logger.warning(f\"   ❌ Erreur parsing CSV: {str(e)[:100]}\")\n",
    "                            logger.info(\"   i Fichier brut sauvegardé sur MinIO\")\n",
    "\n",
    "            else:\n",
    "                logger.error(f\"   ❌ Erreur téléchargement GKG: {gkg_r.status_code}\")\n",
    "        else:\n",
    "            logger.warning(\"   ⚠️ Aucun fichier GKG trouvé dans lastupdate.txt\")\n",
    "    else:\n",
    "        logger.error(f\"   ❌ Erreur accès lastupdate.txt: {r.status_code}\")\n",
    "\n",
    "except Exception as e:\n",
    "    log_error(\"GDELT\", e, \"Collecte Big Data\")\n",
    "    logger.warning(f\"❌ Erreur GDELT: {str(e)[:200]}\")\n",
    "    logger.info(\"i GDELT peut être temporairement indisponible (service tiers)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Création du Manifest JSON\n",
    "\n",
    "Génération d'un manifest JSON pour traçabilité complète de toutes les ingestions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Baromètres DataSens - Sources Métier (E2/E3)\n",
    "\n",
    "Les 5 sources de base (E1) sont complètes. Pour enrichir le dataset avec des données métier spécialisées, voici **10 types de baromètres** à implémenter dans les phases E2/E3 :\n",
    "\n",
    "### 📋 Liste des Baromètres\n",
    "\n",
    "1. **🔹 Baromètre de confiance politique & sociale**\n",
    "   - **Source** : CEVIPOF – La confiance des Français dans la politique\n",
    "   - **Thématique** : Société, gouvernance, démocratie, institutions\n",
    "   - **Format** : CSV / PDF / API\n",
    "   - **Mapping E1** : API / Fichier plat\n",
    "\n",
    "2. **🔹 Baromètre des émotions et du moral des Français**\n",
    "   - **Source** : Kantar Public / Ipsos Mood of France\n",
    "   - **Thématique** : Joie, anxiété, colère, espoir (→ table EMOTION)\n",
    "   - **Format** : CSV / scraping\n",
    "   - **Mapping E1** : CSV / Web Scraping\n",
    "\n",
    "3. **🔹 Baromètre environnemental**\n",
    "   - **Source** : ADEME / IFOP pour la transition écologique\n",
    "   - **Thématique** : Écologie, énergie, climat, sobriété\n",
    "   - **Format** : Dataset plat + API\n",
    "   - **Mapping E1** : API / CSV\n",
    "\n",
    "4. **🔹 Baromètre économique et social**\n",
    "   - **Source** : INSEE Conjoncture + BVA Observatoire social\n",
    "   - **Thématique** : Pouvoir d'achat, chômage, inflation, emploi\n",
    "   - **Format** : Base SQL / CSV\n",
    "   - **Mapping E1** : Base de données / CSV\n",
    "\n",
    "5. **🔹 Baromètre des médias et de la confiance**\n",
    "   - **Source** : La Croix – Baromètre Kantar sur les médias\n",
    "   - **Thématique** : Information, confiance médiatique, fake news\n",
    "   - **Format** : Web scraping\n",
    "   - **Mapping E1** : Web Scraping\n",
    "\n",
    "6. **🔹 Baromètre sport & cohésion sociale**\n",
    "   - **Source** : Ministère des Sports / CNOSF / Paris 2024\n",
    "   - **Thématique** : Sport, bien-être, fierté nationale, cohésion\n",
    "   - **Format** : CSV / API\n",
    "   - **Mapping E1** : CSV / API\n",
    "\n",
    "7. **🔹 Baromètre des discriminations et égalité**\n",
    "   - **Source** : Défenseur des Droits / IFOP\n",
    "   - **Thématique** : Inclusion, diversité, égalité femmes-hommes\n",
    "   - **Format** : CSV / API\n",
    "   - **Mapping E1** : CSV / API\n",
    "\n",
    "8. **🔹 Baromètre santé mentale et bien-être**\n",
    "   - **Source** : Santé Publique France – CoviPrev\n",
    "   - **Thématique** : Stress, anxiété, santé mentale post-COVID\n",
    "   - **Format** : CSV\n",
    "   - **Mapping E1** : CSV\n",
    "\n",
    "9. **🔹 Baromètre climat social et tensions**\n",
    "   - **Source** : Elabe / BFMTV Opinion 2024\n",
    "   - **Thématique** : Colère, frustration, confiance, peur\n",
    "   - **Format** : Web Scraping\n",
    "   - **Mapping E1** : Web Scraping\n",
    "\n",
    "10. **🔹 Baromètre innovation et IA**\n",
    "    - **Source** : CNIL / France IA / Capgemini Research Institute\n",
    "    - **Thématique** : Adoption de l'IA, confiance numérique\n",
    "    - **Format** : PDF / API\n",
    "    - **Mapping E1** : API / PDF scraping\n",
    "\n",
    "### 📚 Documentation Complète\n",
    "\n",
    "Voir `docs/BAROMETRES_SOURCES.md` pour :\n",
    "- Détails par baromètre (URLs, format, tables PostgreSQL)\n",
    "- Plan d'implémentation E2/E3\n",
    "- Notes techniques et RGPD\n",
    "\n",
    "### 🎯 Plan d'Implémentation\n",
    "\n",
    "**Phase E2 (Priorité)** :\n",
    "1. Baromètre économique et social (INSEE)\n",
    "2. Baromètre des émotions (Kantar/Ipsos)\n",
    "3. Baromètre santé mentale (Santé Publique France)\n",
    "\n",
    "**Phase E3 (Complément)** :\n",
    "4-10. Autres baromètres selon priorités métier\n",
    "\n",
    "**Architecture** : Tous les baromètres suivront le même pipeline que les sources E1 :\n",
    "- Logging structuré\n",
    "- Upload MinIO\n",
    "- Insertion PostgreSQL avec helpers\n",
    "- Déduplication SHA-256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# SOURCES 2, 3, 4, 5 : À IMPLÉMENTER AVEC VRAIES SOURCES\n",
    "# =====================================================\n",
    "#\n",
    "# Pour respecter l'architecture pipeline du notebook datasens_E1_v2.ipynb,\n",
    "# les sources 2-5 doivent être implémentées avec :\n",
    "# 1. Collecte réelle depuis API/BDD/Scraping/GDELT\n",
    "# 2. Upload MinIO pour traçabilité DataLake\n",
    "# 3. Insertion PostgreSQL avec fonctions helpers (create_flux, insert_documents)\n",
    "# 4. Logging complet via logger.info/error\n",
    "#\n",
    "# Voir notebook datasens_E1_v2.ipynb pour implémentations complètes :\n",
    "# - Source 2 : Kaggle DB (SQLite → Postgres via Pandas)\n",
    "# - Source 3 : OpenWeatherMap API (voir Cell 20 du notebook existant)\n",
    "# - Source 4 : Web Scraping MonAvisCitoyen (voir Cell 26 du notebook existant)\n",
    "# - Source 5 : GDELT GKG Big Data (voir Cell 28 du notebook existant)\n",
    "\n",
    "logger.info(\"\\n📋 Pour sources 2-5 : Voir notebooks/datasens_E1_v2.ipynb\")\n",
    "logger.info(\"   → Exemples complets avec vraies API keys et collectes réelles\")\n",
    "\n",
    "# =====================================================\n",
    "# MANIFEST JSON (Traçabilité finale)\n",
    "# =====================================================\n",
    "logger.info(\"📋 Création du manifest JSON\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "# Compter les données collectées\n",
    "with engine.connect() as conn:\n",
    "    counts = {\n",
    "        \"documents\": conn.execute(text(\"SELECT COUNT(*) FROM document\")).scalar(),\n",
    "        \"flux\": conn.execute(text(\"SELECT COUNT(*) FROM flux\")).scalar(),\n",
    "        \"sources\": conn.execute(text(\"SELECT COUNT(*) FROM source\")).scalar(),\n",
    "        \"meteo\": conn.execute(text(\"SELECT COUNT(*) FROM meteo\")).scalar(),\n",
    "        \"evenements\": conn.execute(text(\"SELECT COUNT(*) FROM evenement\")).scalar(),\n",
    "    }\n",
    "\n",
    "manifest = {\n",
    "    \"run_id\": ts(),\n",
    "    \"timestamp_utc\": datetime.now(UTC).isoformat(),\n",
    "    \"notebook_version\": \"03_ingest_sources.ipynb\",\n",
    "    \"sources_ingested\": [\n",
    "        \"Kaggle CSV (fichier plat - 50% PG + 50% MinIO)\",\n",
    "        \"Kaggle DB (base de données - à implémenter)\",\n",
    "        \"OpenWeatherMap (API - à implémenter)\",\n",
    "        \"MonAvisCitoyen (scraping - à implémenter)\",\n",
    "        \"GDELT GKG (big data - à implémenter)\"\n",
    "    ],\n",
    "    \"counts\": counts,\n",
    "    \"postgres_db\": PG_DB,\n",
    "    \"minio_bucket\": MINIO_BUCKET,\n",
    "    \"raw_data_location\": str(RAW_DIR),\n",
    "    \"log_file\": str(log_file)\n",
    "}\n",
    "\n",
    "# Sauvegarder manifest local + MinIO\n",
    "manifest_path = MANIFESTS_DIR / f\"manifest_{manifest['run_id']}.json\"\n",
    "manifest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with manifest_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(manifest, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "try:\n",
    "    manifest_minio_uri = minio_upload(manifest_path, f\"manifests/{manifest_path.name}\")\n",
    "    logger.info(f\"✅ Manifest créé : {manifest_path.name}\")\n",
    "    logger.info(f\"☁️ Manifest MinIO : {manifest_minio_uri}\")\n",
    "except Exception as e:\n",
    "    log_error(\"MinIO\", e, \"Upload manifest\")\n",
    "    manifest_minio_uri = f\"local://{manifest_path}\"\n",
    "\n",
    "logger.info(\"\\n📊 Résumé ingestion :\")\n",
    "for key, value in counts.items():\n",
    "    logger.info(f\"   • {key}: {value}\")\n",
    "\n",
    "logger.info(\"\\n✅ Ingestion terminée ! (Source 1/5 complète, sources 2-5 à documenter)\")\n",
    "logger.info(\"   ➡️ Passez au notebook 04_crud_tests.ipynb\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
