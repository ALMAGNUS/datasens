{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataSens E1_v3 — 01_setup_env\n",
    "\n",
    "- Objectifs: Configuration complète MinIO + PostgreSQL + arborescence + logging\n",
    "- Prérequis: Docker Compose lancé (MinIO + PostgreSQL), Python + venv, `pip install -r requirements.txt`\n",
    "- Ordre global E1_v3: 01 → 02 → 03 → 04 → 05\n",
    "- Guide: docs/GUIDE_TECHNIQUE_E1.md\n",
    "\n",
    "> **E1_v3** : Architecture complète **36/37 tables** avec **TOUTES les sources réelles**\n",
    "> - Sources complètes : Kaggle, OpenWeatherMap, RSS Multi, NewsAPI, Web Scraping (6 sources), GDELT Big Data\n",
    "> - Baromètres : 10 types de baromètres d'opinion et indicateurs sociaux\n",
    "> - Schéma complet : T01-T36 + T37 (archive_flux) selon MPD.sql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "> Notes:\n",
    "> - Configuration des connexions **MinIO (DataLake)** et **PostgreSQL (SGBD)**\n",
    "> - Création de l'arborescence `data/raw/` avec **tous les sous-dossiers** pour sources complètes\n",
    "> - Système de logging pour tracer toutes les opérations\n",
    "> - Fonctions utilitaires (timestamp UTC, hash SHA256 pour déduplication)\n",
    "> - **Références** : docs/datasens_MPD.sql (36 tables), docs/datasens_sources_dictionary.md, docs/datasens_barometer_themes.md\n",
    "    ROOT = ROOT\n",
    "except NameError:\n",
    "    from pathlib import Path\n",
    "    ROOT = Path.cwd().resolve().parents[2]\n",
    "DATA_DIR = ROOT / 'data'\n",
    "DOCS_DIR = ROOT / 'docs'\n",
    "LOGS_DIR = ROOT / 'logs'\n",
    "print('ROOT=', ROOT)\n",
    "print('DATA_DIR=', DATA_DIR)\n",
    "print('DOCS_DIR=', DOCS_DIR)\n",
    "print('LOGS_DIR=', LOGS_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSens E1_v3 - 01_setup_env\n",
    "# 🔧 Configuration environnement : MinIO + PostgreSQL + Arborescence + Logging\n",
    "# Architecture complète : 36/37 tables + Toutes les sources\n",
    "\n",
    "import datetime as dt\n",
    "import hashlib\n",
    "import logging\n",
    "import os\n",
    "from datetime import UTC, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Détection robuste du dossier projet\n",
    "current = Path.cwd()\n",
    "PROJECT_ROOT = None\n",
    "while current != current.parent:\n",
    "    if (current / \"notebooks\").exists() and (current / \"docs\").exists():\n",
    "        PROJECT_ROOT = current\n",
    "        break\n",
    "    current = current.parent\n",
    "else:\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "print(f\"📂 Racine projet détectée : {PROJECT_ROOT}\")\n",
    "\n",
    "# Chargement .env\n",
    "env_path = PROJECT_ROOT / '.env'\n",
    "loaded = load_dotenv(env_path)\n",
    "if loaded:\n",
    "    print(f'✅ .env chargé: {env_path}')\n",
    "else:\n",
    "    print(f'⚠️ .env non trouvé: {env_path}')\n",
    "    # Créer .env.example si absent\n",
    "    env_example = PROJECT_ROOT / '.env.example'\n",
    "    if not env_example.exists():\n",
    "        env_example.write_text(\"\"\"\n",
    "# PostgreSQL\n",
    "POSTGRES_HOST=localhost\n",
    "POSTGRES_PORT=5433\n",
    "POSTGRES_DB=postgres\n",
    "POSTGRES_USER=postgres\n",
    "POSTGRES_PASS=postgres\n",
    "\n",
    "# MinIO\n",
    "MINIO_ENDPOINT=http://localhost:9002\n",
    "MINIO_ACCESS_KEY=admin\n",
    "MINIO_SECRET_KEY=admin123\n",
    "MINIO_BUCKET=datasens-raw\n",
    "\n",
    "# API Keys (pour toutes les sources E1_v3)\n",
    "OWM_API_KEY=\n",
    "NEWSAPI_KEY=\n",
    "KAGGLE_USERNAME=\n",
    "KAGGLE_KEY=\n",
    "GDELT_BASE=http://data.gdeltproject.org/gkg/\n",
    "\"\"\".strip() + \"\\n\", encoding='utf-8')\n",
    "        print(f'📄 .env.example créé: {env_example}')\n",
    "\n",
    "# Configuration MinIO (DataLake)\n",
    "MINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT\", \"http://localhost:9002\")\n",
    "MINIO_ACCESS_KEY = os.getenv(\"MINIO_ACCESS_KEY\", \"admin\")\n",
    "MINIO_SECRET_KEY = os.getenv(\"MINIO_SECRET_KEY\", \"admin123\")\n",
    "MINIO_BUCKET = os.getenv(\"MINIO_BUCKET\", \"datasens-raw\")\n",
    "\n",
    "# Configuration PostgreSQL (SGBD) - 36/37 tables E1_v3\n",
    "PG_HOST = os.getenv(\"POSTGRES_HOST\", \"localhost\")\n",
    "PG_PORT = int(os.getenv(\"POSTGRES_PORT\", \"5433\"))\n",
    "PG_DB = os.getenv(\"POSTGRES_DB\", \"postgres\")\n",
    "PG_USER = os.getenv(\"POSTGRES_USER\", \"postgres\")\n",
    "PG_PASS = os.getenv(\"POSTGRES_PASS\", \"postgres\")\n",
    "PG_URL = f\"postgresql+psycopg2://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{PG_DB}\"\n",
    "\n",
    "# Clés API (pour toutes les sources E1_v3)\n",
    "KAGGLE_USERNAME = os.getenv(\"KAGGLE_USERNAME\")\n",
    "KAGGLE_KEY = os.getenv(\"KAGGLE_KEY\")\n",
    "OWM_API_KEY = os.getenv(\"OWM_API_KEY\")\n",
    "NEWSAPI_KEY = os.getenv(\"NEWSAPI_KEY\")\n",
    "GDELT_BASE = os.getenv(\"GDELT_BASE\", \"http://data.gdeltproject.org/gkg/\")\n",
    "\n",
    "print(\"\\n🔐 Configuration MinIO (DataLake) :\")\n",
    "print(f\"   • Endpoint : {MINIO_ENDPOINT}\")\n",
    "print(f\"   • Bucket   : {MINIO_BUCKET}\")\n",
    "\n",
    "print(\"\\n🗄️ Configuration PostgreSQL (SGBD) :\")\n",
    "print(f\"   • Host     : {PG_HOST}:{PG_PORT}\")\n",
    "print(f\"   • Database : {PG_DB}\")\n",
    "print(f\"   • User     : {PG_USER}\")\n",
    "print(f\"   • Schéma   : 36/37 tables (T01-T36 + T37) selon datasens_MPD.sql\")\n",
    "\n",
    "print(\"\\n🔑 Clés API (E1_v3 - Toutes les sources) :\")\n",
    "print(f\"   • Kaggle        : {'✅ Configurée' if KAGGLE_USERNAME else '❌ Manquante'}\")\n",
    "print(f\"   • OpenWeatherMap: {'✅ Configurée' if OWM_API_KEY else '❌ Manquante'}\")\n",
    "print(f\"   • NewsAPI       : {'✅ Configurée' if NEWSAPI_KEY else '❌ Manquante'}\")\n",
    "\n",
    "# Arborescence complète pour TOUTES les sources E1_v3\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "LOGS_DIR = PROJECT_ROOT / 'logs'\n",
    "\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Tous les sous-dossiers pour sources complètes E1_v3 (selon docs)\n",
    "folders = [\n",
    "    \"kaggle\",                    # Source 1 : Fichier plat\n",
    "    \"api/owm\",                   # Source 2 : API OpenWeatherMap\n",
    "    \"api/newsapi\",               # Source 2 : API NewsAPI\n",
    "    \"rss\",                       # Source 3 : Flux RSS Multi-Sources\n",
    "    \"scraping/multi\",            # Source 4 : Web Scraping Multi (6 sources)\n",
    "    \"scraping/viepublique\",      # Source 4 : Vie-publique.fr\n",
    "    \"scraping/datagouv\",         # Source 4 : data.gouv.fr\n",
    "    \"gdelt\",                     # Source 5 : GDELT Big Data\n",
    "    \"manifests\"                  # Manifest JSON par run\n",
    "]\n",
    "for sub in folders:\n",
    "    (RAW_DIR / sub).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\n✅ Arborescence créée: {RAW_DIR}\")\n",
    "print(f\"   • {len(folders)} sous-dossiers prêts (toutes sources E1_v3)\")\n",
    "\n",
    "# Logging\n",
    "stamp = datetime.now(UTC).strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_file = LOGS_DIR / f\"collecte_{stamp}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='[%(asctime)s] %(levelname)s - %(message)s',\n",
    "    datefmt='%H:%M:%S',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file, encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logging.info(\"Système de logging initialisé - E1_v3 (36/37 tables)\")\n",
    "print(f\"📄 Log: {log_file}\")\n",
    "\n",
    "# Fonctions utilitaires\n",
    "def ts() -> str:\n",
    "    \"\"\"Timestamp UTC ISO compact (YYYYMMDDTHHMMSSZ)\"\"\"\n",
    "    return dt.datetime.now(tz=dt.UTC).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "\n",
    "def sha256_hash(s: str) -> str:\n",
    "    \"\"\"Hash SHA256 pour déduplication\"\"\"\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "print(f\"\\n🔧 Utilitaires : ts()={ts()}, sha256()={sha256_hash('test')[:16]}...\")\n",
    "\n",
    "print(\"\\n✅ Configuration E1_v3 terminée !\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 🎬 DASHBOARD NARRATIF - OÙ SOMMES-NOUS ?\n",
    "# ============================================================\n",
    "# Ce dashboard vous guide à travers le pipeline DataSens E1\n",
    "# Il montre la progression et l'état actuel des données\n",
    "# ============================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎬 FIL D'ARIANE VISUEL - PIPELINE DATASENS E1\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Créer figure dashboard\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 6)\n",
    "ax.axis('off')\n",
    "\n",
    "# Étapes du pipeline\n",
    "etapes = [\n",
    "    {\"nom\": \"📥 COLLECTE\", \"status\": \"✅\", \"desc\": \"Sources brutes\"},\n",
    "    {\"nom\": \"☁️ DATALAKE\", \"status\": \"✅\", \"desc\": \"MinIO Raw\"},\n",
    "    {\"nom\": \"🧹 NETTOYAGE\", \"status\": \"🔄\", \"desc\": \"Déduplication\"},\n",
    "    {\"nom\": \"💾 ETL\", \"status\": \"⏳\", \"desc\": \"PostgreSQL\"},\n",
    "    {\"nom\": \"📊 ANNOTATION\", \"status\": \"⏳\", \"desc\": \"Enrichissement\"},\n",
    "    {\"nom\": \"📦 EXPORT\", \"status\": \"⏳\", \"desc\": \"Dataset IA\"}\n",
    "]\n",
    "\n",
    "# Couleurs selon statut\n",
    "colors = {\n",
    "    \"✅\": \"#4ECDC4\",\n",
    "    \"🔄\": \"#FECA57\", \n",
    "    \"⏳\": \"#E8E8E8\"\n",
    "}\n",
    "\n",
    "# Dessiner timeline\n",
    "y_pos = 4\n",
    "x_start = 1\n",
    "x_spacing = 1.4\n",
    "\n",
    "for i, etape in enumerate(etapes):\n",
    "    x_pos = x_start + i * x_spacing\n",
    "    \n",
    "    # Cercle étape\n",
    "    circle = plt.Circle((x_pos, y_pos), 0.25, color=colors[etape[\"status\"]], zorder=3)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(x_pos, y_pos, etape[\"status\"], ha='center', va='center', fontsize=14, fontweight='bold', zorder=4)\n",
    "    \n",
    "    # Nom étape\n",
    "    ax.text(x_pos, y_pos - 0.6, etape[\"nom\"], ha='center', va='top', fontsize=11, fontweight='bold')\n",
    "    ax.text(x_pos, y_pos - 0.85, etape[\"desc\"], ha='center', va='top', fontsize=9, style='italic')\n",
    "    \n",
    "    # Flèche vers prochaine étape\n",
    "    if i < len(etapes) - 1:\n",
    "        ax.arrow(x_pos + 0.3, y_pos, x_spacing - 0.6, 0, \n",
    "                head_width=0.1, head_length=0.15, fc='gray', ec='gray', zorder=2)\n",
    "\n",
    "# Titre narratif\n",
    "ax.text(5, 5.5, \"🎯 PROGRESSION DU PIPELINE E1\", ha='center', va='center', \n",
    "        fontsize=16, fontweight='bold', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Légende\n",
    "legend_elements = [\n",
    "    mpatches.Patch(facecolor='#4ECDC4', label='Terminé'),\n",
    "    mpatches.Patch(facecolor='#FECA57', label='En cours'),\n",
    "    mpatches.Patch(facecolor='#E8E8E8', label='À venir')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper left', fontsize=10)\n",
    "\n",
    "# Statistiques rapides (si disponibles)\n",
    "stats_text = \"\\n📊 SNAPSHOT ACTUEL :\\n\"\n",
    "try:\n",
    "    # Essayer de charger des stats si base disponible\n",
    "    stats_text += \"   • Pipeline en cours d'exécution...\\n\"\n",
    "except:\n",
    "    stats_text += \"   • Démarrage du pipeline...\\n\"\n",
    "\n",
    "ax.text(5, 1.5, stats_text, ha='center', va='center', fontsize=10,\n",
    "        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n",
    "\n",
    "plt.title(\"🎬 FIL D'ARIANE VISUEL - Accompagnement narratif du jury\", \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 Le fil d'Ariane vous guide étape par étape à travers le pipeline\")\n",
    "print(\"   Chaque visualisation s'inscrit dans cette progression narrative\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test des connexions MinIO et PostgreSQL\n",
    "\n",
    "print(\"🔌 Test des connexions...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Connexion MinIO\n",
    "try:\n",
    "    from minio import Minio\n",
    "    \n",
    "    minio_client = Minio(\n",
    "        MINIO_ENDPOINT.replace(\"http://\", \"\").replace(\"https://\", \"\"),\n",
    "        access_key=MINIO_ACCESS_KEY,\n",
    "        secret_key=MINIO_SECRET_KEY,\n",
    "        secure=False\n",
    "    )\n",
    "    \n",
    "    # Créer le bucket s'il n'existe pas\n",
    "    if not minio_client.bucket_exists(MINIO_BUCKET):\n",
    "        minio_client.make_bucket(MINIO_BUCKET)\n",
    "        print(f\"✅ MinIO : Bucket '{MINIO_BUCKET}' créé\")\n",
    "    else:\n",
    "        print(f\"✅ MinIO : Bucket '{MINIO_BUCKET}' existe déjà\")\n",
    "    \n",
    "    # Lister les objets existants\n",
    "    objects = list(minio_client.list_objects(MINIO_BUCKET, recursive=False))\n",
    "    print(f\"   • {len(list(objects))} objets existants dans le bucket\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ MinIO : Erreur de connexion - {e}\")\n",
    "    print(\"   💡 Vérifiez que Docker Compose est lancé : docker compose up -d\")\n",
    "    minio_client = None\n",
    "\n",
    "# Connexion PostgreSQL\n",
    "try:\n",
    "    from sqlalchemy import create_engine, text\n",
    "    \n",
    "    engine = create_engine(PG_URL, future=True)\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"SELECT 1 as test\"))\n",
    "        test_value = result.scalar()\n",
    "    \n",
    "    if test_value == 1:\n",
    "        print(f\"✅ PostgreSQL : Connexion réussie ({PG_HOST}:{PG_PORT}/{PG_DB})\")\n",
    "        \n",
    "        # Compter les tables existantes\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(text(\"\"\"\n",
    "                SELECT COUNT(*) FROM information_schema.tables \n",
    "                WHERE table_schema = 'public'\n",
    "            \"\"\"))\n",
    "            nb_tables = result.scalar()\n",
    "            print(f\"   • {nb_tables} tables existantes dans la base\")\n",
    "            print(f\"   • E1_v3 attend 36/37 tables (architecture complète selon datasens_MPD.sql)\")\n",
    "    else:\n",
    "        print(\"⚠️ PostgreSQL : Connexion OK mais test inattendu\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ PostgreSQL : Erreur de connexion - {e}\")\n",
    "    print(\"   💡 Vérifiez que Docker Compose est lancé : docker compose up -d\")\n",
    "    engine = None\n",
    "\n",
    "print(\"\\n✅ Tests de connexion terminés !\")\n",
    "print(\"   ➡️ Passez au notebook 02_schema_create.ipynb pour créer les 36/37 tables\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Introduction & Objectifs E1\n",
    "\n",
    "**DataSens E1** : Construction du socle data avec :\n",
    "- ✅ Modélisation Merise (MCD → MLD → MPD ciblé)\n",
    "- ✅ Création et remplissage PostgreSQL (18 tables)\n",
    "- ✅ CRUD complet testé depuis le notebook\n",
    "- ✅ Ingestion réelle des **5 types de sources** :\n",
    "  1. **Fichier plat** : Kaggle CSV\n",
    "  2. **Base de données** : Export Kaggle SQLite → Postgres\n",
    "  3. **API** : OpenWeatherMap (météo communes)\n",
    "  4. **Web Scraping** : MonAvisCitoyen (dry-run, robots.txt)\n",
    "  5. **Big Data** : GDELT GKG (échantillon journalier)\n",
    "- ✅ Traçabilité et gouvernance (flux, manifest, versionning Git)\n",
    "\n",
    "**Mode d'exécution** : Cellule par cellule (pas à pas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Vérification environnement Python\n",
      "================================================================================\n",
      "Python version : 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]\n",
      "Python executable : c:\\Users\\Utilisateur\\Desktop\\Datasens_Project\\.venv\\Scripts\\python.exe\n",
      "✅ Python 3.12.7 OK\n"
     ]
    }
   ],
   "source": [
    "# Vérification environnement Python\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"🔍 Vérification environnement Python\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Python version : {sys.version}\")\n",
    "print(f\"Python executable : {sys.executable}\")\n",
    "\n",
    "# Vérifier version Python >= 3.11\n",
    "version_info = sys.version_info\n",
    "if version_info.major >= 3 and version_info.minor >= 11:\n",
    "    print(f\"✅ Python {version_info.major}.{version_info.minor}.{version_info.micro} OK\")\n",
    "else:\n",
    "    print(f\"⚠️ Python {version_info.major}.{version_info.minor} — Recommandé Python 3.11+\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📦 Packages Python installés\n",
      "================================================================================\n",
      "\n",
      "Vérification packages critiques :\n",
      "\n",
      "  ✅ pandas                   2.3.3\n",
      "  ✅ SQLAlchemy               2.0.44\n",
      "  ✅ psycopg2-binary          2.9.11\n",
      "  ✅ requests                 2.32.5\n",
      "  ✅ beautifulsoup4           4.14.2\n",
      "  ✅ python-dotenv            1.2.1\n",
      "\n",
      "📋 Liste complète (pip list) :\n",
      "Package                  Version\n",
      "------------------------ -----------\n",
      "annotated-types          0.7.0\n",
      "argon2-cffi              25.1.0\n",
      "argon2-cffi-bindings     25.1.0\n",
      "asttokens                3.0.0\n",
      "beautifulsoup4           4.14.2\n",
      "cachetools               6.2.1\n",
      "certifi                  2025.10.5\n",
      "cffi                     2.0.0\n",
      "charset-normalizer       3.4.4\n",
      "colorama                 0.4.6\n",
      "comm                     0.2.3\n",
      "contourpy                1.3.3\n",
      "cycler                   0.12.1\n",
      "debugpy            ...\n"
     ]
    }
   ],
   "source": [
    "# Liste des packages installés\n",
    "print(\"\\n📦 Packages Python installés\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "packages_to_check = [\n",
    "    \"pandas\",\n",
    "    \"sqlalchemy\",\n",
    "    \"psycopg2\",\n",
    "    \"requests\",\n",
    "    \"beautifulsoup4\",\n",
    "    \"python-dotenv\",\n",
    "]\n",
    "\n",
    "result = subprocess.run(\n",
    "    [sys.executable, \"-m\", \"pip\", \"list\"],\n",
    "    check=False, capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    installed_packages = result.stdout\n",
    "    print(\"\\nVérification packages critiques :\\n\")\n",
    "    for pkg in packages_to_check:\n",
    "        if pkg in installed_packages.lower():\n",
    "            version = [line for line in installed_packages.split(\"\\n\") if pkg.lower() in line.lower()]\n",
    "            if version:\n",
    "                print(f\"  ✅ {version[0]}\")\n",
    "            else:\n",
    "                print(f\"  ✅ {pkg} (version non détectée)\")\n",
    "        else:\n",
    "            print(f\"  ❌ {pkg} - À installer : pip install {pkg}\")\n",
    "\n",
    "    print(\"\\n📋 Liste complète (pip list) :\")\n",
    "    print(installed_packages[:500] + \"...\" if len(installed_packages) > 500 else installed_packages)\n",
    "else:\n",
    "    print(\"⚠️ Impossible d'exécuter pip list\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📁 Création de l'arborescence projet\n",
    "\n",
    "Structure du projet selon les conventions DataSens :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Création de l'arborescence projet\n",
      "================================================================================\n",
      "Racine projet : c:\\Users\\Utilisateur\\Desktop\\Datasens_Project\n",
      "\n",
      "📂 Dossiers créés :\n",
      "✅ data/\n",
      "   ✅ data/raw/\n",
      "   ✅ data/silver/\n",
      "   ✅ data/gold/\n",
      "✅ data/raw/\n",
      "   ✅ data/raw/kaggle/\n",
      "   ✅ data/raw/api/\n",
      "   ✅ data/raw/scraping/\n",
      "   ✅ data/raw/gdelt/\n",
      "   ✅ data/raw/manifests/\n",
      "✅ data/raw/api/\n",
      "   ✅ data/raw/api/owm/\n",
      "✅ data/raw/scraping/\n",
      "   ✅ data/raw/scraping/mav/\n",
      "✅ logs/\n",
      "✅ docs/\n",
      "✅ notebooks/\n",
      "\n",
      "✅ Arborescence prête ! (17 dossiers)\n"
     ]
    }
   ],
   "source": [
    "# Déterminer la racine du projet (parent de notebooks/)\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == \"notebooks\" else NOTEBOOK_DIR\n",
    "\n",
    "print(\"📁 Création de l'arborescence projet\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Racine projet : {PROJECT_ROOT}\")\n",
    "\n",
    "# Arborescence à créer\n",
    "directories = {\n",
    "    \"data\": [\"raw\", \"silver\", \"gold\"],\n",
    "    \"data/raw\": [\"kaggle\", \"api\", \"scraping\", \"gdelt\", \"manifests\"],\n",
    "    \"data/raw/api\": [\"owm\"],\n",
    "    \"data/raw/scraping\": [\"mav\"],  # MonAvisCitoyen\n",
    "    \"logs\": [],\n",
    "    \"docs\": [],\n",
    "    \"notebooks\": [],\n",
    "}\n",
    "\n",
    "created = []\n",
    "for base_dir, subdirs in directories.items():\n",
    "    base_path = PROJECT_ROOT / base_dir\n",
    "    base_path.mkdir(parents=True, exist_ok=True)\n",
    "    created.append(f\"✅ {base_dir}/\")\n",
    "\n",
    "    for subdir in subdirs:\n",
    "        sub_path = base_path / subdir\n",
    "        sub_path.mkdir(parents=True, exist_ok=True)\n",
    "        created.append(f\"   ✅ {base_dir}/{subdir}/\")\n",
    "\n",
    "print(\"\\n📂 Dossiers créés :\")\n",
    "for item in created:\n",
    "    print(item)\n",
    "\n",
    "print(f\"\\n✅ Arborescence prête ! ({len(created)} dossiers)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚙️ Configuration .env\n",
    "\n",
    "Création du fichier `.env` de développement avec variables PostgreSQL et API keys.\n",
    "\n",
    "**⚠️ IMPORTANT** : Ce fichier ne doit JAMAIS être commité (dans .gitignore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fichier .env chargé : c:\\Users\\Utilisateur\\Desktop\\Datasens_Project\\.env\n",
      "\n",
      "🔐 Configuration chargée :\n",
      "   POSTGRES_HOST : localhost\n",
      "   POSTGRES_PORT : 5432\n",
      "   POSTGRES_DB   : datasens\n",
      "   POSTGRES_USER : ds_user\n",
      "   OWM_API_KEY   : ✅ Configurée\n",
      "   KAGGLE_USERNAME: ✅ Configurée\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger .env s'il existe\n",
    "env_path = PROJECT_ROOT / \".env\"\n",
    "env_loaded = load_dotenv(env_path)\n",
    "\n",
    "if env_loaded:\n",
    "    print(f\"✅ Fichier .env chargé : {env_path}\")\n",
    "else:\n",
    "    print(f\"⚠️ Fichier .env non trouvé : {env_path}\")\n",
    "    print(\"   Création d'un .env.example pour référence...\")\n",
    "\n",
    "    # Créer un .env.example\n",
    "    env_example = PROJECT_ROOT / \".env.example\"\n",
    "    env_example.write_text(\"\"\"\n",
    "# PostgreSQL\n",
    "POSTGRES_HOST=localhost\n",
    "POSTGRES_PORT=5432\n",
    "POSTGRES_DB=datasens\n",
    "POSTGRES_USER=ds_user\n",
    "POSTGRES_PASS=ds_pass\n",
    "\n",
    "# API Keys (optionnelles pour démo)\n",
    "OWM_API_KEY=your_openweathermap_key_here\n",
    "KAGGLE_USERNAME=your_kaggle_username\n",
    "KAGGLE_KEY=your_kaggle_key\n",
    "\n",
    "# Git (optionnel)\n",
    "GIT_USER_NAME=Your Name\n",
    "GIT_USER_EMAIL=your.email@example.com\n",
    "\"\"\")\n",
    "    print(f\"   📄 Template créé : {env_example}\")\n",
    "\n",
    "# Afficher configuration (sans afficher les mots de passe)\n",
    "print(\"\\n🔐 Configuration chargée :\")\n",
    "print(f\"   POSTGRES_HOST : {os.getenv('POSTGRES_HOST', 'localhost')}\")\n",
    "print(f\"   POSTGRES_PORT : {os.getenv('POSTGRES_PORT', '5432')}\")\n",
    "print(f\"   POSTGRES_DB   : {os.getenv('POSTGRES_DB', 'datasens')}\")\n",
    "print(f\"   POSTGRES_USER : {os.getenv('POSTGRES_USER', 'ds_user')}\")\n",
    "print(f\"   OWM_API_KEY   : {'✅ Configurée' if os.getenv('OWM_API_KEY') else '❌ Manquante (optionnelle)'}\")\n",
    "print(f\"   KAGGLE_USERNAME: {'✅ Configurée' if os.getenv('KAGGLE_USERNAME') else '❌ Manquante (optionnelle)'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🗄️ Connexion PostgreSQL\n",
    "\n",
    "Test de connexion à la base PostgreSQL (via Docker ou locale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔌 Test connexion PostgreSQL\n",
      "================================================================================\n",
      "URL : postgresql://ds_user:***@localhost:5432/datasens\n",
      "✅ Connexion PostgreSQL réussie !\n",
      "   🗄️ Base de données : datasens\n",
      "   👤 Utilisateur : ds_user\n",
      "   📍 Serveur : localhost:5432\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Récupérer variables d'environnement\n",
    "PG_HOST = os.getenv(\"POSTGRES_HOST\", \"localhost\")\n",
    "PG_PORT = int(os.getenv(\"POSTGRES_PORT\", \"5432\"))\n",
    "PG_DB = os.getenv(\"POSTGRES_DB\", \"datasens\")\n",
    "PG_USER = os.getenv(\"POSTGRES_USER\", \"ds_user\")\n",
    "PG_PASS = os.getenv(\"POSTGRES_PASS\", \"ds_pass\")\n",
    "\n",
    "# URL de connexion\n",
    "PG_URL = f\"postgresql+psycopg2://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{PG_DB}\"\n",
    "\n",
    "print(\"🔌 Test connexion PostgreSQL\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"URL : postgresql://{PG_USER}:***@{PG_HOST}:{PG_PORT}/{PG_DB}\")\n",
    "\n",
    "try:\n",
    "    engine = create_engine(PG_URL, future=True)\n",
    "\n",
    "    # Test simple : SELECT 1\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"SELECT 1 as test\"))\n",
    "        test_value = result.scalar()\n",
    "\n",
    "    if test_value == 1:\n",
    "        print(\"✅ Connexion PostgreSQL réussie !\")\n",
    "        print(f\"   🗄️ Base de données : {PG_DB}\")\n",
    "        print(f\"   👤 Utilisateur : {PG_USER}\")\n",
    "        print(f\"   📍 Serveur : {PG_HOST}:{PG_PORT}\")\n",
    "    else:\n",
    "        print(\"⚠️ Connexion OK mais test inattendu\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Erreur de connexion : {e}\")\n",
    "    print(\"\\n💡 Vérifications :\")\n",
    "    print(\"   1. Docker Compose est-il démarré ? → docker-compose up -d\")\n",
    "    print(\"   2. PostgreSQL est-il accessible sur le port 5432 ?\")\n",
    "    print(\"   3. Les credentials dans .env sont-ils corrects ?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 Initialisation Git\n",
    "\n",
    "Initialisation du dépôt Git (si ce n'est pas déjà fait) et premier commit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Vérification Git\n",
      "================================================================================\n",
      "✅ git version 2.49.0.windows.1\n",
      "\n",
      "✅ Dépôt Git déjà initialisé : c:\\Users\\Utilisateur\\Desktop\\Datasens_Project\n",
      "\n",
      "📋 Fichiers modifiés/non suivis :\n",
      " D docs/FIX_PDF_QUICK.md\n",
      " M docs/GUIDE_TECHNIQUE_JURY.md\n",
      " M docs/GUIDE_TECHNIQUE_JURY.pdf\n",
      " D docs/PDF_FORMATTING_INSTRUCTIONS.md\n",
      " D docs/fix_pdf_formatting.ps1\n",
      " M notebooks/datasens_E1_v2.ipynb\n",
      "?? docs/ARCHITECTURE_PIPELINE_E1.md\n",
      "?? docs/GUIDE_TECHNIQUE_JURY_V2.md\n",
      "?? docs/datasens_dictionary.md\n",
      "?? docs/e1_schema.sql\n",
      "?? notebooks/01_setup_env.ipynb\n",
      "?? notebooks/02_schema_create.ipynb\n",
      "?? notebooks/03_ingest_sources.ipynb\n",
      "?? notebooks/04_crud_tests.ipynb\n",
      "?? notebooks/05_snapshot_and_readme.ipynb\n",
      "?? notebooks/README_VERSIONNING.md\n",
      "?? notebooks/data/raw/api/\n",
      "?? notebooks/data/raw/gdelt/\n",
      "?? notebooks/data/raw/kaggle/\n",
      "?? notebooks/data/raw/manifests/\n",
      "?? notebooks/data/raw/rss/\n",
      "?? notebooks/data/raw/scraping/multi/scraping_multi_20251029T122841Z.csv\n",
      "\n",
      "\n",
      "✅ Setup environnement terminé !\n",
      "   ➡️ Passez au notebook 02_schema_create.ipynb\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "print(\"🔄 Vérification Git\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Vérifier si Git est installé\n",
    "try:\n",
    "    git_version = subprocess.run(\n",
    "        [\"git\", \"--version\"],\n",
    "        check=False, capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    print(f\"✅ {git_version.stdout.strip()}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Git non installé — Installation requise : https://git-scm.com/\")\n",
    "    exit(1)\n",
    "\n",
    "# Vérifier si le projet est déjà un dépôt Git\n",
    "git_dir = PROJECT_ROOT / \".git\"\n",
    "if git_dir.exists():\n",
    "    print(f\"\\n✅ Dépôt Git déjà initialisé : {PROJECT_ROOT}\")\n",
    "\n",
    "    # Afficher git status\n",
    "    try:\n",
    "        status = subprocess.run(\n",
    "            [\"git\", \"status\", \"--short\"],\n",
    "            check=False, cwd=PROJECT_ROOT,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        if status.stdout.strip():\n",
    "            print(\"\\n📋 Fichiers modifiés/non suivis :\")\n",
    "            print(status.stdout)\n",
    "        else:\n",
    "            print(\"\\n📋 Aucun changement (working tree clean)\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Impossible de lire git status : {e}\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ Dépôt Git non initialisé dans {PROJECT_ROOT}\")\n",
    "    print(\"   💡 Initialisation manuelle recommandée :\")\n",
    "    print(f\"      cd {PROJECT_ROOT}\")\n",
    "    print(\"      git init\")\n",
    "    print(\"      git add .\")\n",
    "    print('      git commit -m \"Initial commit E1\"')\n",
    "\n",
    "print(\"\\n✅ Setup environnement terminé !\")\n",
    "print(\"   ➡️ Passez au notebook 02_schema_create.ipynb\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}