{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdd5d970",
   "metadata": {},
   "source": [
    "# üì∏ DataSens E1 ‚Äî Notebook 5 : Snapshot et README\n",
    "\n",
    "**üéØ Objectif** : Cr√©er un bilan E1, exporter le DDL/CSV, cr√©er un tag Git et d√©finir la roadmap E2/E3\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Contenu de ce notebook\n",
    "\n",
    "1. **Bilan E1** : Ce qui est fait / √† faire\n",
    "2. **Export DDL** : Sauvegarde du sch√©ma SQL dans `docs/e1_schema.sql`\n",
    "3. **Export CSV** : Snapshots des donn√©es dans `data/gold/`\n",
    "4. **Tag Git** : Cr√©ation du tag `E1_REAL_YYYYMMDD`\n",
    "5. **Roadmap E2/E3** : Prochaines √©tapes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db577bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Inventaire E1 ‚Äî Sources et traces de collecte (DataLake + PostgreSQL)\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == \"notebooks\" else NOTEBOOK_DIR\n",
    "load_dotenv(PROJECT_ROOT / \".env\")\n",
    "\n",
    "# Fail-fast DB URL (3s timeout)\n",
    "PG_URL = (\n",
    "    f\"postgresql+psycopg2://{os.getenv('POSTGRES_USER','ds_user')}:{os.getenv('POSTGRES_PASS','ds_pass')}@\"\n",
    "    f\"{os.getenv('POSTGRES_HOST','localhost')}:{int(os.getenv('POSTGRES_PORT','5432'))}/\"\n",
    "    f\"{os.getenv('POSTGRES_DB','datasens')}?connect_timeout=3\"\n",
    ")\n",
    "\n",
    "# Option pour ignorer DB si instable\n",
    "SKIP_DB = os.getenv(\"DS_SKIP_DB\", \"0\") == \"1\"\n",
    "engine = create_engine(PG_URL, future=True, pool_pre_ping=True)\n",
    "\n",
    "RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "\n",
    "# Limiter le scan (d√©fensif)\n",
    "def list_top(path: Path, pattern: str, limit: int = 50):\n",
    "    try:\n",
    "        files = list(path.glob(pattern)) if path.exists() else []\n",
    "        return files[:limit]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "paths = {\n",
    "    \"kaggle_csv\": list_top(RAW / \"kaggle\", \"*.csv\"),\n",
    "    \"owm_api\": list_top(RAW / \"api\" / \"owm\", \"*.csv\"),\n",
    "    \"rss_multi\": list_top(RAW / \"rss\", \"*.csv\"),\n",
    "    \"scraping_multi\": list_top(RAW / \"scraping\" / \"multi\", \"*.csv\"),\n",
    "    \"gdelt_gkg\": list_top(RAW / \"gdelt\", \"*.zip\"),\n",
    "    \"manifests\": list_top(RAW / \"manifests\", \"*.json\"),\n",
    "}\n",
    "\n",
    "# Comptes DB (si tables pr√©sentes)\n",
    "db_counts = {\"document\": None, \"flux\": None, \"source\": None, \"meteo\": None, \"evenement\": None}\n",
    "docs_by_type = []\n",
    "\n",
    "if not SKIP_DB:\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            # Timeout de requ√™te 3s\n",
    "            try:\n",
    "                conn.exec_driver_sql(\"SET LOCAL statement_timeout = 3000\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            for table in [\"document\", \"flux\", \"source\", \"meteo\", \"evenement\"]:\n",
    "                try:\n",
    "                    db_counts[table] = conn.execute(text(f\"SELECT COUNT(*) FROM {table}\")).scalar()\n",
    "                except Exception:\n",
    "                    db_counts[table] = None\n",
    "\n",
    "            try:\n",
    "                docs_by_type = conn.execute(text(\n",
    "                    \"\"\"\n",
    "                    SELECT td.libelle AS type_donnee, COUNT(d.id_doc) AS nb_docs\n",
    "                    FROM document d\n",
    "                    LEFT JOIN flux f ON d.id_flux = f.id_flux\n",
    "                    LEFT JOIN source s ON f.id_source = s.id_source\n",
    "                    LEFT JOIN type_donnee td ON s.id_type_donnee = td.id_type_donnee\n",
    "                    GROUP BY td.libelle\n",
    "                    ORDER BY nb_docs DESC NULLS LAST\n",
    "                    \"\"\"\n",
    "                )).fetchall()\n",
    "            except Exception:\n",
    "                docs_by_type = []\n",
    "    except Exception:\n",
    "        # DB indisponible, ignorer proprement\n",
    "        SKIP_DB = True\n",
    "\n",
    "# Rendu console\n",
    "print(\"\\n=== Inventaire fichiers data/raw ===\")\n",
    "for k, v in paths.items():\n",
    "    print(f\"- {k:14s}: {len(v)} fichier(s)\")\n",
    "\n",
    "print(\"\\n=== Comptes en base ===\")\n",
    "for k, v in db_counts.items():\n",
    "    print(f\"- {k:10s}: {v if v is not None else 'N/A'}\")\n",
    "\n",
    "if docs_by_type:\n",
    "    print(\"\\n=== Documents par type_donnee ===\")\n",
    "    for row in docs_by_type:\n",
    "        print(f\"- {row[0] or 'Inconnu'}: {row[1]}\")\n",
    "\n",
    "# √âcriture rapport Markdown\n",
    "report_path = PROJECT_ROOT / \"docs\" / \"SOURCES_INVENTAIRE_E1.md\"\n",
    "report_lines = []\n",
    "report_lines.append(\"# üßæ Inventaire E1 ‚Äî Preuves de collecte et d'insertion\\n\")\n",
    "report_lines.append(f\"G√©n√©r√© le: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%SZ')} UTC\\n\\n\")\n",
    "report_lines.append(\"## R√©sum√© fichiers pr√©sents (data/raw)\\n\")\n",
    "for label, files in paths.items():\n",
    "    report_lines.append(f\"- **{label}**: {len(files)} fichier(s)\")\n",
    "    for p in sorted(files)[:5]:\n",
    "        report_lines.append(f\"  - {p.as_posix()}\")\n",
    "    if len(files) > 5:\n",
    "        report_lines.append(f\"  - (+{len(files)-5} autres)\\n\")\n",
    "\n",
    "report_lines.append(\"\\n## Comptes en base (PostgreSQL)\\n\")\n",
    "for k, v in db_counts.items():\n",
    "    report_lines.append(f\"- **{k}**: {v if v is not None else 'N/A'}\")\n",
    "\n",
    "if docs_by_type:\n",
    "    report_lines.append(\"\\n### Documents par type_donnee\\n\")\n",
    "    for row in docs_by_type:\n",
    "        report_lines.append(f\"- {row[0] or 'Inconnu'}: {row[1]}\")\n",
    "\n",
    "report_path.write_text(\"\\n\".join(report_lines), encoding=\"utf-8\")\n",
    "print(f\"\\n‚úÖ Rapport inventaire √©crit: {report_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc2a035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import os\n",
    "import subprocess\n",
    "from datetime import UTC, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == \"notebooks\" else NOTEBOOK_DIR\n",
    "load_dotenv(PROJECT_ROOT / \".env\")\n",
    "\n",
    "PG_HOST = os.getenv(\"POSTGRES_HOST\", \"localhost\")\n",
    "PG_PORT = int(os.getenv(\"POSTGRES_PORT\", \"5432\"))\n",
    "PG_DB = os.getenv(\"POSTGRES_DB\", \"datasens\")\n",
    "PG_USER = os.getenv(\"POSTGRES_USER\", \"ds_user\")\n",
    "PG_PASS = os.getenv(\"POSTGRES_PASS\", \"ds_pass\")\n",
    "\n",
    "PG_URL = f\"postgresql+psycopg2://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{PG_DB}\"\n",
    "engine = create_engine(PG_URL, future=True)\n",
    "\n",
    "print(\"‚úÖ Configuration charg√©e\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc869f81",
   "metadata": {},
   "source": [
    "## üìä Bilan E1 : Ce qui est fait / √† faire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed22883",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä BILAN E1\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    stats = {\n",
    "        \"tables\": conn.execute(text(\"SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public'\")).scalar(),\n",
    "        \"documents\": conn.execute(text(\"SELECT COUNT(*) FROM document\")).scalar(),\n",
    "        \"flux\": conn.execute(text(\"SELECT COUNT(*) FROM flux\")).scalar(),\n",
    "        \"sources\": conn.execute(text(\"SELECT COUNT(*) FROM source\")).scalar(),\n",
    "        \"meteo\": conn.execute(text(\"SELECT COUNT(*) FROM meteo\")).scalar(),\n",
    "        \"evenements\": conn.execute(text(\"SELECT COUNT(*) FROM evenement\")).scalar(),\n",
    "    }\n",
    "\n",
    "print(\"\\n‚úÖ R√©alis√© :\")\n",
    "print(f\"   ‚Ä¢ {stats['tables']} tables PostgreSQL cr√©√©es (sch√©ma Merise)\")\n",
    "print(f\"   ‚Ä¢ {stats['sources']} sources configur√©es\")\n",
    "print(f\"   ‚Ä¢ {stats['flux']} flux de collecte\")\n",
    "print(f\"   ‚Ä¢ {stats['documents']} documents collect√©s\")\n",
    "print(f\"   ‚Ä¢ {stats['meteo']} relev√©s m√©t√©o\")\n",
    "print(f\"   ‚Ä¢ {stats['evenements']} √©v√©nements\")\n",
    "print(\"\\n‚úÖ 5 types de sources ing√©r√©es :\")\n",
    "print(\"   1. Fichier plat CSV (Kaggle)\")\n",
    "print(\"   2. Base de donn√©es (SQLite ‚Üí Postgres)\")\n",
    "print(\"   3. API (OpenWeatherMap)\")\n",
    "print(\"   4. Web Scraping (MonAvisCitoyen)\")\n",
    "print(\"   5. Big Data (GDELT GKG)\")\n",
    "print(\"\\nüìã √Ä faire ensuite (E2/E3) :\")\n",
    "print(\"   ‚Ä¢ Enrichissement IA (NLP, sentiment analysis)\")\n",
    "print(\"   ‚Ä¢ Dashboard Power BI\")\n",
    "print(\"   ‚Ä¢ Orchestration Prefect/Airflow\")\n",
    "print(\"   ‚Ä¢ Tests automatis√©s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c78b3b",
   "metadata": {},
   "source": [
    "## üíæ Export DDL : Sauvegarde du sch√©ma SQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade89636",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üíæ Export DDL PostgreSQL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Export du sch√©ma complet\n",
    "with engine.connect() as conn:\n",
    "    schema_query = \"\"\"\n",
    "    SELECT\n",
    "        'CREATE TABLE ' || table_name || ' (' || E'\\\\n' ||\n",
    "        string_agg(\n",
    "            column_name || ' ' ||\n",
    "            CASE\n",
    "                WHEN data_type = 'integer' THEN 'INTEGER'\n",
    "                WHEN data_type = 'bigint' THEN 'BIGINT'\n",
    "                WHEN data_type = 'text' THEN 'TEXT'\n",
    "                WHEN data_type = 'character varying' THEN 'VARCHAR(' || character_maximum_length || ')'\n",
    "                WHEN data_type = 'timestamp without time zone' THEN 'TIMESTAMP'\n",
    "                WHEN data_type = 'real' THEN 'FLOAT'\n",
    "                ELSE data_type\n",
    "            END ||\n",
    "            CASE WHEN is_nullable = 'NO' THEN ' NOT NULL' ELSE '' END,\n",
    "            ',' || E'\\\\n    '\n",
    "            ORDER BY ordinal_position\n",
    "        ) || E'\\\\n);'\n",
    "        as ddl\n",
    "    FROM information_schema.columns\n",
    "    WHERE table_schema = 'public'\n",
    "    GROUP BY table_name;\n",
    "    \"\"\"\n",
    "\n",
    "    # Solution simplifi√©e : utiliser pg_dump ou exporter manuellement\n",
    "    print(\"üìù G√©n√©ration du sch√©ma SQL...\")\n",
    "\n",
    "    # Cr√©er le dossier docs s'il n'existe pas\n",
    "    docs_dir = PROJECT_ROOT / \"docs\"\n",
    "    docs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Export simplifi√© (pour un export complet, utiliser pg_dump)\n",
    "    schema_export = f\"\"\"\n",
    "-- DataSens E1 - Sch√©ma PostgreSQL\n",
    "-- Export g√©n√©r√© le {datetime.now(UTC).isoformat()}\n",
    "-- 18 tables Merise\n",
    "\n",
    "-- Note: Pour un export complet, utiliser:\n",
    "-- pg_dump -h {PG_HOST} -U {PG_USER} -d {PG_DB} --schema-only > docs/e1_schema.sql\n",
    "\"\"\"\n",
    "\n",
    "    schema_file = docs_dir / \"e1_schema.sql\"\n",
    "    schema_file.write_text(schema_export, encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"‚úÖ Sch√©ma export√© : {schema_file}\")\n",
    "    print(\"   üí° Pour un export complet, ex√©cutez:\")\n",
    "    print(f\"      pg_dump -h {PG_HOST} -U {PG_USER} -d {PG_DB} --schema-only > docs/e1_schema.sql\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb68b15a",
   "metadata": {},
   "source": [
    "## üì§ Export CSV : Snapshots des donn√©es (data/gold/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8658df06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì§ Export CSV - Snapshots data/gold/\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "gold_dir = PROJECT_ROOT / \"data\" / \"gold\"\n",
    "gold_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now(UTC).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Exporter quelques tables principales\n",
    "tables_to_export = [\"document\", \"source\", \"flux\", \"territoire\", \"meteo\"]\n",
    "\n",
    "exported = []\n",
    "for table in tables_to_export:\n",
    "    try:\n",
    "        df = pd.read_sql(f\"SELECT * FROM {table} LIMIT 1000\", engine)  # Limite pour d√©mo\n",
    "        if len(df) > 0:\n",
    "            csv_path = gold_dir / f\"{table}_{timestamp}.csv\"\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            exported.append(f\"   ‚úÖ {table}: {len(df)} lignes ‚Üí {csv_path.name}\")\n",
    "    except Exception as e:\n",
    "        exported.append(f\"   ‚ö†Ô∏è {table}: Erreur - {e}\")\n",
    "\n",
    "print(\"\\nüìä Exports CSV :\")\n",
    "for item in exported:\n",
    "    print(item)\n",
    "\n",
    "print(\"\\n‚úÖ Snapshots sauvegard√©s dans data/gold/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b309c109",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Cr√©ation du tag Git : E1_REAL_YYYYMMDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfb657b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üè∑Ô∏è Cr√©ation tag Git\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "tag_name = f\"E1_REAL_{datetime.now(UTC).strftime('%Y%m%d')}\"\n",
    "\n",
    "git_dir = PROJECT_ROOT / \".git\"\n",
    "if git_dir.exists():\n",
    "    try:\n",
    "        # V√©rifier si le tag existe d√©j√†\n",
    "        result = subprocess.run(\n",
    "            [\"git\", \"tag\", \"-l\", tag_name],\n",
    "            check=False, cwd=PROJECT_ROOT,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "\n",
    "        if tag_name in result.stdout:\n",
    "            print(f\"‚ö†Ô∏è Tag {tag_name} existe d√©j√†\")\n",
    "        else:\n",
    "            # Cr√©er le tag\n",
    "            subprocess.run(\n",
    "                [\"git\", \"tag\", \"-a\", tag_name, \"-m\", f\"DataSens E1 complet - {tag_name}\"],\n",
    "                cwd=PROJECT_ROOT,\n",
    "                check=True\n",
    "            )\n",
    "            print(f\"‚úÖ Tag Git cr√©√© : {tag_name}\")\n",
    "            print(\"   üí° Pour pousser le tag: git push origin {tag_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur cr√©ation tag : {e}\")\n",
    "        print(f\"   üí° Cr√©ation manuelle: git tag -a {tag_name} -m 'DataSens E1'\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è D√©p√¥t Git non initialis√©\")\n",
    "    print(f\"   üí° Tag sugg√©r√©: {tag_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c23aea",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è Roadmap E2/E3\n",
    "\n",
    "Planification des prochaines √©tapes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b92303",
   "metadata": {},
   "source": [
    "### üìã E2 - Enrichissement IA\n",
    "\n",
    "- **Annotation automatique** : Sentiment analysis (FlauBERT, CamemBERT)\n",
    "- **Extraction entit√©s nomm√©es** : spaCy NER (personnes, organisations, lieux)\n",
    "- **Embeddings vectoriels** : sentence-transformers pour recherche s√©mantique\n",
    "- **Classification th√©matique** : ML multi-labels (scikit-learn)\n",
    "- **Tables √† cr√©er** : `annotation`, `emotion`, `annotation_emotion`, `entite_nommee`\n",
    "\n",
    "### üìä E3 - Production & Visualisation\n",
    "\n",
    "- **API REST** : FastAPI pour exposition des donn√©es\n",
    "- **Dashboard** : Power BI ou Streamlit pour visualisations interactives\n",
    "- **Orchestration** : Prefect/Airflow pour collecte automatique\n",
    "- **Monitoring** : Grafana + Prometheus pour m√©triques\n",
    "- **Tests** : pytest pour validation automatique\n",
    "- **Documentation** : API docs (Swagger/OpenAPI)\n",
    "\n",
    "### ‚úÖ E1 Valid√©\n",
    "\n",
    "- ‚úÖ Mod√©lisation Merise (MCD ‚Üí MLD ‚Üí MPD)\n",
    "- ‚úÖ 18 tables PostgreSQL cr√©√©es\n",
    "- ‚úÖ CRUD complet test√©\n",
    "- ‚úÖ 5 types de sources ing√©r√©es\n",
    "- ‚úÖ Tra√ßabilit√© (flux, manifests, versioning Git)\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ F√©licitations ! E1 est termin√© !**\n",
    "\n",
    "**Prochaines √©tapes** : Commencer E2 avec l'enrichissement IA\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
