{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdd5d970",
   "metadata": {},
   "source": [
    "# 📸 DataSens E1_v3 — Notebook 5 : Snapshot et README\n",
    "\n",
    "**🎯 Objectif** : Créer un bilan E1_v3, exporter le DDL/CSV, créer un tag Git et définir la roadmap E2/E3\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 Contenu de ce notebook\n",
    "\n",
    "1. **Bilan E1_v3** : Dataset préparé pour E2 (annotation simple)\n",
    "2. **Export DDL** : Sauvegarde du schéma SQL dans `docs/e1_schema.sql`\n",
    "3. **Export CSV** : Snapshots du dataset annoté simple dans `data/gold/`\n",
    "4. **Export Dataset IA** : Export Parquet/CSV structuré pour enrichissement IA (E2)\n",
    "5. **Vérification Tables** : Vérification que toutes les tables sont remplies (thèmes, etc.)\n",
    "6. **Tag Git** : Création du tag `E1_REAL_YYYYMMDD`\n",
    "7. **Roadmap E2/E3** : Annotation IA avancée (CamemBERT, FlauBERT) dans E2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db577bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 Inventaire E1 — Sources et traces de collecte (DataLake + PostgreSQL)\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == \"notebooks\" else NOTEBOOK_DIR\n",
    "load_dotenv(PROJECT_ROOT / \".env\")\n",
    "\n",
    "# Fail-fast DB URL (3s timeout)\n",
    "PG_URL = (\n",
    "    f\"postgresql+psycopg2://{os.getenv('POSTGRES_USER','ds_user')}:{os.getenv('POSTGRES_PASS','ds_pass')}@\"\n",
    "    f\"{os.getenv('POSTGRES_HOST','localhost')}:{int(os.getenv('POSTGRES_PORT','5432'))}/\"\n",
    "    f\"{os.getenv('POSTGRES_DB','datasens')}?connect_timeout=3\"\n",
    ")\n",
    "\n",
    "# Option pour ignorer DB si instable\n",
    "SKIP_DB = os.getenv(\"DS_SKIP_DB\", \"0\") == \"1\"\n",
    "engine = create_engine(PG_URL, future=True, pool_pre_ping=True)\n",
    "\n",
    "RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "\n",
    "# Limiter le scan (défensif)\n",
    "def list_top(path: Path, pattern: str, limit: int = 50):\n",
    "    try:\n",
    "        files = list(path.glob(pattern)) if path.exists() else []\n",
    "        return files[:limit]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "paths = {\n",
    "    \"kaggle_csv\": list_top(RAW / \"kaggle\", \"*.csv\"),\n",
    "    \"owm_api\": list_top(RAW / \"api\" / \"owm\", \"*.csv\"),\n",
    "    \"rss_multi\": list_top(RAW / \"rss\", \"*.csv\"),\n",
    "    \"scraping_multi\": list_top(RAW / \"scraping\" / \"multi\", \"*.csv\"),\n",
    "    \"gdelt_gkg\": list_top(RAW / \"gdelt\", \"*.zip\"),\n",
    "    \"manifests\": list_top(RAW / \"manifests\", \"*.json\"),\n",
    "}\n",
    "\n",
    "# Comptes DB (si tables présentes)\n",
    "db_counts = {\"document\": None, \"flux\": None, \"source\": None, \"meteo\": None, \"evenement\": None}\n",
    "docs_by_type = []\n",
    "\n",
    "if not SKIP_DB:\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            # Timeout de requête 3s\n",
    "            try:\n",
    "                conn.exec_driver_sql(\"SET LOCAL statement_timeout = 3000\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # Corrigé avec préfixes tXX_\n",
    "            for table, tname in [(\"document\", \"t04_document\"), (\"flux\", \"t03_flux\"), (\"source\", \"t02_source\"), (\"meteo\", \"t19_meteo\"), (\"evenement\", \"t25_evenement\")]:\n",
    "                try:\n",
    "                    db_counts[table] = conn.execute(text(f\"SELECT COUNT(*) FROM {tname}\")).scalar()\n",
    "                except Exception:\n",
    "                    db_counts[table] = None\n",
    "\n",
    "            try:\n",
    "                docs_by_type = conn.execute(text(\n",
    "                    \"\"\"\n",
    "                    SELECT td.libelle AS type_donnee, COUNT(d.id_doc) AS nb_docs\n",
    "                    FROM t04_document d\n",
    "                    LEFT JOIN t03_flux f ON d.id_flux = f.id_flux\n",
    "                    LEFT JOIN t02_source s ON f.id_source = s.id_source\n",
    "                    LEFT JOIN t01_type_donnee td ON s.id_type_donnee = td.id_type_donnee\n",
    "                    GROUP BY td.libelle\n",
    "                    ORDER BY nb_docs DESC NULLS LAST\n",
    "                    \"\"\"\n",
    "                )).fetchall()\n",
    "            except Exception:\n",
    "                docs_by_type = []\n",
    "    except Exception:\n",
    "        # DB indisponible, ignorer proprement\n",
    "        SKIP_DB = True\n",
    "\n",
    "# Rendu console\n",
    "print(\"\\n=== Inventaire fichiers data/raw ===\")\n",
    "for k, v in paths.items():\n",
    "    print(f\"- {k:14s}: {len(v)} fichier(s)\")\n",
    "\n",
    "print(\"\\n=== Comptes en base ===\")\n",
    "for k, v in db_counts.items():\n",
    "    print(f\"- {k:10s}: {v if v is not None else 'N/A'}\")\n",
    "\n",
    "if docs_by_type:\n",
    "    print(\"\\n=== Documents par type_donnee ===\")\n",
    "    for row in docs_by_type:\n",
    "        print(f\"- {row[0] or 'Inconnu'}: {row[1]}\")\n",
    "\n",
    "# Écriture rapport Markdown\n",
    "report_path = PROJECT_ROOT / \"docs\" / \"SOURCES_INVENTAIRE_E1.md\"\n",
    "report_lines = []\n",
    "report_lines.append(\"# 🧾 Inventaire E1 — Preuves de collecte et d'insertion\\n\")\n",
    "report_lines.append(f\"Généré le: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%SZ')} UTC\\n\\n\")\n",
    "report_lines.append(\"## Résumé fichiers présents (data/raw)\\n\")\n",
    "for label, files in paths.items():\n",
    "    report_lines.append(f\"- **{label}**: {len(files)} fichier(s)\")\n",
    "    for p in sorted(files)[:5]:\n",
    "        report_lines.append(f\"  - {p.as_posix()}\")\n",
    "    if len(files) > 5:\n",
    "        report_lines.append(f\"  - (+{len(files)-5} autres)\\n\")\n",
    "\n",
    "report_lines.append(\"\\n## Comptes en base (PostgreSQL)\\n\")\n",
    "for k, v in db_counts.items():\n",
    "    report_lines.append(f\"- **{k}**: {v if v is not None else 'N/A'}\")\n",
    "\n",
    "if docs_by_type:\n",
    "    report_lines.append(\"\\n### Documents par type_donnee\\n\")\n",
    "    for row in docs_by_type:\n",
    "        report_lines.append(f\"- {row[0] or 'Inconnu'}: {row[1]}\")\n",
    "\n",
    "report_path.write_text(\"\\n\".join(report_lines), encoding=\"utf-8\")\n",
    "print(f\"\\n✅ Rapport inventaire écrit: {report_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc2a035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import os\n",
    "import subprocess\n",
    "from datetime import UTC, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == \"notebooks\" else NOTEBOOK_DIR\n",
    "load_dotenv(PROJECT_ROOT / \".env\")\n",
    "\n",
    "PG_HOST = os.getenv(\"POSTGRES_HOST\", \"localhost\")\n",
    "PG_PORT = int(os.getenv(\"POSTGRES_PORT\", \"5432\"))\n",
    "PG_DB = os.getenv(\"POSTGRES_DB\", \"datasens\")\n",
    "PG_USER = os.getenv(\"POSTGRES_USER\", \"ds_user\")\n",
    "PG_PASS = os.getenv(\"POSTGRES_PASS\", \"ds_pass\")\n",
    "\n",
    "PG_URL = f\"postgresql+psycopg2://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{PG_DB}\"\n",
    "engine = create_engine(PG_URL, future=True)\n",
    "\n",
    "print(\"✅ Configuration chargée\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc869f81",
   "metadata": {},
   "source": [
    "## 📊 Bilan E1 : Ce qui est fait / à faire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed22883",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📊 BILAN E1_V3 - Dataset Final Annoté\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # Statistiques avec préfixes tXX_ corrigés\n",
    "    stats = {\n",
    "        \"tables\": conn.execute(text(\"SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public' OR table_schema = 'datasens'\")).scalar(),\n",
    "        \"documents\": conn.execute(text(\"SELECT COUNT(*) FROM t04_document\")).scalar(),\n",
    "        \"flux\": conn.execute(text(\"SELECT COUNT(*) FROM t03_flux\")).scalar(),\n",
    "        \"sources\": conn.execute(text(\"SELECT COUNT(*) FROM t02_source\")).scalar(),\n",
    "        \"meteo\": conn.execute(text(\"SELECT COUNT(*) FROM t19_meteo\")).scalar(),\n",
    "        \"evenements\": conn.execute(text(\"SELECT COUNT(*) FROM t25_evenement\")).scalar(),\n",
    "        \"themes\": conn.execute(text(\"SELECT COUNT(*) FROM t24_theme\")).scalar(),\n",
    "    }\n",
    "    \n",
    "    # Statistiques par type de donnée\n",
    "    df_final = pd.read_sql_query(\"\"\"\n",
    "        SELECT \n",
    "            td.libelle AS type_donnee,\n",
    "            COUNT(DISTINCT s.id_source) AS nb_sources,\n",
    "            COUNT(DISTINCT d.id_doc) AS nb_documents,\n",
    "            COUNT(DISTINCT f.id_flux) AS nb_flux\n",
    "        FROM t01_type_donnee td\n",
    "        LEFT JOIN t02_source s ON td.id_type_donnee = s.id_type_donnee\n",
    "        LEFT JOIN t03_flux f ON s.id_source = f.id_source\n",
    "        LEFT JOIN t04_document d ON f.id_flux = d.id_flux\n",
    "        GROUP BY td.libelle\n",
    "        ORDER BY nb_documents DESC\n",
    "    \"\"\", conn)\n",
    "\n",
    "print(\"\\n✅ Réalisé E1_v3 :\")\n",
    "print(f\"   • {stats['tables']} tables PostgreSQL créées (architecture complète 36/37 tables)\")\n",
    "print(f\"   • {stats['sources']} sources configurées\")\n",
    "print(f\"   • {stats['flux']} flux de collecte\")\n",
    "print(f\"   • {stats['documents']:,} documents collectés et nettoyés\")\n",
    "print(f\"   • {stats['meteo']} relevés météo\")\n",
    "print(f\"   • {stats['evenements']} événements\")\n",
    "print(f\"   • {stats['themes']} thèmes identifiés\")\n",
    "\n",
    "# Vérification que toutes les tables de référentiels sont remplies\n",
    "print(\"\\n📊 VÉRIFICATION REMPLISSAGE TABLES (E1_v3)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # Vérification thèmes et catégories\n",
    "    nb_theme_cat = conn.execute(text(\"SELECT COUNT(*) FROM t23_theme_category\")).scalar()\n",
    "    nb_themes = conn.execute(text(\"SELECT COUNT(*) FROM t24_theme\")).scalar()\n",
    "    nb_type_donnee = conn.execute(text(\"SELECT COUNT(*) FROM t01_type_donnee\")).scalar()\n",
    "    nb_territoire = conn.execute(text(\"SELECT COUNT(*) FROM t17_territoire\")).scalar()\n",
    "    nb_indicateur = conn.execute(text(\"SELECT COUNT(*) FROM t22_indicateur\")).scalar()\n",
    "    \n",
    "    df_tables = pd.DataFrame({\n",
    "        \"Table\": [\"t23_theme_category\", \"t24_theme\", \"t01_type_donnee\", \"t17_territoire\", \"t22_indicateur\"],\n",
    "        \"Nb enregistrements\": [nb_theme_cat, nb_themes, nb_type_donnee, nb_territoire, nb_indicateur],\n",
    "        \"Statut\": [\n",
    "            \"✅ OK\" if nb_theme_cat >= 12 else \"⚠️ Incomplet\",\n",
    "            \"✅ OK\" if nb_themes >= 12 else \"⚠️ Incomplet\",\n",
    "            \"✅ OK\" if nb_type_donnee >= 5 else \"⚠️ Incomplet\",\n",
    "            \"✅ OK\" if nb_territoire > 0 else \"⚠️ Vide\",\n",
    "            \"✅ OK\" if nb_indicateur > 0 else \"ℹ️ Optionnel\"\n",
    "        ]\n",
    "    })\n",
    "    display(df_tables)\n",
    "    \n",
    "    # Détails des thèmes\n",
    "    if nb_themes > 0:\n",
    "        df_themes_detail = pd.read_sql_query(\"\"\"\n",
    "            SELECT \n",
    "                tc.libelle AS categorie,\n",
    "                COUNT(t.id_theme) AS nb_themes,\n",
    "                STRING_AGG(t.libelle, ', ' ORDER BY t.libelle) AS themes\n",
    "            FROM t23_theme_category tc\n",
    "            LEFT JOIN t24_theme t ON tc.id_theme_cat = t.id_theme_cat\n",
    "            GROUP BY tc.id_theme_cat, tc.libelle\n",
    "            ORDER BY tc.id_theme_cat\n",
    "        \"\"\", conn)\n",
    "        print(\"\\n📋 Détail thèmes par catégorie :\")\n",
    "        display(df_themes_detail)\n",
    "    \n",
    "    if nb_theme_cat < 12 or nb_themes < 12:\n",
    "        print(\"\\n⚠️ ATTENTION : Tous les thèmes ne sont pas encore remplis\")\n",
    "        print(\"   💡 Réexécutez le notebook 02_schema_create.ipynb pour compléter\")\n",
    "\n",
    "# Visualisations dataset final\n",
    "print(\"\\n📊 VISUALISATIONS DATASET FINAL ANNOTÉ (E1_V3)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Graphique 1 : Répartition par type de donnée\n",
    "if len(df_final) > 0:\n",
    "    print(\"\\n📋 Répartition par type de donnée :\")\n",
    "    display(df_final)\n",
    "    \n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    bars = plt.bar(df_final[\"type_donnee\"], df_final[\"nb_documents\"], color=plt.cm.Set2(range(len(df_final))))\n",
    "    for bar, value in zip(bars, df_final[\"nb_documents\"]):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(df_final[\"nb_documents\"]) * 0.02,\n",
    "                f\"{int(value):,}\", ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    plt.title(\"📊 Documents par type de donnée (Dataset Final)\", fontsize=12, fontweight='bold')\n",
    "    plt.ylabel(\"Nombre de documents\", fontsize=11)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.pie(df_final[\"nb_documents\"], labels=df_final[\"type_donnee\"], autopct='%1.1f%%', startangle=90)\n",
    "    plt.title(\"📊 Répartition documents par type (%)\", fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    bars = plt.bar(df_final[\"type_donnee\"], df_final[\"nb_sources\"], color=plt.cm.Pastel1(range(len(df_final))))\n",
    "    for bar, value in zip(bars, df_final[\"nb_sources\"]):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(df_final[\"nb_sources\"]) * 0.02,\n",
    "                str(int(value)), ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    plt.title(\"📊 Sources par type de donnée\", fontsize=12, fontweight='bold')\n",
    "    plt.ylabel(\"Nombre de sources\", fontsize=11)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    bars = plt.bar(df_final[\"type_donnee\"], df_final[\"nb_flux\"], color='#4ECDC4')\n",
    "    for bar, value in zip(bars, df_final[\"nb_flux\"]):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(df_final[\"nb_flux\"]) * 0.02,\n",
    "                str(int(value)), ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    plt.title(\"📊 Flux par type de donnée\", fontsize=12, fontweight='bold')\n",
    "    plt.ylabel(\"Nombre de flux\", fontsize=11)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Vue d'ensemble des documents\n",
    "print(\"\\n📋 Aperçu dataset final (50 premiers documents) :\")\n",
    "df_docs_final = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        d.id_doc,\n",
    "        LEFT(d.titre, 60) AS titre,\n",
    "        LEFT(d.texte, 100) AS texte_apercu,\n",
    "        d.langue,\n",
    "        d.date_publication,\n",
    "        s.nom AS source,\n",
    "        td.libelle AS type_donnee\n",
    "    FROM t04_document d\n",
    "    JOIN t03_flux f ON d.id_flux = f.id_flux\n",
    "    JOIN t02_source s ON f.id_source = s.id_source\n",
    "    JOIN t01_type_donnee td ON s.id_type_donnee = td.id_type_donnee\n",
    "    ORDER BY d.id_doc DESC\n",
    "    LIMIT 50\n",
    "\"\"\", engine)\n",
    "display(df_docs_final)\n",
    "\n",
    "print(\"\\n✅ 6 types de sources ingérées E1_v3 :\")\n",
    "print(\"   1. Fichier plat CSV (Kaggle)\")\n",
    "print(\"   2. API OpenWeatherMap (météo)\")\n",
    "print(\"   3. Flux RSS Multi-Sources (Franceinfo, 20 Minutes, Le Monde)\")\n",
    "print(\"   4. NewsAPI (optionnel)\")\n",
    "print(\"   5. Web Scraping Multi-Sources (Reddit, YouTube, Vie-publique, data.gouv)\")\n",
    "print(\"   6. Big Data GDELT GKG\")\n",
    "print(\"\\n📋 E1_v3 : Dataset préparé pour E2\")\n",
    "print(\"   ✅ Annotation simple : nettoyage, déduplication, QA de base\")\n",
    "print(\"   ✅ Structure prête pour enrichissement IA (E2)\")\n",
    "print(\"\\n📋 À faire ensuite (E2/E3) :\")\n",
    "print(\"   • E2 : Enrichissement IA (CamemBERT, FlauBERT)\")\n",
    "print(\"   • E2 : Annotation sentiment, NER, keywords (IA avancée)\")\n",
    "print(\"   • E3 : Dashboard Power BI\")\n",
    "print(\"   • E3 : Orchestration Prefect/Airflow\")\n",
    "print(\"   • E3 : Tests automatisés\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c78b3b",
   "metadata": {},
   "source": [
    "## 💾 Export DDL : Sauvegarde du schéma SQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade89636",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"💾 Export DDL PostgreSQL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Export du schéma complet\n",
    "with engine.connect() as conn:\n",
    "    schema_query = \"\"\"\n",
    "    SELECT\n",
    "        'CREATE TABLE ' || table_name || ' (' || E'\\\\n' ||\n",
    "        string_agg(\n",
    "            column_name || ' ' ||\n",
    "            CASE\n",
    "                WHEN data_type = 'integer' THEN 'INTEGER'\n",
    "                WHEN data_type = 'bigint' THEN 'BIGINT'\n",
    "                WHEN data_type = 'text' THEN 'TEXT'\n",
    "                WHEN data_type = 'character varying' THEN 'VARCHAR(' || character_maximum_length || ')'\n",
    "                WHEN data_type = 'timestamp without time zone' THEN 'TIMESTAMP'\n",
    "                WHEN data_type = 'real' THEN 'FLOAT'\n",
    "                ELSE data_type\n",
    "            END ||\n",
    "            CASE WHEN is_nullable = 'NO' THEN ' NOT NULL' ELSE '' END,\n",
    "            ',' || E'\\\\n    '\n",
    "            ORDER BY ordinal_position\n",
    "        ) || E'\\\\n);'\n",
    "        as ddl\n",
    "    FROM information_schema.columns\n",
    "    WHERE table_schema = 'public'\n",
    "    GROUP BY table_name;\n",
    "    \"\"\"\n",
    "\n",
    "    # Solution simplifiée : utiliser pg_dump ou exporter manuellement\n",
    "    print(\"📝 Génération du schéma SQL...\")\n",
    "\n",
    "    # Créer le dossier docs s'il n'existe pas\n",
    "    docs_dir = PROJECT_ROOT / \"docs\"\n",
    "    docs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Export simplifié (pour un export complet, utiliser pg_dump)\n",
    "    schema_export = f\"\"\"\n",
    "-- DataSens E1 - Schéma PostgreSQL\n",
    "-- Export généré le {datetime.now(UTC).isoformat()}\n",
    "-- 18 tables Merise\n",
    "\n",
    "-- Note: Pour un export complet, utiliser:\n",
    "-- pg_dump -h {PG_HOST} -U {PG_USER} -d {PG_DB} --schema-only > docs/e1_schema.sql\n",
    "\"\"\"\n",
    "\n",
    "    schema_file = docs_dir / \"e1_schema.sql\"\n",
    "    schema_file.write_text(schema_export, encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"✅ Schéma exporté : {schema_file}\")\n",
    "    print(\"   💡 Pour un export complet, exécutez:\")\n",
    "    print(f\"      pg_dump -h {PG_HOST} -U {PG_USER} -d {PG_DB} --schema-only > docs/e1_schema.sql\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb68b15a",
   "metadata": {},
   "source": [
    "## 📤 Export CSV : Snapshots des données (data/gold/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8658df06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📤 Export CSV - Snapshots Dataset Final Annoté (data/gold/)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "gold_dir = PROJECT_ROOT / \"data\" / \"gold\"\n",
    "gold_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now(UTC).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Exporter tables principales (corrigé avec préfixes tXX_)\n",
    "tables_to_export = [\n",
    "    (\"document\", \"t04_document\"),\n",
    "    (\"source\", \"t02_source\"),\n",
    "    (\"flux\", \"t03_flux\"),\n",
    "    (\"territoire\", \"t17_territoire\"),\n",
    "    (\"meteo\", \"t19_meteo\"),\n",
    "    (\"evenement\", \"t25_evenement\"),\n",
    "    (\"theme\", \"t24_theme\")\n",
    "]\n",
    "\n",
    "exported = []\n",
    "for table_name, table_full in tables_to_export:\n",
    "    try:\n",
    "        df = pd.read_sql_query(f\"SELECT * FROM {table_full} LIMIT 1000\", engine)  # Limite pour démo\n",
    "        if len(df) > 0:\n",
    "            csv_path = gold_dir / f\"{table_name}_{timestamp}.csv\"\n",
    "            df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "            exported.append(f\"   ✅ {table_name}: {len(df)} lignes → {csv_path.name}\")\n",
    "    except Exception as e:\n",
    "        exported.append(f\"   ⚠️ {table_name}: Erreur - {str(e)[:80]}\")\n",
    "\n",
    "print(\"\\n📊 Exports CSV dataset final :\")\n",
    "for item in exported:\n",
    "    print(item)\n",
    "\n",
    "# Visualisation des exports\n",
    "if len(exported) > 0:\n",
    "    print(\"\\n📊 Visualisation des snapshots exportés :\")\n",
    "    export_data = []\n",
    "    for item in exported:\n",
    "        if \"✅\" in item:\n",
    "            parts = item.split(\": \")\n",
    "            table = parts[0].replace(\"   ✅ \", \"\")\n",
    "            count = parts[1].split(\" lignes\")[0]\n",
    "            export_data.append({\"Table\": table, \"Lignes exportées\": int(count)})\n",
    "    \n",
    "    if export_data:\n",
    "        df_exports = pd.DataFrame(export_data)\n",
    "        display(df_exports)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bars = plt.bar(df_exports[\"Table\"], df_exports[\"Lignes exportées\"], color=plt.cm.Set3(range(len(df_exports))))\n",
    "        for bar, value in zip(bars, df_exports[\"Lignes exportées\"]):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(df_exports[\"Lignes exportées\"]) * 0.02,\n",
    "                    f\"{int(value):,}\", ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "        plt.title(\"📤 Snapshots exportés vers data/gold/ (Dataset Final)\", fontsize=12, fontweight='bold')\n",
    "        plt.ylabel(\"Nombre de lignes\", fontsize=11)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"\\n✅ Snapshots dataset final annoté sauvegardés dans data/gold/\")\n",
    "\n",
    "# ============================================================\n",
    "# EXPORT DATASET STRUCTURÉ POUR IA (Parquet)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"📦 EXPORT DATASET STRUCTURÉ POUR IA (Format Parquet)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "    from pathlib import Path\n",
    "    \n",
    "    print(\"\\n📊 Export dataset complet pour enrichissement IA (E2)...\")\n",
    "    \n",
    "    # Créer le dossier export si nécessaire\n",
    "    export_dir = PROJECT_ROOT / \"data\" / \"gold\" / \"dataset_ia\"\n",
    "    export_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Requête consolidée : Documents + métadonnées prêtes pour IA\n",
    "    dataset_query = \"\"\"\n",
    "        SELECT \n",
    "            d.id_doc,\n",
    "            d.titre,\n",
    "            d.texte,\n",
    "            d.langue,\n",
    "            d.date_publication,\n",
    "            d.hash_fingerprint,\n",
    "            s.nom AS source_nom,\n",
    "            td.libelle AS type_donnee,\n",
    "            f.date_collecte,\n",
    "            t.ville AS territoire,\n",
    "            -- Agrégation thèmes\n",
    "            STRING_AGG(DISTINCT th.libelle, '; ') AS themes,\n",
    "            -- Comptage annotations (si présentes)\n",
    "            (SELECT COUNT(*) FROM t05_annotation ann WHERE ann.id_doc = d.id_doc) AS nb_annotations\n",
    "        FROM t04_document d\n",
    "        LEFT JOIN t03_flux f ON d.id_flux = f.id_flux\n",
    "        LEFT JOIN t02_source s ON f.id_source = s.id_source\n",
    "        LEFT JOIN t01_type_donnee td ON s.id_type_donnee = td.id_type_donnee\n",
    "        LEFT JOIN t17_territoire t ON d.id_territoire = t.id_territoire\n",
    "        LEFT JOIN t26_document_theme dt ON d.id_doc = dt.id_doc\n",
    "        LEFT JOIN t24_theme th ON dt.id_theme = th.id_theme\n",
    "        GROUP BY d.id_doc, d.titre, d.texte, d.langue, d.date_publication, d.hash_fingerprint,\n",
    "                 s.nom, td.libelle, f.date_collecte, t.ville\n",
    "        ORDER BY d.date_publication DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    df_dataset_ia = pd.read_sql_query(dataset_query, engine)\n",
    "    \n",
    "    if len(df_dataset_ia) > 0:\n",
    "        # Export Parquet (format optimal pour IA)\n",
    "        parquet_path = export_dir / f\"datasens_dataset_ia_{timestamp}.parquet\"\n",
    "        df_dataset_ia.to_parquet(parquet_path, engine='pyarrow', compression='snappy', index=False)\n",
    "        \n",
    "        file_size_mb = parquet_path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"\\n✅ Dataset IA exporté :\")\n",
    "        print(f\"   📄 Fichier : {parquet_path.name}\")\n",
    "        print(f\"   📊 {len(df_dataset_ia):,} documents\")\n",
    "        print(f\"   💾 Taille : {file_size_mb:.2f} MB\")\n",
    "        print(f\"   📁 Chemin : {parquet_path}\")\n",
    "        \n",
    "        # Export CSV également (pour compatibilité)\n",
    "        csv_path = export_dir / f\"datasens_dataset_ia_{timestamp}.csv\"\n",
    "        df_dataset_ia.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "        csv_size_mb = csv_path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"\\n✅ Export CSV complémentaire :\")\n",
    "        print(f\"   📄 Fichier : {csv_path.name}\")\n",
    "        print(f\"   💾 Taille : {csv_size_mb:.2f} MB\")\n",
    "        \n",
    "        # Aperçu du dataset\n",
    "        print(\"\\n📋 Aperçu dataset IA (5 premiers documents) :\")\n",
    "        display(df_dataset_ia.head())\n",
    "        \n",
    "        # Statistiques par type de donnée\n",
    "        print(\"\\n📊 Statistiques dataset par type de donnée :\")\n",
    "        stats_type = df_dataset_ia.groupby('type_donnee').agg({\n",
    "            'id_doc': 'count',\n",
    "            'langue': lambda x: x.value_counts().to_dict()\n",
    "        }).rename(columns={'id_doc': 'nb_documents'})\n",
    "        display(stats_type)\n",
    "        \n",
    "    else:\n",
    "        print(\"⚠️ Aucun document à exporter\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"⚠️ PyArrow non installé - export Parquet impossible\")\n",
    "    print(\"   💡 Installez : pip install pyarrow\")\n",
    "    print(\"   ✅ Export CSV disponible ci-dessus\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Erreur export dataset IA : {str(e)[:100]}\")\n",
    "    print(\"   ✅ Export CSV disponible ci-dessus\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ EXPORT DATASET STRUCTURÉ TERMINÉ\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n📋 Fichiers disponibles pour téléchargement :\")\n",
    "print(f\"   • Parquet (recommandé) : data/gold/dataset_ia/datasens_dataset_ia_{timestamp}.parquet\")\n",
    "print(f\"   • CSV (compatibilité) : data/gold/dataset_ia/datasens_dataset_ia_{timestamp}.csv\")\n",
    "print(\"\\n🎯 Ce dataset est prêt pour enrichissement IA (E2) avec CamemBERT et FlauBERT\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b309c109",
   "metadata": {},
   "source": [
    "## 🏷️ Création du tag Git : E1_REAL_YYYYMMDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfb657b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🏷️ Création tag Git\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "tag_name = f\"E1_REAL_{datetime.now(UTC).strftime('%Y%m%d')}\"\n",
    "\n",
    "git_dir = PROJECT_ROOT / \".git\"\n",
    "if git_dir.exists():\n",
    "    try:\n",
    "        # Vérifier si le tag existe déjà\n",
    "        result = subprocess.run(\n",
    "            [\"git\", \"tag\", \"-l\", tag_name],\n",
    "            check=False, cwd=PROJECT_ROOT,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "\n",
    "        if tag_name in result.stdout:\n",
    "            print(f\"⚠️ Tag {tag_name} existe déjà\")\n",
    "        else:\n",
    "            # Créer le tag\n",
    "            subprocess.run(\n",
    "                [\"git\", \"tag\", \"-a\", tag_name, \"-m\", f\"DataSens E1 complet - {tag_name}\"],\n",
    "                cwd=PROJECT_ROOT,\n",
    "                check=True\n",
    "            )\n",
    "            print(f\"✅ Tag Git créé : {tag_name}\")\n",
    "            print(\"   💡 Pour pousser le tag: git push origin {tag_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erreur création tag : {e}\")\n",
    "        print(f\"   💡 Création manuelle: git tag -a {tag_name} -m 'DataSens E1'\")\n",
    "else:\n",
    "    print(\"⚠️ Dépôt Git non initialisé\")\n",
    "    print(f\"   💡 Tag suggéré: {tag_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c23aea",
   "metadata": {},
   "source": [
    "## 🗺️ Roadmap E2/E3\n",
    "\n",
    "Planification des prochaines étapes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b92303",
   "metadata": {},
   "source": [
    "### 📋 E2 - Enrichissement IA (CamemBERT, FlauBERT)\n",
    "\n",
    "**E1_v3 prépare le dataset** avec annotation simple (nettoyage, déduplication, QA de base).\n",
    "\n",
    "**E2 ajoutera l'annotation IA avancée** :\n",
    "- **Annotation automatique** : Sentiment analysis (FlauBERT, CamemBERT)\n",
    "- **Extraction entités nommées** : spaCy NER (personnes, organisations, lieux)\n",
    "- **Embeddings vectoriels** : sentence-transformers pour recherche sémantique\n",
    "- **Classification thématique** : ML multi-labels (scikit-learn)\n",
    "- **Tables à utiliser** : `t05_annotation`, `t08_emotion`, `t06_annotation_emotion` (déjà créées dans E1_v3)\n",
    "\n",
    "### 📊 E3 - Production & Visualisation\n",
    "\n",
    "- **API REST** : FastAPI pour exposition des données\n",
    "- **Dashboard** : Power BI ou Streamlit pour visualisations interactives\n",
    "- **Orchestration** : Prefect/Airflow pour collecte automatique\n",
    "- **Monitoring** : Grafana + Prometheus pour métriques\n",
    "- **Tests** : pytest pour validation automatique\n",
    "- **Documentation** : API docs (Swagger/OpenAPI)\n",
    "\n",
    "### ✅ E1_v3 Validé\n",
    "\n",
    "- ✅ Modélisation Merise (MCD → MLD → MPD) - 36/37 tables\n",
    "- ✅ Architecture PostgreSQL complète créée\n",
    "- ✅ CRUD complet testé\n",
    "- ✅ 6 types de sources ingérées\n",
    "- ✅ Annotation simple : nettoyage, déduplication, QA de base\n",
    "- ✅ Dataset préparé pour enrichissement IA (E2)\n",
    "- ✅ Traçabilité (flux, manifests, versioning Git)\n",
    "\n",
    "---\n",
    "\n",
    "**🎉 Félicitations ! E1_v3 est terminé !**\n",
    "\n",
    "**E1_v3** : Dataset nettoyé et annoté simplement, prêt pour E2  \n",
    "**Prochaines étapes** : E2 avec enrichissement IA avancée (CamemBERT, FlauBERT)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
