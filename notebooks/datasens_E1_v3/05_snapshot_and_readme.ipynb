{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdd5d970",
   "metadata": {},
   "source": [
    "# üì∏ DataSens E1_v3 ‚Äî Notebook 5 : Snapshot et README\n",
    "\n",
    "**üéØ Objectif** : Cr√©er un bilan E1_v3, exporter le DDL/CSV, cr√©er un tag Git et d√©finir la roadmap E2/E3\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Contenu de ce notebook\n",
    "\n",
    "1. **Bilan E1_v3** : Dataset pr√©par√© pour E2 (annotation simple)\n",
    "2. **Export DDL** : Sauvegarde du sch√©ma SQL dans `docs/e1_schema.sql`\n",
    "3. **Export CSV** : Snapshots du dataset annot√© simple dans `data/gold/`\n",
    "4. **Export Dataset IA** : Export Parquet/CSV structur√© pour enrichissement IA (E2)\n",
    "5. **V√©rification Tables** : V√©rification que toutes les tables sont remplies (th√®mes, etc.)\n",
    "6. **Tag Git** : Cr√©ation du tag `E1_REAL_YYYYMMDD`\n",
    "7. **Roadmap E2/E3** : Annotation IA avanc√©e (CamemBERT, FlauBERT) dans E2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db577bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Inventaire E1 ‚Äî Sources et traces de collecte (DataLake + PostgreSQL)\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == \"notebooks\" else NOTEBOOK_DIR\n",
    "load_dotenv(PROJECT_ROOT / \".env\")\n",
    "\n",
    "# Fail-fast DB URL (3s timeout)\n",
    "PG_URL = (\n",
    "    f\"postgresql+psycopg2://{os.getenv('POSTGRES_USER','ds_user')}:{os.getenv('POSTGRES_PASS','ds_pass')}@\"\n",
    "    f\"{os.getenv('POSTGRES_HOST','localhost')}:{int(os.getenv('POSTGRES_PORT','5432'))}/\"\n",
    "    f\"{os.getenv('POSTGRES_DB','datasens')}?connect_timeout=3\"\n",
    ")\n",
    "\n",
    "# Option pour ignorer DB si instable\n",
    "SKIP_DB = os.getenv(\"DS_SKIP_DB\", \"0\") == \"1\"\n",
    "engine = create_engine(PG_URL, future=True, pool_pre_ping=True)\n",
    "\n",
    "RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "\n",
    "# Limiter le scan (d√©fensif)\n",
    "def list_top(path: Path, pattern: str, limit: int = 50):\n",
    "    try:\n",
    "        files = list(path.glob(pattern)) if path.exists() else []\n",
    "        return files[:limit]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "paths = {\n",
    "    \"kaggle_csv\": list_top(RAW / \"kaggle\", \"*.csv\"),\n",
    "    \"owm_api\": list_top(RAW / \"api\" / \"owm\", \"*.csv\"),\n",
    "    \"rss_multi\": list_top(RAW / \"rss\", \"*.csv\"),\n",
    "    \"scraping_multi\": list_top(RAW / \"scraping\" / \"multi\", \"*.csv\"),\n",
    "    \"gdelt_gkg\": list_top(RAW / \"gdelt\", \"*.zip\"),\n",
    "    \"manifests\": list_top(RAW / \"manifests\", \"*.json\"),\n",
    "}\n",
    "\n",
    "# Comptes DB (si tables pr√©sentes)\n",
    "db_counts = {\"document\": None, \"flux\": None, \"source\": None, \"meteo\": None, \"evenement\": None}\n",
    "docs_by_type = []\n",
    "\n",
    "if not SKIP_DB:\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            # Timeout de requ√™te 3s\n",
    "            try:\n",
    "                conn.exec_driver_sql(\"SET LOCAL statement_timeout = 3000\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # Corrig√© avec pr√©fixes tXX_\n",
    "            for table, tname in [(\"document\", \"t04_document\"), (\"flux\", \"t03_flux\"), (\"source\", \"t02_source\"), (\"meteo\", \"t19_meteo\"), (\"evenement\", \"t25_evenement\")]:\n",
    "                try:\n",
    "                    db_counts[table] = conn.execute(text(f\"SELECT COUNT(*) FROM {tname}\")).scalar()\n",
    "                except Exception:\n",
    "                    db_counts[table] = None\n",
    "\n",
    "            try:\n",
    "                docs_by_type = conn.execute(text(\n",
    "                    \"\"\"\n",
    "                    SELECT td.libelle AS type_donnee, COUNT(d.id_doc) AS nb_docs\n",
    "                    FROM t04_document d\n",
    "                    LEFT JOIN t03_flux f ON d.id_flux = f.id_flux\n",
    "                    LEFT JOIN t02_source s ON f.id_source = s.id_source\n",
    "                    LEFT JOIN t01_type_donnee td ON s.id_type_donnee = td.id_type_donnee\n",
    "                    GROUP BY td.libelle\n",
    "                    ORDER BY nb_docs DESC NULLS LAST\n",
    "                    \"\"\"\n",
    "                )).fetchall()\n",
    "            except Exception:\n",
    "                docs_by_type = []\n",
    "    except Exception:\n",
    "        # DB indisponible, ignorer proprement\n",
    "        SKIP_DB = True\n",
    "\n",
    "# Rendu console\n",
    "print(\"\\n=== Inventaire fichiers data/raw ===\")\n",
    "for k, v in paths.items():\n",
    "    print(f\"- {k:14s}: {len(v)} fichier(s)\")\n",
    "\n",
    "print(\"\\n=== Comptes en base ===\")\n",
    "for k, v in db_counts.items():\n",
    "    print(f\"- {k:10s}: {v if v is not None else 'N/A'}\")\n",
    "\n",
    "if docs_by_type:\n",
    "    print(\"\\n=== Documents par type_donnee ===\")\n",
    "    for row in docs_by_type:\n",
    "        print(f\"- {row[0] or 'Inconnu'}: {row[1]}\")\n",
    "\n",
    "# √âcriture rapport Markdown\n",
    "report_path = PROJECT_ROOT / \"docs\" / \"SOURCES_INVENTAIRE_E1.md\"\n",
    "report_lines = []\n",
    "report_lines.append(\"# üßæ Inventaire E1 ‚Äî Preuves de collecte et d'insertion\\n\")\n",
    "report_lines.append(f\"G√©n√©r√© le: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%SZ')} UTC\\n\\n\")\n",
    "report_lines.append(\"## R√©sum√© fichiers pr√©sents (data/raw)\\n\")\n",
    "for label, files in paths.items():\n",
    "    report_lines.append(f\"- **{label}**: {len(files)} fichier(s)\")\n",
    "    for p in sorted(files)[:5]:\n",
    "        report_lines.append(f\"  - {p.as_posix()}\")\n",
    "    if len(files) > 5:\n",
    "        report_lines.append(f\"  - (+{len(files)-5} autres)\\n\")\n",
    "\n",
    "report_lines.append(\"\\n## Comptes en base (PostgreSQL)\\n\")\n",
    "for k, v in db_counts.items():\n",
    "    report_lines.append(f\"- **{k}**: {v if v is not None else 'N/A'}\")\n",
    "\n",
    "if docs_by_type:\n",
    "    report_lines.append(\"\\n### Documents par type_donnee\\n\")\n",
    "    for row in docs_by_type:\n",
    "        report_lines.append(f\"- {row[0] or 'Inconnu'}: {row[1]}\")\n",
    "\n",
    "report_path.write_text(\"\\n\".join(report_lines), encoding=\"utf-8\")\n",
    "print(f\"\\n‚úÖ Rapport inventaire √©crit: {report_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc2a035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import os\n",
    "import subprocess\n",
    "from datetime import UTC, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == \"notebooks\" else NOTEBOOK_DIR\n",
    "load_dotenv(PROJECT_ROOT / \".env\")\n",
    "\n",
    "PG_HOST = os.getenv(\"POSTGRES_HOST\", \"localhost\")\n",
    "PG_PORT = int(os.getenv(\"POSTGRES_PORT\", \"5432\"))\n",
    "PG_DB = os.getenv(\"POSTGRES_DB\", \"datasens\")\n",
    "PG_USER = os.getenv(\"POSTGRES_USER\", \"ds_user\")\n",
    "PG_PASS = os.getenv(\"POSTGRES_PASS\", \"ds_pass\")\n",
    "\n",
    "PG_URL = f\"postgresql+psycopg2://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{PG_DB}\"\n",
    "engine = create_engine(PG_URL, future=True)\n",
    "\n",
    "print(\"‚úÖ Configuration charg√©e\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc869f81",
   "metadata": {},
   "source": [
    "## üìä Bilan E1 : Ce qui est fait / √† faire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed22883",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä BILAN E1_V3 - Dataset Final Annot√©\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # Statistiques avec pr√©fixes tXX_ corrig√©s\n",
    "    stats = {\n",
    "        \"tables\": conn.execute(text(\"SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public' OR table_schema = 'datasens'\")).scalar(),\n",
    "        \"documents\": conn.execute(text(\"SELECT COUNT(*) FROM t04_document\")).scalar(),\n",
    "        \"flux\": conn.execute(text(\"SELECT COUNT(*) FROM t03_flux\")).scalar(),\n",
    "        \"sources\": conn.execute(text(\"SELECT COUNT(*) FROM t02_source\")).scalar(),\n",
    "        \"meteo\": conn.execute(text(\"SELECT COUNT(*) FROM t19_meteo\")).scalar(),\n",
    "        \"evenements\": conn.execute(text(\"SELECT COUNT(*) FROM t25_evenement\")).scalar(),\n",
    "        \"themes\": conn.execute(text(\"SELECT COUNT(*) FROM t24_theme\")).scalar(),\n",
    "    }\n",
    "    \n",
    "    # Statistiques par type de donn√©e\n",
    "    df_final = pd.read_sql_query(\"\"\"\n",
    "        SELECT \n",
    "            td.libelle AS type_donnee,\n",
    "            COUNT(DISTINCT s.id_source) AS nb_sources,\n",
    "            COUNT(DISTINCT d.id_doc) AS nb_documents,\n",
    "            COUNT(DISTINCT f.id_flux) AS nb_flux\n",
    "        FROM t01_type_donnee td\n",
    "        LEFT JOIN t02_source s ON td.id_type_donnee = s.id_type_donnee\n",
    "        LEFT JOIN t03_flux f ON s.id_source = f.id_source\n",
    "        LEFT JOIN t04_document d ON f.id_flux = d.id_flux\n",
    "        GROUP BY td.libelle\n",
    "        ORDER BY nb_documents DESC\n",
    "    \"\"\", conn)\n",
    "\n",
    "print(\"\\n‚úÖ R√©alis√© E1_v3 :\")\n",
    "print(f\"   ‚Ä¢ {stats['tables']} tables PostgreSQL cr√©√©es (architecture compl√®te 36/37 tables)\")\n",
    "print(f\"   ‚Ä¢ {stats['sources']} sources configur√©es\")\n",
    "print(f\"   ‚Ä¢ {stats['flux']} flux de collecte\")\n",
    "print(f\"   ‚Ä¢ {stats['documents']:,} documents collect√©s et nettoy√©s\")\n",
    "print(f\"   ‚Ä¢ {stats['meteo']} relev√©s m√©t√©o\")\n",
    "print(f\"   ‚Ä¢ {stats['evenements']} √©v√©nements\")\n",
    "print(f\"   ‚Ä¢ {stats['themes']} th√®mes identifi√©s\")\n",
    "\n",
    "# V√©rification que toutes les tables de r√©f√©rentiels sont remplies\n",
    "print(\"\\nüìä V√âRIFICATION REMPLISSAGE TABLES (E1_v3)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # V√©rification th√®mes et cat√©gories\n",
    "    nb_theme_cat = conn.execute(text(\"SELECT COUNT(*) FROM t23_theme_category\")).scalar()\n",
    "    nb_themes = conn.execute(text(\"SELECT COUNT(*) FROM t24_theme\")).scalar()\n",
    "    nb_type_donnee = conn.execute(text(\"SELECT COUNT(*) FROM t01_type_donnee\")).scalar()\n",
    "    nb_territoire = conn.execute(text(\"SELECT COUNT(*) FROM t17_territoire\")).scalar()\n",
    "    nb_indicateur = conn.execute(text(\"SELECT COUNT(*) FROM t22_indicateur\")).scalar()\n",
    "    \n",
    "    df_tables = pd.DataFrame({\n",
    "        \"Table\": [\"t23_theme_category\", \"t24_theme\", \"t01_type_donnee\", \"t17_territoire\", \"t22_indicateur\"],\n",
    "        \"Nb enregistrements\": [nb_theme_cat, nb_themes, nb_type_donnee, nb_territoire, nb_indicateur],\n",
    "        \"Statut\": [\n",
    "            \"‚úÖ OK\" if nb_theme_cat >= 12 else \"‚ö†Ô∏è Incomplet\",\n",
    "            \"‚úÖ OK\" if nb_themes >= 12 else \"‚ö†Ô∏è Incomplet\",\n",
    "            \"‚úÖ OK\" if nb_type_donnee >= 5 else \"‚ö†Ô∏è Incomplet\",\n",
    "            \"‚úÖ OK\" if nb_territoire > 0 else \"‚ö†Ô∏è Vide\",\n",
    "            \"‚úÖ OK\" if nb_indicateur > 0 else \"‚ÑπÔ∏è Optionnel\"\n",
    "        ]\n",
    "    })\n",
    "    display(df_tables)\n",
    "    \n",
    "    # D√©tails des th√®mes\n",
    "    if nb_themes > 0:\n",
    "        df_themes_detail = pd.read_sql_query(\"\"\"\n",
    "            SELECT \n",
    "                tc.libelle AS categorie,\n",
    "                COUNT(t.id_theme) AS nb_themes,\n",
    "                STRING_AGG(t.libelle, ', ' ORDER BY t.libelle) AS themes\n",
    "            FROM t23_theme_category tc\n",
    "            LEFT JOIN t24_theme t ON tc.id_theme_cat = t.id_theme_cat\n",
    "            GROUP BY tc.id_theme_cat, tc.libelle\n",
    "            ORDER BY tc.id_theme_cat\n",
    "        \"\"\", conn)\n",
    "        print(\"\\nüìã D√©tail th√®mes par cat√©gorie :\")\n",
    "        display(df_themes_detail)\n",
    "    \n",
    "    if nb_theme_cat < 12 or nb_themes < 12:\n",
    "        print(\"\\n‚ö†Ô∏è ATTENTION : Tous les th√®mes ne sont pas encore remplis\")\n",
    "        print(\"   üí° R√©ex√©cutez le notebook 02_schema_create.ipynb pour compl√©ter\")\n",
    "\n",
    "# Visualisations dataset final\n",
    "print(\"\\nüìä VISUALISATIONS DATASET FINAL ANNOT√â (E1_V3)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Graphique 1 : R√©partition par type de donn√©e\n",
    "if len(df_final) > 0:\n",
    "    print(\"\\nüìã R√©partition par type de donn√©e :\")\n",
    "    display(df_final)\n",
    "    \n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    bars = plt.bar(df_final[\"type_donnee\"], df_final[\"nb_documents\"], color=plt.cm.Set2(range(len(df_final))))\n",
    "    for bar, value in zip(bars, df_final[\"nb_documents\"]):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(df_final[\"nb_documents\"]) * 0.02,\n",
    "                f\"{int(value):,}\", ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    plt.title(\"üìä Documents par type de donn√©e (Dataset Final)\", fontsize=12, fontweight='bold')\n",
    "    plt.ylabel(\"Nombre de documents\", fontsize=11)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.pie(df_final[\"nb_documents\"], labels=df_final[\"type_donnee\"], autopct='%1.1f%%', startangle=90)\n",
    "    plt.title(\"üìä R√©partition documents par type (%)\", fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    bars = plt.bar(df_final[\"type_donnee\"], df_final[\"nb_sources\"], color=plt.cm.Pastel1(range(len(df_final))))\n",
    "    for bar, value in zip(bars, df_final[\"nb_sources\"]):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(df_final[\"nb_sources\"]) * 0.02,\n",
    "                str(int(value)), ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    plt.title(\"üìä Sources par type de donn√©e\", fontsize=12, fontweight='bold')\n",
    "    plt.ylabel(\"Nombre de sources\", fontsize=11)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    bars = plt.bar(df_final[\"type_donnee\"], df_final[\"nb_flux\"], color='#4ECDC4')\n",
    "    for bar, value in zip(bars, df_final[\"nb_flux\"]):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(df_final[\"nb_flux\"]) * 0.02,\n",
    "                str(int(value)), ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    plt.title(\"üìä Flux par type de donn√©e\", fontsize=12, fontweight='bold')\n",
    "    plt.ylabel(\"Nombre de flux\", fontsize=11)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Vue d'ensemble des documents\n",
    "print(\"\\nüìã Aper√ßu dataset final (50 premiers documents) :\")\n",
    "df_docs_final = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        d.id_doc,\n",
    "        LEFT(d.titre, 60) AS titre,\n",
    "        LEFT(d.texte, 100) AS texte_apercu,\n",
    "        d.langue,\n",
    "        d.date_publication,\n",
    "        s.nom AS source,\n",
    "        td.libelle AS type_donnee\n",
    "    FROM t04_document d\n",
    "    JOIN t03_flux f ON d.id_flux = f.id_flux\n",
    "    JOIN t02_source s ON f.id_source = s.id_source\n",
    "    JOIN t01_type_donnee td ON s.id_type_donnee = td.id_type_donnee\n",
    "    ORDER BY d.id_doc DESC\n",
    "    LIMIT 50\n",
    "\"\"\", engine)\n",
    "display(df_docs_final)\n",
    "\n",
    "print(\"\\n‚úÖ 6 types de sources ing√©r√©es E1_v3 :\")\n",
    "print(\"   1. Fichier plat CSV (Kaggle)\")\n",
    "print(\"   2. API OpenWeatherMap (m√©t√©o)\")\n",
    "print(\"   3. Flux RSS Multi-Sources (Franceinfo, 20 Minutes, Le Monde)\")\n",
    "print(\"   4. NewsAPI (optionnel)\")\n",
    "print(\"   5. Web Scraping Multi-Sources (Reddit, YouTube, Vie-publique, data.gouv)\")\n",
    "print(\"   6. Big Data GDELT GKG\")\n",
    "print(\"\\nüìã E1_v3 : Dataset pr√©par√© pour E2\")\n",
    "print(\"   ‚úÖ Annotation simple : nettoyage, d√©duplication, QA de base\")\n",
    "print(\"   ‚úÖ Structure pr√™te pour enrichissement IA (E2)\")\n",
    "print(\"\\nüìã √Ä faire ensuite (E2/E3) :\")\n",
    "print(\"   ‚Ä¢ E2 : Enrichissement IA (CamemBERT, FlauBERT)\")\n",
    "print(\"   ‚Ä¢ E2 : Annotation sentiment, NER, keywords (IA avanc√©e)\")\n",
    "print(\"   ‚Ä¢ E3 : Dashboard Power BI\")\n",
    "print(\"   ‚Ä¢ E3 : Orchestration Prefect/Airflow\")\n",
    "print(\"   ‚Ä¢ E3 : Tests automatis√©s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c78b3b",
   "metadata": {},
   "source": [
    "## üíæ Export DDL : Sauvegarde du sch√©ma SQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade89636",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üíæ Export DDL PostgreSQL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Export du sch√©ma complet\n",
    "with engine.connect() as conn:\n",
    "    schema_query = \"\"\"\n",
    "    SELECT\n",
    "        'CREATE TABLE ' || table_name || ' (' || E'\\\\n' ||\n",
    "        string_agg(\n",
    "            column_name || ' ' ||\n",
    "            CASE\n",
    "                WHEN data_type = 'integer' THEN 'INTEGER'\n",
    "                WHEN data_type = 'bigint' THEN 'BIGINT'\n",
    "                WHEN data_type = 'text' THEN 'TEXT'\n",
    "                WHEN data_type = 'character varying' THEN 'VARCHAR(' || character_maximum_length || ')'\n",
    "                WHEN data_type = 'timestamp without time zone' THEN 'TIMESTAMP'\n",
    "                WHEN data_type = 'real' THEN 'FLOAT'\n",
    "                ELSE data_type\n",
    "            END ||\n",
    "            CASE WHEN is_nullable = 'NO' THEN ' NOT NULL' ELSE '' END,\n",
    "            ',' || E'\\\\n    '\n",
    "            ORDER BY ordinal_position\n",
    "        ) || E'\\\\n);'\n",
    "        as ddl\n",
    "    FROM information_schema.columns\n",
    "    WHERE table_schema = 'public'\n",
    "    GROUP BY table_name;\n",
    "    \"\"\"\n",
    "\n",
    "    # Solution simplifi√©e : utiliser pg_dump ou exporter manuellement\n",
    "    print(\"üìù G√©n√©ration du sch√©ma SQL...\")\n",
    "\n",
    "    # Cr√©er le dossier docs s'il n'existe pas\n",
    "    docs_dir = PROJECT_ROOT / \"docs\"\n",
    "    docs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Export simplifi√© (pour un export complet, utiliser pg_dump)\n",
    "    schema_export = f\"\"\"\n",
    "-- DataSens E1 - Sch√©ma PostgreSQL\n",
    "-- Export g√©n√©r√© le {datetime.now(UTC).isoformat()}\n",
    "-- 18 tables Merise\n",
    "\n",
    "-- Note: Pour un export complet, utiliser:\n",
    "-- pg_dump -h {PG_HOST} -U {PG_USER} -d {PG_DB} --schema-only > docs/e1_schema.sql\n",
    "\"\"\"\n",
    "\n",
    "    schema_file = docs_dir / \"e1_schema.sql\"\n",
    "    schema_file.write_text(schema_export, encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"‚úÖ Sch√©ma export√© : {schema_file}\")\n",
    "    print(\"   üí° Pour un export complet, ex√©cutez:\")\n",
    "    print(f\"      pg_dump -h {PG_HOST} -U {PG_USER} -d {PG_DB} --schema-only > docs/e1_schema.sql\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb68b15a",
   "metadata": {},
   "source": [
    "## üì§ Export CSV : Snapshots des donn√©es (data/gold/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8658df06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì§ Export CSV - Snapshots Dataset Final Annot√© (data/gold/)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "gold_dir = PROJECT_ROOT / \"data\" / \"gold\"\n",
    "gold_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now(UTC).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Exporter tables principales (corrig√© avec pr√©fixes tXX_)\n",
    "tables_to_export = [\n",
    "    (\"document\", \"t04_document\"),\n",
    "    (\"source\", \"t02_source\"),\n",
    "    (\"flux\", \"t03_flux\"),\n",
    "    (\"territoire\", \"t17_territoire\"),\n",
    "    (\"meteo\", \"t19_meteo\"),\n",
    "    (\"evenement\", \"t25_evenement\"),\n",
    "    (\"theme\", \"t24_theme\")\n",
    "]\n",
    "\n",
    "exported = []\n",
    "for table_name, table_full in tables_to_export:\n",
    "    try:\n",
    "        df = pd.read_sql_query(f\"SELECT * FROM {table_full} LIMIT 1000\", engine)  # Limite pour d√©mo\n",
    "        if len(df) > 0:\n",
    "            csv_path = gold_dir / f\"{table_name}_{timestamp}.csv\"\n",
    "            df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "            exported.append(f\"   ‚úÖ {table_name}: {len(df)} lignes ‚Üí {csv_path.name}\")\n",
    "    except Exception as e:\n",
    "        exported.append(f\"   ‚ö†Ô∏è {table_name}: Erreur - {str(e)[:80]}\")\n",
    "\n",
    "print(\"\\nüìä Exports CSV dataset final :\")\n",
    "for item in exported:\n",
    "    print(item)\n",
    "\n",
    "# Visualisation des exports\n",
    "if len(exported) > 0:\n",
    "    print(\"\\nüìä Visualisation des snapshots export√©s :\")\n",
    "    export_data = []\n",
    "    for item in exported:\n",
    "        if \"‚úÖ\" in item:\n",
    "            parts = item.split(\": \")\n",
    "            table = parts[0].replace(\"   ‚úÖ \", \"\")\n",
    "            count = parts[1].split(\" lignes\")[0]\n",
    "            export_data.append({\"Table\": table, \"Lignes export√©es\": int(count)})\n",
    "    \n",
    "    if export_data:\n",
    "        df_exports = pd.DataFrame(export_data)\n",
    "        display(df_exports)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bars = plt.bar(df_exports[\"Table\"], df_exports[\"Lignes export√©es\"], color=plt.cm.Set3(range(len(df_exports))))\n",
    "        for bar, value in zip(bars, df_exports[\"Lignes export√©es\"]):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(df_exports[\"Lignes export√©es\"]) * 0.02,\n",
    "                    f\"{int(value):,}\", ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "        plt.title(\"üì§ Snapshots export√©s vers data/gold/ (Dataset Final)\", fontsize=12, fontweight='bold')\n",
    "        plt.ylabel(\"Nombre de lignes\", fontsize=11)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Snapshots dataset final annot√© sauvegard√©s dans data/gold/\")\n",
    "\n",
    "# ============================================================\n",
    "# EXPORT DATASET STRUCTUR√â POUR IA (Parquet)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üì¶ EXPORT DATASET STRUCTUR√â POUR IA (Format Parquet)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "    from pathlib import Path\n",
    "    \n",
    "    print(\"\\nüìä Export dataset complet pour enrichissement IA (E2)...\")\n",
    "    \n",
    "    # Cr√©er le dossier export si n√©cessaire\n",
    "    export_dir = PROJECT_ROOT / \"data\" / \"gold\" / \"dataset_ia\"\n",
    "    export_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Requ√™te consolid√©e : Documents + m√©tadonn√©es pr√™tes pour IA\n",
    "    dataset_query = \"\"\"\n",
    "        SELECT \n",
    "            d.id_doc,\n",
    "            d.titre,\n",
    "            d.texte,\n",
    "            d.langue,\n",
    "            d.date_publication,\n",
    "            d.hash_fingerprint,\n",
    "            s.nom AS source_nom,\n",
    "            td.libelle AS type_donnee,\n",
    "            f.date_collecte,\n",
    "            t.ville AS territoire,\n",
    "            -- Agr√©gation th√®mes\n",
    "            STRING_AGG(DISTINCT th.libelle, '; ') AS themes,\n",
    "            -- Comptage annotations (si pr√©sentes)\n",
    "            (SELECT COUNT(*) FROM t05_annotation ann WHERE ann.id_doc = d.id_doc) AS nb_annotations\n",
    "        FROM t04_document d\n",
    "        LEFT JOIN t03_flux f ON d.id_flux = f.id_flux\n",
    "        LEFT JOIN t02_source s ON f.id_source = s.id_source\n",
    "        LEFT JOIN t01_type_donnee td ON s.id_type_donnee = td.id_type_donnee\n",
    "        LEFT JOIN t17_territoire t ON d.id_territoire = t.id_territoire\n",
    "        LEFT JOIN t26_document_theme dt ON d.id_doc = dt.id_doc\n",
    "        LEFT JOIN t24_theme th ON dt.id_theme = th.id_theme\n",
    "        GROUP BY d.id_doc, d.titre, d.texte, d.langue, d.date_publication, d.hash_fingerprint,\n",
    "                 s.nom, td.libelle, f.date_collecte, t.ville\n",
    "        ORDER BY d.date_publication DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    df_dataset_ia = pd.read_sql_query(dataset_query, engine)\n",
    "    \n",
    "    if len(df_dataset_ia) > 0:\n",
    "        # Export Parquet (format optimal pour IA)\n",
    "        parquet_path = export_dir / f\"datasens_dataset_ia_{timestamp}.parquet\"\n",
    "        df_dataset_ia.to_parquet(parquet_path, engine='pyarrow', compression='snappy', index=False)\n",
    "        \n",
    "        file_size_mb = parquet_path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"\\n‚úÖ Dataset IA export√© :\")\n",
    "        print(f\"   üìÑ Fichier : {parquet_path.name}\")\n",
    "        print(f\"   üìä {len(df_dataset_ia):,} documents\")\n",
    "        print(f\"   üíæ Taille : {file_size_mb:.2f} MB\")\n",
    "        print(f\"   üìÅ Chemin : {parquet_path}\")\n",
    "        \n",
    "        # Export CSV √©galement (pour compatibilit√©)\n",
    "        csv_path = export_dir / f\"datasens_dataset_ia_{timestamp}.csv\"\n",
    "        df_dataset_ia.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "        csv_size_mb = csv_path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"\\n‚úÖ Export CSV compl√©mentaire :\")\n",
    "        print(f\"   üìÑ Fichier : {csv_path.name}\")\n",
    "        print(f\"   üíæ Taille : {csv_size_mb:.2f} MB\")\n",
    "        \n",
    "        # Aper√ßu du dataset\n",
    "        print(\"\\nüìã Aper√ßu dataset IA (5 premiers documents) :\")\n",
    "        display(df_dataset_ia.head())\n",
    "        \n",
    "        # Statistiques par type de donn√©e\n",
    "        print(\"\\nüìä Statistiques dataset par type de donn√©e :\")\n",
    "        stats_type = df_dataset_ia.groupby('type_donnee').agg({\n",
    "            'id_doc': 'count',\n",
    "            'langue': lambda x: x.value_counts().to_dict()\n",
    "        }).rename(columns={'id_doc': 'nb_documents'})\n",
    "        display(stats_type)\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Aucun document √† exporter\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è PyArrow non install√© - export Parquet impossible\")\n",
    "    print(\"   üí° Installez : pip install pyarrow\")\n",
    "    print(\"   ‚úÖ Export CSV disponible ci-dessus\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erreur export dataset IA : {str(e)[:100]}\")\n",
    "    print(\"   ‚úÖ Export CSV disponible ci-dessus\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ EXPORT DATASET STRUCTUR√â TERMIN√â\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nüìã Fichiers disponibles pour t√©l√©chargement :\")\n",
    "print(f\"   ‚Ä¢ Parquet (recommand√©) : data/gold/dataset_ia/datasens_dataset_ia_{timestamp}.parquet\")\n",
    "print(f\"   ‚Ä¢ CSV (compatibilit√©) : data/gold/dataset_ia/datasens_dataset_ia_{timestamp}.csv\")\n",
    "print(\"\\nüéØ Ce dataset est pr√™t pour enrichissement IA (E2) avec CamemBERT et FlauBERT\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b309c109",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Cr√©ation du tag Git : E1_REAL_YYYYMMDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfb657b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üè∑Ô∏è Cr√©ation tag Git\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "tag_name = f\"E1_REAL_{datetime.now(UTC).strftime('%Y%m%d')}\"\n",
    "\n",
    "git_dir = PROJECT_ROOT / \".git\"\n",
    "if git_dir.exists():\n",
    "    try:\n",
    "        # V√©rifier si le tag existe d√©j√†\n",
    "        result = subprocess.run(\n",
    "            [\"git\", \"tag\", \"-l\", tag_name],\n",
    "            check=False, cwd=PROJECT_ROOT,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "\n",
    "        if tag_name in result.stdout:\n",
    "            print(f\"‚ö†Ô∏è Tag {tag_name} existe d√©j√†\")\n",
    "        else:\n",
    "            # Cr√©er le tag\n",
    "            subprocess.run(\n",
    "                [\"git\", \"tag\", \"-a\", tag_name, \"-m\", f\"DataSens E1 complet - {tag_name}\"],\n",
    "                cwd=PROJECT_ROOT,\n",
    "                check=True\n",
    "            )\n",
    "            print(f\"‚úÖ Tag Git cr√©√© : {tag_name}\")\n",
    "            print(\"   üí° Pour pousser le tag: git push origin {tag_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur cr√©ation tag : {e}\")\n",
    "        print(f\"   üí° Cr√©ation manuelle: git tag -a {tag_name} -m 'DataSens E1'\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è D√©p√¥t Git non initialis√©\")\n",
    "    print(f\"   üí° Tag sugg√©r√©: {tag_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c23aea",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è Roadmap E2/E3\n",
    "\n",
    "Planification des prochaines √©tapes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b92303",
   "metadata": {},
   "source": [
    "### üìã E2 - Enrichissement IA (CamemBERT, FlauBERT)\n",
    "\n",
    "**E1_v3 pr√©pare le dataset** avec annotation simple (nettoyage, d√©duplication, QA de base).\n",
    "\n",
    "**E2 ajoutera l'annotation IA avanc√©e** :\n",
    "- **Annotation automatique** : Sentiment analysis (FlauBERT, CamemBERT)\n",
    "- **Extraction entit√©s nomm√©es** : spaCy NER (personnes, organisations, lieux)\n",
    "- **Embeddings vectoriels** : sentence-transformers pour recherche s√©mantique\n",
    "- **Classification th√©matique** : ML multi-labels (scikit-learn)\n",
    "- **Tables √† utiliser** : `t05_annotation`, `t08_emotion`, `t06_annotation_emotion` (d√©j√† cr√©√©es dans E1_v3)\n",
    "\n",
    "### üìä E3 - Production & Visualisation\n",
    "\n",
    "- **API REST** : FastAPI pour exposition des donn√©es\n",
    "- **Dashboard** : Power BI ou Streamlit pour visualisations interactives\n",
    "- **Orchestration** : Prefect/Airflow pour collecte automatique\n",
    "- **Monitoring** : Grafana + Prometheus pour m√©triques\n",
    "- **Tests** : pytest pour validation automatique\n",
    "- **Documentation** : API docs (Swagger/OpenAPI)\n",
    "\n",
    "### ‚úÖ E1_v3 Valid√©\n",
    "\n",
    "- ‚úÖ Mod√©lisation Merise (MCD ‚Üí MLD ‚Üí MPD) - 36/37 tables\n",
    "- ‚úÖ Architecture PostgreSQL compl√®te cr√©√©e\n",
    "- ‚úÖ CRUD complet test√©\n",
    "- ‚úÖ 6 types de sources ing√©r√©es\n",
    "- ‚úÖ Annotation simple : nettoyage, d√©duplication, QA de base\n",
    "- ‚úÖ Dataset pr√©par√© pour enrichissement IA (E2)\n",
    "- ‚úÖ Tra√ßabilit√© (flux, manifests, versioning Git)\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ F√©licitations ! E1_v3 est termin√© !**\n",
    "\n",
    "**E1_v3** : Dataset nettoy√© et annot√© simplement, pr√™t pour E2  \n",
    "**Prochaines √©tapes** : E2 avec enrichissement IA avanc√©e (CamemBERT, FlauBERT)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
