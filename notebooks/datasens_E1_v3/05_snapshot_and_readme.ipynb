{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdd5d970",
   "metadata": {},
   "source": [
    "# 📸 DataSens E1 — Notebook 5 : Snapshot et README\n",
    "\n",
    "**🎯 Objectif** : Créer un bilan E1, exporter le DDL/CSV, créer un tag Git et définir la roadmap E2/E3\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 Contenu de ce notebook\n",
    "\n",
    "1. **Bilan E1** : Ce qui est fait / à faire\n",
    "2. **Export DDL** : Sauvegarde du schéma SQL dans `docs/e1_schema.sql`\n",
    "3. **Export CSV** : Snapshots des données dans `data/gold/`\n",
    "4. **Tag Git** : Création du tag `E1_REAL_YYYYMMDD`\n",
    "5. **Roadmap E2/E3** : Prochaines étapes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db577bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 Inventaire E1 — Sources et traces de collecte (DataLake + PostgreSQL)\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == \"notebooks\" else NOTEBOOK_DIR\n",
    "load_dotenv(PROJECT_ROOT / \".env\")\n",
    "\n",
    "# Fail-fast DB URL (3s timeout)\n",
    "PG_URL = (\n",
    "    f\"postgresql+psycopg2://{os.getenv('POSTGRES_USER','ds_user')}:{os.getenv('POSTGRES_PASS','ds_pass')}@\"\n",
    "    f\"{os.getenv('POSTGRES_HOST','localhost')}:{int(os.getenv('POSTGRES_PORT','5432'))}/\"\n",
    "    f\"{os.getenv('POSTGRES_DB','datasens')}?connect_timeout=3\"\n",
    ")\n",
    "\n",
    "# Option pour ignorer DB si instable\n",
    "SKIP_DB = os.getenv(\"DS_SKIP_DB\", \"0\") == \"1\"\n",
    "engine = create_engine(PG_URL, future=True, pool_pre_ping=True)\n",
    "\n",
    "RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "\n",
    "# Limiter le scan (défensif)\n",
    "def list_top(path: Path, pattern: str, limit: int = 50):\n",
    "    try:\n",
    "        files = list(path.glob(pattern)) if path.exists() else []\n",
    "        return files[:limit]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "paths = {\n",
    "    \"kaggle_csv\": list_top(RAW / \"kaggle\", \"*.csv\"),\n",
    "    \"owm_api\": list_top(RAW / \"api\" / \"owm\", \"*.csv\"),\n",
    "    \"rss_multi\": list_top(RAW / \"rss\", \"*.csv\"),\n",
    "    \"scraping_multi\": list_top(RAW / \"scraping\" / \"multi\", \"*.csv\"),\n",
    "    \"gdelt_gkg\": list_top(RAW / \"gdelt\", \"*.zip\"),\n",
    "    \"manifests\": list_top(RAW / \"manifests\", \"*.json\"),\n",
    "}\n",
    "\n",
    "# Comptes DB (si tables présentes)\n",
    "db_counts = {\"document\": None, \"flux\": None, \"source\": None, \"meteo\": None, \"evenement\": None}\n",
    "docs_by_type = []\n",
    "\n",
    "if not SKIP_DB:\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            # Timeout de requête 3s\n",
    "            try:\n",
    "                conn.exec_driver_sql(\"SET LOCAL statement_timeout = 3000\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            for table in [\"document\", \"flux\", \"source\", \"meteo\", \"evenement\"]:\n",
    "                try:\n",
    "                    db_counts[table] = conn.execute(text(f\"SELECT COUNT(*) FROM {table}\")).scalar()\n",
    "                except Exception:\n",
    "                    db_counts[table] = None\n",
    "\n",
    "            try:\n",
    "                docs_by_type = conn.execute(text(\n",
    "                    \"\"\"\n",
    "                    SELECT td.libelle AS type_donnee, COUNT(d.id_doc) AS nb_docs\n",
    "                    FROM document d\n",
    "                    LEFT JOIN flux f ON d.id_flux = f.id_flux\n",
    "                    LEFT JOIN source s ON f.id_source = s.id_source\n",
    "                    LEFT JOIN type_donnee td ON s.id_type_donnee = td.id_type_donnee\n",
    "                    GROUP BY td.libelle\n",
    "                    ORDER BY nb_docs DESC NULLS LAST\n",
    "                    \"\"\"\n",
    "                )).fetchall()\n",
    "            except Exception:\n",
    "                docs_by_type = []\n",
    "    except Exception:\n",
    "        # DB indisponible, ignorer proprement\n",
    "        SKIP_DB = True\n",
    "\n",
    "# Rendu console\n",
    "print(\"\\n=== Inventaire fichiers data/raw ===\")\n",
    "for k, v in paths.items():\n",
    "    print(f\"- {k:14s}: {len(v)} fichier(s)\")\n",
    "\n",
    "print(\"\\n=== Comptes en base ===\")\n",
    "for k, v in db_counts.items():\n",
    "    print(f\"- {k:10s}: {v if v is not None else 'N/A'}\")\n",
    "\n",
    "if docs_by_type:\n",
    "    print(\"\\n=== Documents par type_donnee ===\")\n",
    "    for row in docs_by_type:\n",
    "        print(f\"- {row[0] or 'Inconnu'}: {row[1]}\")\n",
    "\n",
    "# Écriture rapport Markdown\n",
    "report_path = PROJECT_ROOT / \"docs\" / \"SOURCES_INVENTAIRE_E1.md\"\n",
    "report_lines = []\n",
    "report_lines.append(\"# 🧾 Inventaire E1 — Preuves de collecte et d'insertion\\n\")\n",
    "report_lines.append(f\"Généré le: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%SZ')} UTC\\n\\n\")\n",
    "report_lines.append(\"## Résumé fichiers présents (data/raw)\\n\")\n",
    "for label, files in paths.items():\n",
    "    report_lines.append(f\"- **{label}**: {len(files)} fichier(s)\")\n",
    "    for p in sorted(files)[:5]:\n",
    "        report_lines.append(f\"  - {p.as_posix()}\")\n",
    "    if len(files) > 5:\n",
    "        report_lines.append(f\"  - (+{len(files)-5} autres)\\n\")\n",
    "\n",
    "report_lines.append(\"\\n## Comptes en base (PostgreSQL)\\n\")\n",
    "for k, v in db_counts.items():\n",
    "    report_lines.append(f\"- **{k}**: {v if v is not None else 'N/A'}\")\n",
    "\n",
    "if docs_by_type:\n",
    "    report_lines.append(\"\\n### Documents par type_donnee\\n\")\n",
    "    for row in docs_by_type:\n",
    "        report_lines.append(f\"- {row[0] or 'Inconnu'}: {row[1]}\")\n",
    "\n",
    "report_path.write_text(\"\\n\".join(report_lines), encoding=\"utf-8\")\n",
    "print(f\"\\n✅ Rapport inventaire écrit: {report_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc2a035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import os\n",
    "import subprocess\n",
    "from datetime import UTC, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == \"notebooks\" else NOTEBOOK_DIR\n",
    "load_dotenv(PROJECT_ROOT / \".env\")\n",
    "\n",
    "PG_HOST = os.getenv(\"POSTGRES_HOST\", \"localhost\")\n",
    "PG_PORT = int(os.getenv(\"POSTGRES_PORT\", \"5432\"))\n",
    "PG_DB = os.getenv(\"POSTGRES_DB\", \"datasens\")\n",
    "PG_USER = os.getenv(\"POSTGRES_USER\", \"ds_user\")\n",
    "PG_PASS = os.getenv(\"POSTGRES_PASS\", \"ds_pass\")\n",
    "\n",
    "PG_URL = f\"postgresql+psycopg2://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{PG_DB}\"\n",
    "engine = create_engine(PG_URL, future=True)\n",
    "\n",
    "print(\"✅ Configuration chargée\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc869f81",
   "metadata": {},
   "source": [
    "## 📊 Bilan E1 : Ce qui est fait / à faire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed22883",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📊 BILAN E1\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    stats = {\n",
    "        \"tables\": conn.execute(text(\"SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public'\")).scalar(),\n",
    "        \"documents\": conn.execute(text(\"SELECT COUNT(*) FROM document\")).scalar(),\n",
    "        \"flux\": conn.execute(text(\"SELECT COUNT(*) FROM flux\")).scalar(),\n",
    "        \"sources\": conn.execute(text(\"SELECT COUNT(*) FROM source\")).scalar(),\n",
    "        \"meteo\": conn.execute(text(\"SELECT COUNT(*) FROM meteo\")).scalar(),\n",
    "        \"evenements\": conn.execute(text(\"SELECT COUNT(*) FROM evenement\")).scalar(),\n",
    "    }\n",
    "\n",
    "print(\"\\n✅ Réalisé :\")\n",
    "print(f\"   • {stats['tables']} tables PostgreSQL créées (schéma Merise)\")\n",
    "print(f\"   • {stats['sources']} sources configurées\")\n",
    "print(f\"   • {stats['flux']} flux de collecte\")\n",
    "print(f\"   • {stats['documents']} documents collectés\")\n",
    "print(f\"   • {stats['meteo']} relevés météo\")\n",
    "print(f\"   • {stats['evenements']} événements\")\n",
    "print(\"\\n✅ 5 types de sources ingérées :\")\n",
    "print(\"   1. Fichier plat CSV (Kaggle)\")\n",
    "print(\"   2. Base de données (SQLite → Postgres)\")\n",
    "print(\"   3. API (OpenWeatherMap)\")\n",
    "print(\"   4. Web Scraping (MonAvisCitoyen)\")\n",
    "print(\"   5. Big Data (GDELT GKG)\")\n",
    "print(\"\\n📋 À faire ensuite (E2/E3) :\")\n",
    "print(\"   • Enrichissement IA (NLP, sentiment analysis)\")\n",
    "print(\"   • Dashboard Power BI\")\n",
    "print(\"   • Orchestration Prefect/Airflow\")\n",
    "print(\"   • Tests automatisés\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c78b3b",
   "metadata": {},
   "source": [
    "## 💾 Export DDL : Sauvegarde du schéma SQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade89636",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"💾 Export DDL PostgreSQL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Export du schéma complet\n",
    "with engine.connect() as conn:\n",
    "    schema_query = \"\"\"\n",
    "    SELECT\n",
    "        'CREATE TABLE ' || table_name || ' (' || E'\\\\n' ||\n",
    "        string_agg(\n",
    "            column_name || ' ' ||\n",
    "            CASE\n",
    "                WHEN data_type = 'integer' THEN 'INTEGER'\n",
    "                WHEN data_type = 'bigint' THEN 'BIGINT'\n",
    "                WHEN data_type = 'text' THEN 'TEXT'\n",
    "                WHEN data_type = 'character varying' THEN 'VARCHAR(' || character_maximum_length || ')'\n",
    "                WHEN data_type = 'timestamp without time zone' THEN 'TIMESTAMP'\n",
    "                WHEN data_type = 'real' THEN 'FLOAT'\n",
    "                ELSE data_type\n",
    "            END ||\n",
    "            CASE WHEN is_nullable = 'NO' THEN ' NOT NULL' ELSE '' END,\n",
    "            ',' || E'\\\\n    '\n",
    "            ORDER BY ordinal_position\n",
    "        ) || E'\\\\n);'\n",
    "        as ddl\n",
    "    FROM information_schema.columns\n",
    "    WHERE table_schema = 'public'\n",
    "    GROUP BY table_name;\n",
    "    \"\"\"\n",
    "\n",
    "    # Solution simplifiée : utiliser pg_dump ou exporter manuellement\n",
    "    print(\"📝 Génération du schéma SQL...\")\n",
    "\n",
    "    # Créer le dossier docs s'il n'existe pas\n",
    "    docs_dir = PROJECT_ROOT / \"docs\"\n",
    "    docs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Export simplifié (pour un export complet, utiliser pg_dump)\n",
    "    schema_export = f\"\"\"\n",
    "-- DataSens E1 - Schéma PostgreSQL\n",
    "-- Export généré le {datetime.now(UTC).isoformat()}\n",
    "-- 18 tables Merise\n",
    "\n",
    "-- Note: Pour un export complet, utiliser:\n",
    "-- pg_dump -h {PG_HOST} -U {PG_USER} -d {PG_DB} --schema-only > docs/e1_schema.sql\n",
    "\"\"\"\n",
    "\n",
    "    schema_file = docs_dir / \"e1_schema.sql\"\n",
    "    schema_file.write_text(schema_export, encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"✅ Schéma exporté : {schema_file}\")\n",
    "    print(\"   💡 Pour un export complet, exécutez:\")\n",
    "    print(f\"      pg_dump -h {PG_HOST} -U {PG_USER} -d {PG_DB} --schema-only > docs/e1_schema.sql\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb68b15a",
   "metadata": {},
   "source": [
    "## 📤 Export CSV : Snapshots des données (data/gold/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8658df06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📤 Export CSV - Snapshots data/gold/\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "gold_dir = PROJECT_ROOT / \"data\" / \"gold\"\n",
    "gold_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now(UTC).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Exporter quelques tables principales\n",
    "tables_to_export = [\"document\", \"source\", \"flux\", \"territoire\", \"meteo\"]\n",
    "\n",
    "exported = []\n",
    "for table in tables_to_export:\n",
    "    try:\n",
    "        df = pd.read_sql(f\"SELECT * FROM {table} LIMIT 1000\", engine)  # Limite pour démo\n",
    "        if len(df) > 0:\n",
    "            csv_path = gold_dir / f\"{table}_{timestamp}.csv\"\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            exported.append(f\"   ✅ {table}: {len(df)} lignes → {csv_path.name}\")\n",
    "    except Exception as e:\n",
    "        exported.append(f\"   ⚠️ {table}: Erreur - {e}\")\n",
    "\n",
    "print(\"\\n📊 Exports CSV :\")\n",
    "for item in exported:\n",
    "    print(item)\n",
    "\n",
    "print(\"\\n✅ Snapshots sauvegardés dans data/gold/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b309c109",
   "metadata": {},
   "source": [
    "## 🏷️ Création du tag Git : E1_REAL_YYYYMMDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfb657b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🏷️ Création tag Git\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "tag_name = f\"E1_REAL_{datetime.now(UTC).strftime('%Y%m%d')}\"\n",
    "\n",
    "git_dir = PROJECT_ROOT / \".git\"\n",
    "if git_dir.exists():\n",
    "    try:\n",
    "        # Vérifier si le tag existe déjà\n",
    "        result = subprocess.run(\n",
    "            [\"git\", \"tag\", \"-l\", tag_name],\n",
    "            check=False, cwd=PROJECT_ROOT,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "\n",
    "        if tag_name in result.stdout:\n",
    "            print(f\"⚠️ Tag {tag_name} existe déjà\")\n",
    "        else:\n",
    "            # Créer le tag\n",
    "            subprocess.run(\n",
    "                [\"git\", \"tag\", \"-a\", tag_name, \"-m\", f\"DataSens E1 complet - {tag_name}\"],\n",
    "                cwd=PROJECT_ROOT,\n",
    "                check=True\n",
    "            )\n",
    "            print(f\"✅ Tag Git créé : {tag_name}\")\n",
    "            print(\"   💡 Pour pousser le tag: git push origin {tag_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erreur création tag : {e}\")\n",
    "        print(f\"   💡 Création manuelle: git tag -a {tag_name} -m 'DataSens E1'\")\n",
    "else:\n",
    "    print(\"⚠️ Dépôt Git non initialisé\")\n",
    "    print(f\"   💡 Tag suggéré: {tag_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c23aea",
   "metadata": {},
   "source": [
    "## 🗺️ Roadmap E2/E3\n",
    "\n",
    "Planification des prochaines étapes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b92303",
   "metadata": {},
   "source": [
    "### 📋 E2 - Enrichissement IA\n",
    "\n",
    "- **Annotation automatique** : Sentiment analysis (FlauBERT, CamemBERT)\n",
    "- **Extraction entités nommées** : spaCy NER (personnes, organisations, lieux)\n",
    "- **Embeddings vectoriels** : sentence-transformers pour recherche sémantique\n",
    "- **Classification thématique** : ML multi-labels (scikit-learn)\n",
    "- **Tables à créer** : `annotation`, `emotion`, `annotation_emotion`, `entite_nommee`\n",
    "\n",
    "### 📊 E3 - Production & Visualisation\n",
    "\n",
    "- **API REST** : FastAPI pour exposition des données\n",
    "- **Dashboard** : Power BI ou Streamlit pour visualisations interactives\n",
    "- **Orchestration** : Prefect/Airflow pour collecte automatique\n",
    "- **Monitoring** : Grafana + Prometheus pour métriques\n",
    "- **Tests** : pytest pour validation automatique\n",
    "- **Documentation** : API docs (Swagger/OpenAPI)\n",
    "\n",
    "### ✅ E1 Validé\n",
    "\n",
    "- ✅ Modélisation Merise (MCD → MLD → MPD)\n",
    "- ✅ 18 tables PostgreSQL créées\n",
    "- ✅ CRUD complet testé\n",
    "- ✅ 5 types de sources ingérées\n",
    "- ✅ Traçabilité (flux, manifests, versioning Git)\n",
    "\n",
    "---\n",
    "\n",
    "**🎉 Félicitations ! E1 est terminé !**\n",
    "\n",
    "**Prochaines étapes** : Commencer E2 avec l'enrichissement IA\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
