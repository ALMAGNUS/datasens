{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataSens logging setup (marker:datasens_logging)\nimport logging\nimport os\nos.makedirs('logs', exist_ok=True)\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s [%(levelname)s] %(message)s',\n    handlers=[\n        logging.StreamHandler(),\n        logging.FileHandler('logs/datasens.log', encoding='utf-8')\n    ]\n)\nlogging.info('D√©marrage')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSens E1_v3 ‚Äî 02_schema_create\n",
    "\n",
    "- Objectifs: Cr√©er le sch√©ma PostgreSQL complet **36/37 tables** (T01-T36 + T37) selon MPD.sql\n",
    "- Pr√©requis: 01_setup_env ex√©cut√© + PostgreSQL d√©marr√©\n",
    "- Sortie: Sch√©ma complet avec contraintes, index, r√©f√©rentiels + visualisations\n",
    "- Guide: docs/GUIDE_TECHNIQUE_E1.md + docs/datasens_MPD.sql\n",
    "\n",
    "> **E1_v3** : Architecture compl√®te selon MPD.sql (T01-T36 + T37 archive_flux)\n",
    "> - Domaine Collecte : T01-T03 + T37\n",
    "> - Documents & Annotations : T04-T12\n",
    "> - G√©ographie : T13-T17\n",
    "> - M√©t√©o : T18-T19\n",
    "> - Indicateurs/Barom√®tres : T20-T22 + T28-T29\n",
    "> - Th√®mes & √âv√©nements : T23-T27\n",
    "> - Pipeline & Qualit√© : T30-T34\n",
    "> - Audit/Versionning : T35-T36\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n# üé¨ DASHBOARD NARRATIF - O√ô SOMMES-NOUS ?\n# ============================================================\n# Ce dashboard vous guide √† travers le pipeline DataSens E1\n# Il montre la progression et l'√©tat actuel des donn√©es\n# ============================================================\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import FancyBboxPatch\nimport matplotlib.patches as mpatches\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üé¨ FIL D'ARIANE VISUEL - PIPELINE DATASENS E1\")\nprint(\"=\"*80)\n\n# Cr√©er figure dashboard\nfig = plt.figure(figsize=(16, 8))\nax = fig.add_subplot(111)\nax.set_xlim(0, 10)\nax.set_ylim(0, 6)\nax.axis('off')\n\n# √âtapes du pipeline\netapes = [\n    {\"nom\": \"üì• COLLECTE\", \"status\": \"‚úÖ\", \"desc\": \"Sources brutes\"},\n    {\"nom\": \"‚òÅÔ∏è DATALAKE\", \"status\": \"‚úÖ\", \"desc\": \"MinIO Raw\"},\n    {\"nom\": \"üßπ NETTOYAGE\", \"status\": \"üîÑ\", \"desc\": \"D√©duplication\"},\n    {\"nom\": \"üíæ ETL\", \"status\": \"‚è≥\", \"desc\": \"PostgreSQL\"},\n    {\"nom\": \"üìä ANNOTATION\", \"status\": \"‚è≥\", \"desc\": \"Enrichissement\"},\n    {\"nom\": \"üì¶ EXPORT\", \"status\": \"‚è≥\", \"desc\": \"Dataset IA\"}\n]\n\n# Couleurs selon statut\ncolors = {\n    \"‚úÖ\": \"#4ECDC4\",\n    \"üîÑ\": \"#FECA57\", \n    \"‚è≥\": \"#E8E8E8\"\n}\n\n# Dessiner timeline\ny_pos = 4\nx_start = 1\nx_spacing = 1.4\n\nfor i, etape in enumerate(etapes):\n    x_pos = x_start + i * x_spacing\n    \n    # Cercle √©tape\n    circle = plt.Circle((x_pos, y_pos), 0.25, color=colors[etape[\"status\"]], zorder=3)\n    ax.add_patch(circle)\n    ax.text(x_pos, y_pos, etape[\"status\"], ha='center', va='center', fontsize=14, fontweight='bold', zorder=4)\n    \n    # Nom √©tape\n    ax.text(x_pos, y_pos - 0.6, etape[\"nom\"], ha='center', va='top', fontsize=11, fontweight='bold')\n    ax.text(x_pos, y_pos - 0.85, etape[\"desc\"], ha='center', va='top', fontsize=9, style='italic')\n    \n    # Fl√®che vers prochaine √©tape\n    if i < len(etapes) - 1:\n        ax.arrow(x_pos + 0.3, y_pos, x_spacing - 0.6, 0, \n                head_width=0.1, head_length=0.15, fc='gray', ec='gray', zorder=2)\n\n# Titre narratif\nax.text(5, 5.5, \"üéØ PROGRESSION DU PIPELINE E1\", ha='center', va='center', \n        fontsize=16, fontweight='bold', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\n# L√©gende\nlegend_elements = [\n    mpatches.Patch(facecolor='#4ECDC4', label='Termin√©'),\n    mpatches.Patch(facecolor='#FECA57', label='En cours'),\n    mpatches.Patch(facecolor='#E8E8E8', label='√Ä venir')\n]\nax.legend(handles=legend_elements, loc='upper left', fontsize=10)\n\n# Statistiques rapides (si disponibles)\nstats_text = \"\\nüìä SNAPSHOT ACTUEL :\\n\"\ntry:\n    # Essayer de charger des stats si base disponible\n    stats_text += \"   ‚Ä¢ Pipeline en cours d'ex√©cution...\\n\"\nexcept:\n    stats_text += \"   ‚Ä¢ D√©marrage du pipeline...\\n\"\n\nax.text(5, 1.5, stats_text, ha='center', va='center', fontsize=10,\n        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n\nplt.title(\"üé¨ FIL D'ARIANE VISUEL - Accompagnement narratif du jury\", \n          fontsize=14, fontweight='bold', pad=20)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nüí° Le fil d'Ariane vous guide √©tape par √©tape √† travers le pipeline\")\nprint(\"   Chaque visualisation s'inscrit dans cette progression narrative\\n\")\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notes:\n",
    "> - **Chargement depuis docs/datasens_MPD.sql** : DDL complet avec toutes les contraintes\n",
    "> - **Pr√©fixe T01-T37** : Nomenclature selon MPD (t01_type_donnee, t02_source, etc.)\n",
    "> - **Bootstrap r√©f√©rentiels** : type_donnee (5 types), valence (3), pays (France)\n",
    "> - **Visualisations** : Graphique r√©partition par domaine + tables pandas pour le jury\n",
    "> - **R√©f√©rences** : docs/datasens_MPD.sql, docs/datasens_tables_dictionary.md\n",
    "- `pays`, `region`, `departement`, `commune`, `territoire`\n",
    "\n",
    "**Contexte** (5 tables) :\n",
    "- `type_meteo`, `meteo`, `type_indicateur`, `source_indicateur`, `indicateur`\n",
    "\n",
    "**Th√®mes & √âv√©nements** (5 tables) :\n",
    "- `theme_category`, `theme`, `evenement`, `document_theme`, `document_evenement`\n",
    "\n",
    "**Barom√®tres** (2 tables) :\n",
    "- `source_barometre`, `document_baro`\n",
    "\n",
    "**Pipeline & Qualit√©** (5 tables) :\n",
    "- `pipeline`, `etape_etl`, `exec_etape`, `qc_rule`, `qc_result`\n",
    "\n",
    "**Gouvernance** (2 tables) :\n",
    "- `table_audit`, `table_version`\n",
    "\n",
    "**Collecte** :\n",
    "- `type_donnee` : Cat√©gorisation des sources (Fichier, Base de donn√©es, API, Web Scraping, Big Data)\n",
    "- `source` : Sources r√©elles (Kaggle, OpenWeatherMap, MonAvisCitoyen, etc.)\n",
    "- `flux` : Tra√ßabilit√© des collectes (date, format, manifest_uri)\n",
    "\n",
    "**Corpus** :\n",
    "- `document` : Documents bruts collect√©s (titre, texte, langue, hash_fingerprint)\n",
    "- `territoire` : G√©olocalisation (ville, code_insee, lat, lon)\n",
    "\n",
    "**Contexte** :\n",
    "- `type_meteo` : Types de conditions m√©t√©o (clair, nuageux, pluie...)\n",
    "- `meteo` : Relev√©s m√©t√©o (temp√©rature, humidit√©, pression, vent)\n",
    "- `type_indicateur` : Types d'indicateurs (population, revenu, etc.)\n",
    "- `source_indicateur` : Sources des indicateurs (INSEE, IGN...)\n",
    "- `indicateur` : Valeurs d'indicateurs par territoire\n",
    "\n",
    "**Th√®mes/√©v√©nements** :\n",
    "- `theme` : Th√®mes documentaires (politique, √©conomie, environnement...)\n",
    "- `evenement` : √âv√©nements temporels (date_event, avg_tone)\n",
    "- `document_evenement` : Relation N-N documents ‚Üî √©v√©nements\n",
    "\n",
    "**Gouvernance pipeline** :\n",
    "- `pipeline` : Description des pipelines ETL\n",
    "- `etape_etl` : √âtapes du pipeline avec ordre d'ex√©cution\n",
    "\n",
    "**Utilisateurs (trace)** :\n",
    "- `utilisateur` : Utilisateurs du syst√®me (pour futures annotations)\n",
    "\n",
    "**Qualit√© (min)** :\n",
    "- `qc_rule` : R√®gles de contr√¥le qualit√© (placeholder)\n",
    "- `qc_result` : R√©sultats des contr√¥les qualit√© (optionnel)\n",
    "\n",
    "---\n",
    "\n",
    "### Sch√©ma Mermaid (simplifi√©)\n",
    "\n",
    "```mermaid\n",
    "erDiagram\n",
    "    TYPE_DONNEE ||--o{ SOURCE : \"a pour\"\n",
    "    SOURCE ||--o{ FLUX : \"g√©n√®re\"\n",
    "    FLUX ||--o{ DOCUMENT : \"contient\"\n",
    "    TERRITOIRE ||--o{ DOCUMENT : \"g√©olocalise\"\n",
    "    TERRITOIRE ||--o{ METEO : \"mesure\"\n",
    "    TERRITOIRE ||--o{ INDICATEUR : \"agr√®ge\"\n",
    "    THEME ||--o{ EVENEMENT : \"classe\"\n",
    "    DOCUMENT ||--o{ DOCUMENT_EVENEMENT : \"ref√®re\"\n",
    "    EVENEMENT ||--o{ DOCUMENT_EVENEMENT : \"associe\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSens E1_v3 - 02_schema_create\n# üíæ Sch√©ma PostgreSQL complet 36/37 tables selon MPD.sql + Bootstrap + Visualisations\n\nimport os\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sqlalchemy import create_engine, text\n\n# Utiliser les variables du notebook 01\nif 'PROJECT_ROOT' not in globals():\n    current = Path.cwd()\n    PROJECT_ROOT = None\n    while current != current.parent:\n        if (current / \"notebooks\").exists() and (current / \"docs\").exists():\n            PROJECT_ROOT = current\n            break\n        current = current.parent\n    else:\n        PROJECT_ROOT = Path.cwd()\n\nif 'PG_URL' not in globals():\n    PG_URL = os.getenv(\"DATASENS_PG_URL\", \"postgresql+psycopg2://postgres:postgres@localhost:5433/postgres\")\n\nengine = create_engine(PG_URL, future=True)\nprint(f\"üìÇ Connexion PostgreSQL : {engine.url.host}:{engine.url.port}/{engine.url.database}\")\n\n# =====================================================\n# FONCTIONS UTILITAIRES DE S√âCURIT√â\n# =====================================================\ndef assert_valid_identifier(name: str) -> None:\n    \"\"\"\n    Valide qu'un identifiant SQL (nom de table, colonne) est s√ªr.\n    L√®ve une ValueError si l'identifiant contient des caract√®res non autoris√©s.\n    \"\"\"\n    if not isinstance(name, str):\n        raise ValueError(\"L'identifiant doit √™tre une cha√Æne de caract√®res.\")\n    # Autorise lettres, chiffres, underscores, et points (pour sch√©mas.tables)\n    if not name.replace('_', '').replace('.', '').isalnum():\n        raise ValueError(f\"Identifiant SQL invalide : {name}. Seuls les caract√®res alphanum√©riques, underscores et points sont autoris√©s.\")\n\nprint(\"‚úÖ Fonctions de s√©curit√© charg√©es\")\nprint(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìê DDL PostgreSQL : Cr√©ation des 36 tables E2\n",
    "\n",
    "Cr√©ation des tables avec contraintes d'int√©grit√© r√©f√©rentielle.  \n",
    "**Ordre de cr√©ation** : Respect des d√©pendances FK (r√©f√©rentiels ‚Üí m√©tier ‚Üí liaisons).\n",
    "\n",
    "**Note** : Les sources obsol√®tes/payantes ne sont **pas** impl√©ment√©es.  \n",
    "**Sources E1 test√©es** : Kaggle CSV, OpenWeatherMap API, RSS Multi-sources, Web Scraping (Vie-publique, data.gouv), GDELT GKG  \n",
    "**Voir** `docs/SOURCES_STATUS.md` pour statut complet des sources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDL complet : 36/37 tables E1_v3\n# Bas√© sur MCD/MLD/MPD valid√©s - Ordre respecte d√©pendances FK\n# Chargement depuis docs/datasens_MPD.sql (architecture compl√®te)\n\nddl_file = PROJECT_ROOT / \"docs\" / \"datasens_MPD.sql\"\n\nif ddl_file.exists():\n    with open(ddl_file, encoding='utf-8') as f:\n        ddl_sql = f.read()\n    print(f\"‚úÖ DDL charg√© depuis {ddl_file.name}\")\n    print(f\"   üìÑ Fichier : {ddl_file}\")\nelse:\n    print(f\"‚ùå Fichier DDL non trouv√©: {ddl_file}\")\n    print(\"   üí° V√©rifiez que docs/datasens_MPD.sql existe\")\n    raise FileNotFoundError(f\"MPD.sql introuvable : {ddl_file}\")\n\nprint(\"\\n‚úÖ DDL charg√© depuis MPD.sql - Pr√™t pour cr√©ation des 36/37 tables\")\nprint(\"=\" * 80)\n\n# Option : Supprimer toutes les tables existantes avant de les recr√©er\nDROP_TABLES = os.getenv(\"DROP_TABLES\", \"false\").lower() == \"true\"  # S√©curit√© : false par d√©faut\n\nwith engine.begin() as conn:\n    if DROP_TABLES:\n        print(\"‚ö†Ô∏è Suppression des tables existantes...\")\n        # Supprimer toutes les tables selon MPD (ordre inverse des d√©pendances)\n        drop_order = [\n            \"t34_qc_result\", \"t33_qc_rule\", \"t32_exec_etape\", \"t31_etape_etl\", \"t30_pipeline\",\n            \"t29_document_baro\", \"t28_source_barometre\",\n            \"t27_document_evenement\", \"t26_document_theme\", \"t25_evenement\", \"t24_theme\", \"t23_theme_category\",\n            \"t22_indicateur\", \"t21_source_indicateur\", \"t20_type_indicateur\",\n            \"t19_meteo\", \"t18_type_meteo\",\n            \"t17_territoire\", \"t16_commune\", \"t15_departement\", \"t14_region\", \"t13_pays\",\n            \"t07_meta_annotation\", \"t06_annotation_emotion\", \"t05_annotation\", \"t08_emotion\", \"t09_type_emotion\", \n            \"t10_valence\", \"t11_modele_ia\", \"t12_utilisateur\",\n            \"t04_document\",\n            \"t37_archive_flux\", \"t03_flux\", \"t02_source\", \"t01_type_donnee\",\n            \"t36_table_version\", \"t35_table_audit\"\n        ]\n        # S√©curit√© : Valider tous les noms de tables avant utilisation\n        for table in drop_order:\n            assert_valid_identifier(table)  # Protection anti-injection SQL\n        # Suppression s√©curis√©e\n        for table in drop_order:\n            try:\n                assert_valid_identifier(table)  # Double validation pour s√©curit√© maximale\n                conn.execute(text(f\"DROP TABLE IF EXISTS datasens.{table} CASCADE\"))\n                conn.execute(text(f\"DROP TABLE IF EXISTS {table} CASCADE\"))\n            except:\n                pass\n        # Supprimer le type enum\n        conn.execute(text(\"DROP TYPE IF EXISTS polarity_enum CASCADE\"))\n        print(\"‚úÖ Tables supprim√©es\")\n    else:\n        print(\"‚ÑπÔ∏è DROP_TABLES=false ‚Üí Tables existantes conserv√©es (utiliser IF NOT EXISTS)\")\n\n    # Cr√©er le sch√©ma datasens si n√©cessaire\n    conn.execute(text(\"CREATE SCHEMA IF NOT EXISTS datasens\"))\n    conn.execute(text(\"SET search_path TO datasens, public\"))\n    \n    # Ex√©cuter le DDL complet depuis MPD.sql\n    # Le MPD.sql contient d√©j√† les CREATE TABLE avec IF NOT EXISTS, donc on peut l'ex√©cuter directement\n    # S√©parer les statements (en ignorant les commentaires et lignes vides)\n    statements = []\n    current_stmt = []\n    \n    for line in ddl_sql.split('\\n'):\n        line_stripped = line.strip()\n        # Ignorer commentaires et lignes vides\n        if not line_stripped or line_stripped.startswith('--'):\n            continue\n        current_stmt.append(line)\n        # Si la ligne se termine par ';', c'est la fin d'un statement\n        if line_stripped.endswith(';'):\n            stmt = '\\n'.join(current_stmt)\n            if stmt.strip():\n                statements.append(stmt)\n            current_stmt = []\n    \n    # Si on a encore du texte dans current_stmt, l'ajouter\n    if current_stmt:\n        stmt = '\\n'.join(current_stmt)\n        if stmt.strip():\n            statements.append(stmt)\n    \n    # Ex√©cuter chaque statement\n    created_tables = 0\n    for i, stmt in enumerate(statements, 1):\n        try:\n            conn.execute(text(stmt))\n            # Compter les CREATE TABLE\n            if 'CREATE TABLE' in stmt.upper():\n                created_tables += 1\n        except Exception as e:\n            # Ignorer erreurs \"already exists\" pour IF NOT EXISTS\n            if 'already exists' not in str(e).lower() and 'duplicate' not in str(e).lower():\n                print(f\"‚ö†Ô∏è Erreur statement {i}: {str(e)[:100]}\")\n\nprint(f\"\\n‚úÖ Sch√©ma E1_v3 cr√©√© : {created_tables} tables cr√©√©es\")\nprint(\"   üìä Architecture compl√®te selon MPD.sql (T01-T36 + T37)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Index et contraintes additionnelles\n",
    "\n",
    "Cr√©ation des index pour optimiser les requ√™tes (hash_fingerprint, dates, cl√©s √©trang√®res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index pour performance\nindexes_sql = \"\"\"\n-- Index sur hash_fingerprint pour d√©duplication rapide\nCREATE INDEX IF NOT EXISTS idx_document_hash_fingerprint ON document(hash_fingerprint);\n\n-- Index sur dates pour requ√™tes temporelles\nCREATE INDEX IF NOT EXISTS idx_document_date_publication ON document(date_publication);\nCREATE INDEX IF NOT EXISTS idx_flux_date_collecte ON flux(date_collecte);\nCREATE INDEX IF NOT EXISTS idx_meteo_date_obs ON meteo(date_obs);\nCREATE INDEX IF NOT EXISTS idx_evenement_date_event ON evenement(date_event);\n\n-- Index sur cl√©s √©trang√®res fr√©quentes\nCREATE INDEX IF NOT EXISTS idx_document_id_flux ON document(id_flux);\nCREATE INDEX IF NOT EXISTS idx_document_id_territoire ON document(id_territoire);\nCREATE INDEX IF NOT EXISTS idx_flux_id_source ON flux(id_source);\nCREATE INDEX IF NOT EXISTS idx_meteo_id_territoire ON meteo(id_territoire);\nCREATE INDEX IF NOT EXISTS idx_indicateur_id_territoire ON indicateur(id_territoire);\n\n-- Index composite pour recherche par territoire + date\nCREATE INDEX IF NOT EXISTS idx_meteo_territoire_date ON meteo(id_territoire, date_obs DESC);\n\"\"\"\n\nprint(\"üîó Cr√©ation des index\")\nprint(\"=\" * 80)\n\nwith engine.begin() as conn:\n    conn.exec_driver_sql(indexes_sql)\n\nprint(\"‚úÖ Index cr√©√©s avec succ√®s !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Insertion des r√©f√©rentiels\n",
    "\n",
    "Insertion des donn√©es de r√©f√©rence n√©cessaires pour normaliser les donn√©es\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìù Bootstrap des r√©f√©rentiels selon MPD.sql\n# Le MPD.sql contient d√©j√† des INSERT dans la section 9, mais on les ex√©cute ici pour s'assurer\n\nprint(\"üìù Bootstrap des r√©f√©rentiels\")\nprint(\"=\" * 80)\n\nwith engine.begin() as conn:\n    # V√©rifier et ins√©rer les r√©f√©rentiels de base selon MPD.sql\n    # T10_VALENCE (d√©j√† dans MPD.sql mais on v√©rifie)\n    conn.execute(text(\"\"\"\n        INSERT INTO t10_valence (label, description)\n        VALUES ('positive','valence positive'), ('neutre','valence neutre'), ('negative','valence n√©gative')\n        ON CONFLICT (label) DO NOTHING\n    \"\"\"))\n    \n    # T01_TYPE_DONNEE (selon MPD.sql section 9) - Classification professionnelle m√©diam√©trie\n    conn.execute(text(\"\"\"\n        INSERT INTO t01_type_donnee (libelle, description, frequence_maj, categorie_metier)\n        VALUES\n          -- 1. Donn√©es de classification ou Nomenclatures (Reference Data)\n          ('Nomenclature','Syst√®me de cat√©gorisation/classification servant de r√©f√©rence aux autres donn√©es (unit√©s de mesure, codes pays ISO, CSP...)','mensuelle','classification'),\n          -- 2. Donn√©es de r√©f√©rences ou donn√©es ma√Ætres (Master Data)\n          ('Donn√©es Ma√Ætres','Donn√©es partag√©es par un ensemble de processus et d''applications (clients, produits, r√©f√©rentiels...)','quotidienne','reference'),\n          -- 3. Donn√©es op√©rationnelles (Operational Data)\n          ('Donn√©es Op√©rationnelles','Donn√©es li√©es √† des op√©rations et activit√©s (transactions, demandes, tickets...)','secondes','operationnelle'),\n          -- 4. Donn√©es d√©cisionnelles (Analytical Data)\n          ('Donn√©es D√©cisionnelles','Donn√©es consolid√©es permettant de faire des analyses √† des fins de prise de d√©cisions (faits de vente, dimensions...)','quotidienne','decisionnelle'),\n          -- 5. M√©tadonn√©es (Metadata)\n          ('M√©tadonn√©es','Donn√©es sur les donn√©es (descriptives, structurelles, administratives, usages, r√©f√©rence, statistiques, l√©gales...)','variable','metadonnees')\n        ON CONFLICT DO NOTHING\n    \"\"\"))\n    \n    # T13_PAYS (France)\n    conn.execute(text(\"\"\"\n        INSERT INTO t13_pays (nom) VALUES ('France') ON CONFLICT DO NOTHING\n    \"\"\"))\n    \n    # V√©rifier les entr√©es ins√©r√©es\n    nb_valence = conn.execute(text(\"SELECT COUNT(*) FROM t10_valence\")).scalar()\n    nb_types = conn.execute(text(\"SELECT COUNT(*) FROM t01_type_donnee\")).scalar()\n    nb_pays = conn.execute(text(\"SELECT COUNT(*) FROM t13_pays\")).scalar()\n    \n    print(f\"‚úÖ Bootstrap r√©f√©rentiels :\")\n    print(f\"   ‚Ä¢ T10_valence : {nb_valence} entr√©es\")\n    print(f\"   ‚Ä¢ T01_type_donnee : {nb_types} entr√©es\")\n    print(f\"   ‚Ä¢ T13_pays : {nb_pays} entr√©es\")\n    \n    # Afficher le contenu des r√©f√©rentiels\n    print(\"\\nüìã Table t01_type_donnee :\")\n    df_type_donnee = pd.read_sql_query(\"SELECT * FROM t01_type_donnee\", engine)\n    display(df_type_donnee)\n    \n    print(\"\\nüìã Table t10_valence :\")\n    df_valence = pd.read_sql_query(\"SELECT * FROM t10_valence\", engine)\n    display(df_valence)\n\nprint(\"\\n‚úÖ Bootstrap des r√©f√©rentiels termin√© !\")\n\n# Ancien code de r√©f√©rentiels (gard√© pour r√©f√©rence si besoin d'enrichissement)\nreferentiels_old = {\n    \"type_donnee\": [\n        (\"Nomenclature\", \"Syst√®me de cat√©gorisation/classification servant de r√©f√©rence\"),\n        (\"Donn√©es Ma√Ætres\", \"Donn√©es partag√©es par un ensemble de processus et d'applications\"),\n        (\"Donn√©es Op√©rationnelles\", \"Donn√©es li√©es √† des op√©rations et activit√©s\"),\n        (\"Donn√©es D√©cisionnelles\", \"Donn√©es consolid√©es pour analyses et prise de d√©cisions\"),\n        (\"M√©tadonn√©es\", \"Donn√©es sur les donn√©es (descriptives, structurelles, administratives...)\"),\n    ],\n    \"type_meteo\": [\n        (\"CLEAR\", \"Ciel clair\"),\n        (\"CLOUDS\", \"Nuageux\"),\n        (\"RAIN\", \"Pluie\"),\n        (\"SNOW\", \"Neige\"),\n        (\"THUNDERSTORM\", \"Orage\"),\n        (\"FOG\", \"Brouillard\"),\n    ],\n    \"type_indicateur\": [\n        (\"POPULATION\", \"Population totale\", \"habitants\"),\n        (\"REVENU_MEDIAN\", \"Revenu m√©dian\", \"‚Ç¨\"),\n        (\"TAUX_CHOMAGE\", \"Taux de ch√¥mage\", \"%\"),\n        (\"SUPERFICIE\", \"Superficie\", \"km¬≤\"),\n    ],\n    \"source_indicateur\": [\n        (\"INSEE\", \"https://www.insee.fr/\"),\n        (\"IGN\", \"https://www.ign.fr/\"),\n        (\"data.gouv.fr\", \"https://www.data.gouv.fr/\"),\n    ],\n    \"theme_category\": [\n        (\"Soci√©t√©\", \"Th√®mes li√©s √† la soci√©t√©\"),\n        (\"Politique\", \"Th√®mes politiques\"),\n        (\"√âconomie\", \"Th√®mes √©conomiques\"),\n        (\"Environnement\", \"Th√®mes environnementaux\"),\n        (\"Sant√©\", \"Th√®mes de sant√©\"),\n    ],\n    \"theme\": [\n        (\"Politique\", \"√âv√©nements et analyses politiques\"),\n        (\"√âconomie\", \"Actualit√©s √©conomiques\"),\n        (\"Soci√©t√©\", \"Faits de soci√©t√©\"),\n        (\"Environnement\", \"√âcologie, climat, biodiversit√©\"),\n        (\"Sant√©\", \"Sant√© publique, m√©dical\"),\n        (\"Sport\", \"√âv√©nements sportifs\"),\n        (\"Culture\", \"Arts, spectacles, culture\"),\n        (\"Technologie\", \"Innovation, num√©rique\"),\n    ],\n    \"valence\": [\n        (\"Positive\", \"√âmotions positives (joie, espoir, satisfaction)\"),\n        (\"Neutre\", \"√âmotions neutres (indiff√©rence, calme)\"),\n        (\"Negative\", \"√âmotions n√©gatives (col√®re, tristesse, peur)\"),\n    ],\n    \"type_emotion\": [\n        (\"Joie\", \"Sentiment de bonheur\", \"Positive\"),\n        (\"Col√®re\", \"Sentiment de frustration ou agressivit√©\", \"Negative\"),\n        (\"Tristesse\", \"Sentiment de peine\", \"Negative\"),\n        (\"Peur\", \"Sentiment d'anxi√©t√©\", \"Negative\"),\n        (\"Espoir\", \"Sentiment d'optimisme\", \"Positive\"),\n        (\"Neutre\", \"Pas d'√©motion particuli√®re\", \"Neutre\"),\n    ],\n    \"pays\": [\n        (\"France\",),\n    ],\n    \"source_barometre\": [\n        (\"INSEE Barom√®tre Social\", \"https://www.insee.fr/\"),\n        (\"Data.gouv.fr\", \"https://www.data.gouv.fr/\"),\n    ],\n    \"qc_rule\": [\n        (\"No duplicates\", \"V√©rifier absence de doublons via hash_fingerprint\", \"SELECT COUNT(*) FROM document GROUP BY hash_fingerprint HAVING COUNT(*) > 1\"),\n        (\"No NULL titles\", \"Tous les documents doivent avoir un titre\", \"SELECT COUNT(*) FROM document WHERE titre IS NULL\"),\n        (\"Date range valid\", \"Les dates de publication doivent √™tre raisonnables\", \"SELECT COUNT(*) FROM document WHERE date_publication < '1900-01-01' OR date_publication > NOW()\"),\n    ],\n}\n\nprint(\"üìù Insertion des r√©f√©rentiels\")\nprint(\"=\" * 80)\n\nwith engine.begin() as conn:\n    # V√©rifier et corriger la structure de type_donnee si n√©cessaire\n    try:\n        # V√©rifier si la colonne description existe\n        result = conn.execute(text(\"\"\"\n            SELECT column_name\n            FROM information_schema.columns\n            WHERE table_name = 'type_donnee' AND column_name = 'description'\n        \"\"\")).fetchone()\n\n        if not result:\n            # Ajouter la colonne description si elle n'existe pas\n            print(\"‚ö†Ô∏è Colonne 'description' manquante dans type_donnee, ajout en cours...\")\n            conn.execute(text(\"ALTER TABLE type_donnee ADD COLUMN IF NOT EXISTS description TEXT\"))\n            print(\"‚úÖ Colonne 'description' ajout√©e\")\n\n        # V√©rifier si la contrainte UNIQUE sur libelle existe\n        constraint_exists = conn.execute(text(\"\"\"\n            SELECT 1\n            FROM information_schema.table_constraints\n            WHERE table_name = 'type_donnee'\n              AND constraint_type = 'UNIQUE'\n              AND constraint_name LIKE '%libelle%'\n        \"\"\")).fetchone()\n\n        if not constraint_exists:\n            # V√©rifier si un index unique existe\n            index_exists = conn.execute(text(\"\"\"\n                SELECT 1\n                FROM pg_indexes\n                WHERE tablename = 'type_donnee'\n                  AND indexdef LIKE '%libelle%'\n                  AND indexdef LIKE '%UNIQUE%'\n            \"\"\")).fetchone()\n\n            if not index_exists:\n                print(\"‚ö†Ô∏è Contrainte UNIQUE manquante sur libelle, ajout en cours...\")\n                conn.execute(text(\"ALTER TABLE type_donnee ADD CONSTRAINT type_donnee_libelle_unique UNIQUE (libelle)\"))\n                print(\"‚úÖ Contrainte UNIQUE sur libelle ajout√©e\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è V√©rification structure: {e}\")\n\n    # type_donnee - Insertion avec gestion robuste des conflits\n    inserted_count = 0\n    for libelle, desc in referentiels[\"type_donnee\"]:\n        try:\n            # Essayer d'abord avec ON CONFLICT\n            result = conn.execute(text(\"\"\"\n                INSERT INTO type_donnee (libelle, description)\n                VALUES (:libelle, :desc)\n                ON CONFLICT (libelle) DO NOTHING\n                RETURNING id_type_donnee\n            \"\"\"), {\"libelle\": libelle, \"desc\": desc})\n            if result.scalar():\n                inserted_count += 1\n        except Exception:\n            # Si ON CONFLICT √©choue, v√©rifier si l'entr√©e existe d√©j√†\n            existing = conn.execute(text(\"\"\"\n                SELECT id_type_donnee\n                FROM type_donnee\n                WHERE libelle = :libelle\n            \"\"\"), {\"libelle\": libelle}).fetchone()\n            if not existing:\n                # Si n'existe pas, ins√©rer sans ON CONFLICT\n                conn.execute(text(\"\"\"\n                    INSERT INTO type_donnee (libelle, description)\n                    VALUES (:libelle, :desc)\n                \"\"\"), {\"libelle\": libelle, \"desc\": desc})\n                inserted_count += 1\n\n    print(f\"‚úÖ type_donnee : {inserted_count} entr√©es ins√©r√©es (total: {len(referentiels['type_donnee'])})\")\n\n    # type_meteo\n    for code, libelle in referentiels[\"type_meteo\"]:\n        conn.execute(text(\"\"\"\n            INSERT INTO type_meteo (code, libelle)\n            VALUES (:code, :libelle)\n            ON CONFLICT (code) DO NOTHING\n        \"\"\"), {\"code\": code, \"libelle\": libelle})\n    print(f\"‚úÖ type_meteo : {len(referentiels['type_meteo'])} entr√©es\")\n\n    # type_indicateur\n    for code, libelle, unite in referentiels[\"type_indicateur\"]:\n        conn.execute(text(\"\"\"\n            INSERT INTO type_indicateur (code, libelle, unite)\n            VALUES (:code, :libelle, :unite)\n            ON CONFLICT (code) DO NOTHING\n        \"\"\"), {\"code\": code, \"libelle\": libelle, \"unite\": unite})\n    print(f\"‚úÖ type_indicateur : {len(referentiels['type_indicateur'])} entr√©es\")\n\n    # source_indicateur\n    for nom, url in referentiels[\"source_indicateur\"]:\n        conn.execute(text(\"\"\"\n            INSERT INTO source_indicateur (nom, url)\n            VALUES (:nom, :url)\n            ON CONFLICT DO NOTHING\n        \"\"\"), {\"nom\": nom, \"url\": url})\n    print(f\"‚úÖ source_indicateur : {len(referentiels['source_indicateur'])} entr√©es\")\n\n    # theme\n    for libelle, desc in referentiels[\"theme\"]:\n        conn.execute(text(\"\"\"\n            INSERT INTO theme (libelle, description)\n            VALUES (:libelle, :desc)\n            ON CONFLICT DO NOTHING\n        \"\"\"), {\"libelle\": libelle, \"desc\": desc})\n    print(f\"‚úÖ theme : {len(referentiels['theme'])} entr√©es\")\n\n    # qc_rule\n    for nom, desc, expr in referentiels[\"qc_rule\"]:\n        try:\n            conn.execute(text(\"\"\"\n                INSERT INTO qc_rule (nom_regle, description, expression_sql)\n                VALUES (:nom, :desc, :expr)\n                ON CONFLICT DO NOTHING\n            \"\"\"), {\"nom\": nom, \"desc\": desc, \"expr\": expr})\n        except Exception:\n            # Si colonne expression_sql n'existe pas (E2), utiliser colonnes E2\n            conn.execute(text(\"\"\"\n                INSERT INTO qc_rule (code, libelle, definition)\n                VALUES (:nom, :desc, :expr)\n                ON CONFLICT (code) DO NOTHING\n            \"\"\"), {\"nom\": nom.lower().replace(' ', '_'), \"desc\": nom, \"expr\": desc})\n    print(f\"‚úÖ qc_rule : {len(referentiels['qc_rule'])} entr√©es\")\n\n    # Nouveaux r√©f√©rentiels E2\n    if \"valence\" in referentiels:\n        for label, desc in referentiels[\"valence\"]:\n            conn.execute(text(\"\"\"\n                INSERT INTO valence (label, description)\n                VALUES (:label, :desc)\n                ON CONFLICT (label) DO NOTHING\n            \"\"\"), {\"label\": label, \"desc\": desc})\n        print(f\"‚úÖ valence : {len(referentiels['valence'])} entr√©es\")\n\n    if \"type_emotion\" in referentiels:\n        for libelle, desc, valence_label in referentiels[\"type_emotion\"]:\n            id_valence = conn.execute(text(\"SELECT id_valence FROM valence WHERE label = :label\"), {\"label\": valence_label}).scalar()\n            if id_valence:\n                conn.execute(text(\"\"\"\n                    INSERT INTO type_emotion (id_valence, libelle, description)\n                    VALUES (:id_valence, :libelle, :desc)\n                    ON CONFLICT (libelle) DO NOTHING\n                \"\"\"), {\"id_valence\": id_valence, \"libelle\": libelle, \"desc\": desc})\n        print(f\"‚úÖ type_emotion : {len(referentiels['type_emotion'])} entr√©es\")\n\n    if \"pays\" in referentiels:\n        for nom in referentiels[\"pays\"]:\n            conn.execute(text(\"\"\"\n                INSERT INTO pays (nom)\n                VALUES (:nom)\n                ON CONFLICT (nom) DO NOTHING\n            \"\"\"), {\"nom\": nom})\n        print(f\"‚úÖ pays : {len(referentiels['pays'])} entr√©es\")\n\n    if \"theme_category\" in referentiels:\n        for libelle, desc in referentiels[\"theme_category\"]:\n            conn.execute(text(\"\"\"\n                INSERT INTO t23_theme_category (libelle, description)\n                VALUES (:libelle, :desc)\n                ON CONFLICT DO NOTHING\n            \"\"\"), {\"libelle\": libelle, \"desc\": desc})\n        print(f\"‚úÖ theme_category (E1_v3) : {len(referentiels['theme_category'])} cat√©gories ins√©r√©es\")\n\n    if \"source_barometre\" in referentiels:\n        for nom, url in referentiels[\"source_barometre\"]:\n            conn.execute(text(\"\"\"\n                INSERT INTO source_barometre (nom, url)\n                VALUES (:nom, :url)\n                ON CONFLICT DO NOTHING\n            \"\"\"), {\"nom\": nom, \"url\": url})\n        print(f\"‚úÖ source_barometre : {len(referentiels['source_barometre'])} entr√©es\")\n\n    # theme avec FK vers theme_category (mapping selon datasens_barometer_themes.md)\n    if \"theme\" in referentiels and \"theme_category\" in referentiels:\n        # Mapping des th√®mes vers leurs cat√©gories\n        theme_to_category = {\n            \"Confiance institutionnelle\": \"Soci√©t√© & Confiance\",\n            \"Pouvoir d'achat\": \"√âconomie & Pouvoir d'achat\",\n            \"Changement climatique\": \"√âcologie & Climat\",\n            \"Sant√© mentale\": \"Sant√© & Bien-√™tre\",\n            \"Diversit√© et √©galit√©\": \"Inclusion & √âgalit√©\",\n            \"Intelligence artificielle\": \"Innovation & Num√©rique\",\n            \"Jeux Olympiques 2024\": \"Sport & Coh√©sion\",\n            \"M√©dias et information\": \"Culture & Identit√©\",\n            \"March√© du travail\": \"Travail & Formation\",\n            \"Syst√®me √©ducatif\": \"Jeunesse & √âducation\",\n            \"Engagement associatif\": \"Solidarit√© & Engagement\",\n            \"Tensions politiques\": \"Politique & Gouvernance\",\n        }\n        \n        for libelle, desc in referentiels[\"theme\"]:\n            # Trouver la cat√©gorie correspondante\n            cat_libelle = theme_to_category.get(libelle, \"Soci√©t√© & Confiance\")  # D√©faut si non trouv√©\n            id_cat = conn.execute(text(\"\"\"\n                SELECT id_theme_cat FROM t23_theme_category WHERE libelle = :libelle\n            \"\"\"), {\"libelle\": cat_libelle}).scalar()\n            \n            if id_cat:\n                conn.execute(text(\"\"\"\n                    INSERT INTO t24_theme (id_theme_cat, libelle, description)\n                    VALUES (:id_cat, :libelle, :desc)\n                    ON CONFLICT DO NOTHING\n                \"\"\"), {\"id_cat\": id_cat, \"libelle\": libelle, \"desc\": desc})\n        print(f\"‚úÖ theme (E1_v3) : {len(referentiels['theme'])} entr√©es avec mapping cat√©gories\")\n\nprint(\"\\n‚úÖ Tous les r√©f√©rentiels ins√©r√©s !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Contr√¥les : V√©rification des tables cr√©√©es\n",
    "\n",
    "Liste des tables et comptage des entr√©es par table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Visualisations : R√©partition des tables par domaine + Tables pandas\n\nprint(\"\\nüìä LISTE DES TABLES E1_V3 (36/37 tables)\")\nprint(\"=\" * 80)\n\n# Lister toutes les tables (sch√©ma datasens + public)\nquery_tables = \"\"\"\nSELECT \n    table_schema,\n    table_name\nFROM information_schema.tables\nWHERE table_type = 'BASE TABLE'\n  AND (table_schema = 'datasens' OR table_schema = 'public')\n  AND table_name LIKE 't%'\nORDER BY table_name;\n\"\"\"\n\ndf_tables = pd.read_sql(query_tables, engine)\nprint(f\"\\n‚úÖ {len(df_tables)} tables d√©tect√©es :\\n\")\n\n# Afficher le DataFrame\ndisplay(df_tables)\n\n# R√©partition par domaine (selon MPD)\ndomaines = {\n    \"Collecte\": [\"t01_type_donnee\", \"t02_source\", \"t03_flux\", \"t37_archive_flux\"],\n    \"Documents & Annotations\": [\"t04_document\", \"t05_annotation\", \"t06_annotation_emotion\", \"t07_meta_annotation\", \n                                 \"t08_emotion\", \"t09_type_emotion\", \"t10_valence\", \"t11_modele_ia\", \"t12_utilisateur\"],\n    \"G√©ographie\": [\"t13_pays\", \"t14_region\", \"t15_departement\", \"t16_commune\", \"t17_territoire\"],\n    \"M√©t√©o\": [\"t18_type_meteo\", \"t19_meteo\"],\n    \"Indicateurs/Barom√®tres\": [\"t20_type_indicateur\", \"t21_source_indicateur\", \"t22_indicateur\", \n                               \"t28_source_barometre\", \"t29_document_baro\"],\n    \"Th√®mes & √âv√©nements\": [\"t23_theme_category\", \"t24_theme\", \"t25_evenement\", \"t26_document_theme\", \"t27_document_evenement\"],\n    \"Pipeline & Qualit√©\": [\"t30_pipeline\", \"t31_etape_etl\", \"t32_exec_etape\", \"t33_qc_rule\", \"t34_qc_result\"],\n    \"Audit/Versionning\": [\"t35_table_audit\", \"t36_table_version\"]\n}\n\n# Compter par domaine\ncounts_domaines = {}\nfor domaine, tables in domaines.items():\n    counts_domaines[domaine] = len(tables)\n\ndf_domaines = pd.DataFrame(list(counts_domaines.items()), columns=[\"Domaine\", \"Nb tables\"])\nprint(\"\\nüìã R√©partition par domaine :\")\ndisplay(df_domaines)\n\n# Graphique r√©partition par domaine\nif len(df_domaines) > 0:\n    plt.figure(figsize=(12, 7))\n    bars = plt.barh(df_domaines[\"Domaine\"], df_domaines[\"Nb tables\"], color=plt.cm.Set3(range(len(df_domaines))))\n    for bar, value in zip(bars, df_domaines[\"Nb tables\"]):\n        plt.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2,\n                str(value), ha='left', va='center', fontweight='bold', fontsize=11)\n    plt.title(\"üìä R√©partition des 36/37 tables par domaine (E1_v3)\", fontsize=14, fontweight='bold')\n    plt.xlabel(\"Nombre de tables\", fontsize=12)\n    plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\n# Compter les entr√©es par table (seulement les tables non-vides)\nprint(\"\\nüìà Nombre d'entr√©es par table (r√©f√©rentiels) :\")\nprint(\"-\" * 80)\n\ncounts = {}\nfor _, row in df_tables.iterrows():\n    schema = row['table_schema']\n    table = row['table_name']\n    full_name = f\"{schema}.{table}\" if schema != 'public' else table\n    try:\n        count = pd.read_sql(text(f\"SELECT COUNT(*) as count FROM {full_name}\"), engine).iloc[0]['count']\n        if count > 0:  # Afficher seulement les tables avec donn√©es\n            counts[table] = count\n    except Exception as e:\n        pass\n\nif counts:\n    df_counts = pd.DataFrame(list(counts.items()), columns=['Table', 'Nb entr√©es'])\n    df_counts = df_counts.sort_values('Nb entr√©es', ascending=False)\n    display(df_counts)\nelse:\n    print(\"   ‚ÑπÔ∏è Aucune donn√©e dans les tables (bootstrap √† venir)\")\n\nprint(f\"\\n‚úÖ Sch√©ma PostgreSQL E1_v3 cr√©√© avec succ√®s !\")\nprint(f\"   üìä {len(df_tables)} tables cr√©√©es (architecture compl√®te)\")\nprint(f\"   üìÇ Sch√©ma : datasens + public\")\nprint(\"\\n   ‚û°Ô∏è Passez au notebook 03_ingest_sources.ipynb pour collecter les donn√©es\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
