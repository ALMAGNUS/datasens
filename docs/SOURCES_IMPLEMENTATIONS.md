# üìã Impl√©mentations des Sources (Extrait de datasens_E1_v2.ipynb)

Ce document r√©capitule les impl√©mentations des 5 sources telles qu'elles ont √©t√© trouv√©es et int√©gr√©es dans le notebook `03_ingest_sources.ipynb`.

## üìÑ Source 1 : Fichier plat CSV (Kaggle)

**Statut** : ‚úÖ D√©j√† impl√©ment√© dans `03_ingest_sources.ipynb`

**Architecture** :
- 50% ‚Üí PostgreSQL (table `document`)
- 50% ‚Üí MinIO DataLake (fichiers bruts)

**Process** :
1. Chargement CSV depuis `data/raw/kaggle/`
2. Calcul SHA256 fingerprint pour d√©duplication
3. Split al√©atoire 50/50
4. Upload 50% vers MinIO
5. Insertion 50% dans PostgreSQL avec tra√ßabilit√© (id_flux)

---

## üå¶Ô∏è Source 2 : API OpenWeatherMap

**Code source** : `datasens_E1_v2.ipynb` (Cellule ~927-994)

**Impl√©mentation** : ‚úÖ Int√©gr√©e dans `03_ingest_sources.ipynb` (Cellules 5-6)

**Villes collect√©es** : Paris, Lyon, Marseille, Lille

**Donn√©es r√©cup√©r√©es** :
- Temp√©rature (¬∞C), Humidit√© (%), Pression (hPa)
- Description m√©t√©o (clair, nuageux, pluie...)
- Vitesse du vent (m/s)
- Timestamp de mesure

**Stockage** :
- **PostgreSQL** : Table `meteo` avec g√©olocalisation (id_territoire FK)
- **MinIO** : CSV brut (`api/owm/owm_*.csv`)

**API Key requise** : `OWM_API_KEY` dans `.env`

---

## üì∞ Source 3 : Flux RSS Multi-Sources (Presse fran√ßaise)

**Code source** : `datasens_E1_v2.ipynb` (Cellule ~1037-1210)

**Impl√©mentation** : ‚úÖ Int√©gr√©e dans `03_ingest_sources.ipynb` (Cellules 7-8)

**Sources RSS** :
- **Franceinfo** : `https://www.francetvinfo.fr/titres.rss`
- **20 Minutes** : `https://www.20minutes.fr/feeds/rss-une.xml`
- **Le Monde** : `https://www.lemonde.fr/rss/une.xml`

**Extraction** : titre, description, date publication, URL source

**Stockage** :
- **PostgreSQL** : Table `document` avec m√©tadonn√©es
- **MinIO** : CSV compil√© (`rss/rss_multi_sources_*.csv`)

**D√©duplication** : SHA256 sur (titre + description)

**Module requis** : `feedparser` (pip install feedparser)

---

## üåê Source 4 : Web Scraping Multi-Sources (Dry-run)

**Code source** : `datasens_E1_v2.ipynb` (Cellule ~1540-1782)

**Impl√©mentation** : ‚úÖ Int√©gr√©e dans `03_ingest_sources.ipynb` (Cellules 9-10)

**Version simplifi√©e pour E1** :
- **Vie-publique.fr** (RSS) : Consultations citoyennes nationales
- **data.gouv.fr** (API) : Open Data datasets CSV officiels

**√âthique & L√©galit√©** :
- ‚úÖ Open Data gouvernemental (.gouv.fr)
- ‚úÖ Respect robots.txt
- ‚úÖ APIs officielles uniquement
- ‚úÖ Aucun scraping de sites priv√©s sans autorisation

**Stockage** :
- **PostgreSQL** : Documents structur√©s
- **MinIO** : CSV bruts (`scraping/multi/scraping_multi_*.csv`)

**Note** : Version compl√®te dans `datasens_E1_v2.ipynb` inclut aussi Reddit, YouTube, SignalConso, Trustpilot

---

## üåç Source 5 : GDELT GKG France (Big Data)

**Code source** : `datasens_E1_v2.ipynb` (Cellule ~1867+)

**Impl√©mentation** : ‚úÖ Int√©gr√©e dans `03_ingest_sources.ipynb` (Cellules 11-12)

**Source** : http://data.gdeltproject.org/gdeltv2/

**Format** : GKG 2.0 (Global Knowledge Graph) - Fichiers CSV.zip (~300 MB/15min)

**Contenu Big Data** :
- √âv√©nements mondiaux g√©olocalis√©s
- **Tonalit√© √©motionnelle** (V2Tone : -100 n√©gatif ‚Üí +100 positif)
- **Th√®mes extraits** (V2Themes : PROTEST, HEALTH, ECONOMY, TERROR...)
- **Entit√©s nomm√©es** (V2Persons, V2Organizations)
- **G√©olocalisation** (V2Locations avec codes pays)

**Filtrage France** :
- S√©lection √©v√©nements avec localisation France (code pays FR)
- Extraction tonalit√© moyenne France
- Top th√®mes fran√ßais

**Strat√©gie Big Data** :
- T√©l√©chargement fichier derni√®res 15min (~6-300 MB brut)
- Parsing colonnes V2* nomm√©es (27 colonnes GKG)
- Filtrage g√©ographique France ‚Üí √©chantillon (100 √©v√©nements pour d√©mo)
- Storage MinIO (fichier brut complet)
- Insertion PostgreSQL (√©v√©nements + documents + th√®mes)

**Stockage** :
- **MinIO** : ZIP brut (`gdelt/*.gkg.csv.zip`)
- **PostgreSQL** : Tables `evenement`, `document`, `document_evenement`, `theme`

---

## üîó Notes d'Int√©gration

Toutes les sources suivent la m√™me architecture pipeline :

1. **Logging structur√©** : `logs/collecte_*.log` + `logs/errors_*.log`
2. **MinIO DataLake** : Upload automatique fichiers bruts ‚Üí `s3://datasens-raw/`
3. **PostgreSQL** : Insertion structur√©e avec tra√ßabilit√© (flux, manifests)
4. **Fonctions helpers** : `create_flux()`, `insert_documents()`, `ensure_territoire()`, `minio_upload()`
5. **D√©duplication** : Hash SHA-256 pour √©viter doublons
6. **RGPD** : Pas de donn√©es personnelles directes

**R√©f√©rence** : Voir `notebooks/datasens_E1_v2.ipynb` pour impl√©mentations compl√®tes avec toutes les variantes
