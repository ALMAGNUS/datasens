# üöÄ **Impl√©mentation des Dumps dans DataSens**

## üìã **Plan d'impl√©mentation**

### **Phase 1 : Infrastructure (E1_v3)**
- ‚úÖ Table `t38_dump_audit` (MPD) dans PostgreSQL
- ‚úÖ Fonctions utilitaires de dump/restore (`scripts/dump_restore_utils.py`)
- üîÑ Notebook `06_dump_and_restore.ipynb` (√† finaliser)
- üîÑ Scripts batch compl√©mentaires (optionnels)

### **Phase 2 : Int√©gration Pipeline (E2)**
- üîÑ Dumps automatiques avant chaque √©tape ETL critique
- üîÑ Rotation automatique (7 derniers dumps)
- üîÑ Monitoring Grafana
- üîÑ Chiffrement des dumps sensibles

### **Phase 3 : Gouvernance (E3)**
- üîÑ Audit RGPD (suppression apr√®s 90 jours)
- üîÑ Validation manuelle avant r√©import Gold
- üîÑ Catalogage dans MinIO avec m√©tadonn√©es

---

## üìä **Table `t38_dump_audit` (DDL)**

```sql
-- T38_DUMP_AUDIT (Gouvernance des dumps DataSens)
CREATE TABLE IF NOT EXISTS t38_dump_audit (
    dump_id INT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    type VARCHAR(50) NOT NULL,  -- 'SQL', 'RAW', 'LOG', 'METRIC', 'VECTOR', 'IMPORT'
    chemin TEXT NOT NULL,
    taille_mb FLOAT,
    date_creation TIMESTAMP DEFAULT NOW(),
    utilisateur VARCHAR(100),
    commentaire TEXT,
    checksum_sha256 VARCHAR(64),
    statut VARCHAR(20) DEFAULT 'ACTIF' CHECK (statut IN ('ACTIF', 'ARCHIVE', 'SUPPRIME')),
    date_expiration TIMESTAMP,
    minio_uri TEXT,
    table_cible VARCHAR(100),
    CONSTRAINT unique_dump_chemin UNIQUE (chemin)
);

CREATE INDEX IF NOT EXISTS idx_dump_audit_type ON t38_dump_audit(type);
CREATE INDEX IF NOT EXISTS idx_dump_audit_date ON t38_dump_audit(date_creation);
CREATE INDEX IF NOT EXISTS idx_dump_audit_statut ON t38_dump_audit(statut);
CREATE INDEX IF NOT EXISTS idx_dump_audit_table ON t38_dump_audit(table_cible);
```

---

## üêç **Fonctions Python (module `scripts/dump_restore_utils.py`)**

### **1. Dump PostgreSQL**

```python
def dump_postgresql(
    engine,
    output_dir: Path,
    tables: list = None,
    commentaire: str = None
) -> Path:
    """
    G√©n√®re un dump SQL de PostgreSQL.
    
    Args:
        engine: SQLAlchemy engine
        output_dir: R√©pertoire de sortie
        tables: Liste de tables √† dumper (None = toutes)
        commentaire: Commentaire pour audit
    
    Returns:
        Path du fichier dump cr√©√©
    """
    from datetime import datetime
    import subprocess
    import hashlib
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    dump_file = output_dir / f"datasens_dump_{timestamp}.sql"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Extraire connexion depuis engine
    pg_url = str(engine.url)
    # Format: postgresql+psycopg2://user:pass@host:port/db
    
    # Commande pg_dump
    cmd = [
        "pg_dump",
        "-U", engine.url.username,
        "-d", engine.url.database,
        "-h", engine.url.host or "localhost",
        "-p", str(engine.url.port or 5432),
        "-f", str(dump_file),
        "--no-owner",
        "--no-acl"
    ]
    
    if tables:
        cmd.extend(["-t", ",".join(tables)])
    
    # Ex√©cuter (n√©cessite pg_dump install√©)
    result = subprocess.run(cmd, capture_output=True, text=True)
    
    if result.returncode != 0:
        raise Exception(f"Erreur pg_dump: {result.stderr}")
    
    # Calculer checksum
    with open(dump_file, 'rb') as f:
        checksum = hashlib.sha256(f.read()).hexdigest()
    
    # Enregistrer dans t38_dump_audit
    with engine.begin() as conn:
        conn.execute(text("""
            INSERT INTO t38_dump_audit (type, chemin, taille_mb, commentaire, checksum_sha256, utilisateur)
            VALUES ('SQL', :chemin, :taille, :commentaire, :checksum, :user)
        """), {
            "chemin": str(dump_file),
            "taille": dump_file.stat().st_size / (1024 * 1024),
            "commentaire": commentaire or "Dump PostgreSQL automatique",
            "checksum": checksum,
            "user": os.getenv("USER", "system")
        })
    
    print(f"‚úÖ Dump PostgreSQL cr√©√©: {dump_file}")
    print(f"   üìä Taille: {dump_file.stat().st_size / (1024 * 1024):.2f} MB")
    print(f"   üîê Checksum: {checksum[:16]}...")
    
    return dump_file
```

### **2. Restauration PostgreSQL**

```python
def restore_postgresql(engine, dump_file: Path, drop_tables: bool = False):
    """
    Restaure un dump PostgreSQL.
    
    Args:
        engine: SQLAlchemy engine
        dump_file: Fichier dump SQL √† restaurer
        drop_tables: Supprimer les tables avant restauration
    """
    import subprocess
    
    if not dump_file.exists():
        raise FileNotFoundError(f"Dump introuvable: {dump_file}")
    
    # V√©rifier dans t38_dump_audit
    with engine.connect() as conn:
        audit = conn.execute(text("""
            SELECT dump_id, checksum_sha256 FROM t38_dump_audit
            WHERE chemin = :chemin AND statut = 'ACTIF'
        """), {"chemin": str(dump_file)}).fetchone()
        
        if not audit:
            print("‚ö†Ô∏è Dump non enregistr√© dans dump_audit")
        else:
            # V√©rifier checksum
            import hashlib
            with open(dump_file, 'rb') as f:
                current_checksum = hashlib.sha256(f.read()).hexdigest()
            
            if current_checksum != audit.checksum_sha256:
                raise ValueError("‚ùå Checksum invalide - Fichier corrompu !")
    
    # Commande psql
    cmd = [
        "psql",
        "-U", engine.url.username,
        "-d", engine.url.database,
        "-h", engine.url.host or "localhost",
        "-p", str(engine.url.port or 5432),
        "-f", str(dump_file)
    ]
    
    if drop_tables:
        print("‚ö†Ô∏è Suppression des tables existantes...")
        # √Ä impl√©menter avec DROP TABLE CASCADE
    
    result = subprocess.run(cmd, capture_output=True, text=True)
    
    if result.returncode != 0:
        raise Exception(f"Erreur restauration: {result.stderr}")
    
    print(f"‚úÖ Dump restaur√©: {dump_file}")
```

### **3. Dump MinIO (objets RAW)**

```python
def dump_minio_objects(
    minio_client,
    bucket: str,
    prefix: str,
    output_dir: Path,
    commentaire: str = None
) -> Path:
    """
    Copie les objets MinIO vers un r√©pertoire local.
    
    Args:
        minio_client: Client MinIO
        bucket: Nom du bucket
        prefix: Pr√©fixe des objets (ex: "raw/rss/")
        output_dir: R√©pertoire de sortie
        commentaire: Commentaire pour audit
    
    Returns:
        Path du r√©pertoire cr√©√©
    """
    from datetime import datetime
    import hashlib
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    dump_dir = output_dir / f"minio_{bucket}_{prefix.replace('/', '_')}_{timestamp}"
    dump_dir.mkdir(parents=True, exist_ok=True)
    
    total_size = 0
    objects_copied = []
    
    # Lister et copier les objets
    for obj in minio_client.list_objects(bucket, prefix=prefix, recursive=True):
        local_path = dump_dir / obj.object_name.replace(prefix, "").lstrip("/")
        local_path.parent.mkdir(parents=True, exist_ok=True)
        
        minio_client.fget_object(bucket, obj.object_name, str(local_path))
        total_size += local_path.stat().st_size
        objects_copied.append(obj.object_name)
    
    # Calculer checksum global (hash des noms + tailles)
    checksum_input = "\n".join([f"{obj}:{dump_dir.joinpath(obj.replace(prefix, '').lstrip('/'))}" for obj in objects_copied])
    checksum = hashlib.sha256(checksum_input.encode()).hexdigest()
    
    # Enregistrer dans t38_dump_audit
    with engine.begin() as conn:
        conn.execute(text("""
            INSERT INTO t38_dump_audit (type, chemin, taille_mb, commentaire, checksum_sha256, utilisateur, minio_uri)
            VALUES ('RAW', :chemin, :taille, :commentaire, :checksum, :user, :minio_uri)
        """), {
            "chemin": str(dump_dir),
            "taille": total_size / (1024 * 1024),
            "commentaire": commentaire or f"Dump MinIO: {bucket}/{prefix}",
            "checksum": checksum,
            "user": os.getenv("USER", "system"),
            "minio_uri": f"s3://{bucket}/{prefix}"
        })
    
    print(f"‚úÖ Dump MinIO cr√©√©: {dump_dir}")
    print(f"   üìä Objets: {len(objects_copied)}")
    print(f"   üì¶ Taille totale: {total_size / (1024 * 1024):.2f} MB")
    
    return dump_dir
```

### **4. Import de fichiers dans tables PostgreSQL**

```python
def import_file_to_table(
    engine,
    file_path: Path,
    table_name: str,
    schema: str = "public",
    if_exists: str = "append",  # 'append', 'replace', 'fail'
    commentaire: str = None
):
    """
    Importe un fichier CSV/JSON/Parquet dans une table PostgreSQL.
    
    Args:
        engine: SQLAlchemy engine
        file_path: Fichier √† importer (CSV, JSON, Parquet)
        table_name: Nom de la table cible
        schema: Sch√©ma PostgreSQL (d√©faut: public)
        if_exists: Action si table existe ('append', 'replace', 'fail')
        commentaire: Commentaire pour audit
    """
    import pandas as pd
    
    if not file_path.exists():
        raise FileNotFoundError(f"Fichier introuvable: {file_path}")
    
    # D√©tecter le format
    ext = file_path.suffix.lower()
    
    if ext == '.csv':
        df = pd.read_csv(file_path)
    elif ext == '.json':
        df = pd.read_json(file_path)
    elif ext == '.parquet':
        df = pd.read_parquet(file_path)
    else:
        raise ValueError(f"Format non support√©: {ext}")
    
    print(f"üì• Import: {file_path.name} ‚Üí {schema}.{table_name}")
    print(f"   üìä Lignes: {len(df):,}")
    print(f"   üìã Colonnes: {list(df.columns)}")
    
    # Importer dans PostgreSQL
    df.to_sql(
        table_name,
        engine,
        schema=schema,
        if_exists=if_exists,
        index=False,
        method='multi'
    )
    
    # Enregistrer dans t38_dump_audit
    with engine.begin() as conn:
        conn.execute(text("""
            INSERT INTO t38_dump_audit (type, chemin, taille_mb, commentaire, utilisateur, table_cible)
            VALUES ('IMPORT', :chemin, :taille, :commentaire, :user, :table)
        """), {
            "chemin": str(file_path),
            "taille": file_path.stat().st_size / (1024 * 1024),
            "commentaire": commentaire or f"Import {file_path.name} ‚Üí {table_name}",
            "user": os.getenv("USER", "system"),
            "table": f"{schema}.{table_name}"
        })
    
    print(f"‚úÖ Import termin√©: {len(df):,} lignes ins√©r√©es dans {schema}.{table_name}")
```

---

## üìù **Notebook `06_dump_and_restore.ipynb` (√† finaliser)**

Contenu attendu :
1. **Configuration** : Connexions PostgreSQL, variables .env, acc√®s MinIO.
2. **Dump PostgreSQL** : Exemple interactif avec `dump_postgresql()` + enregistrement dans `t38_dump_audit`.
3. **Dump MinIO** : Exemple `dump_minio_objects()` avec import automatique dans le bucket archive.
4. **Restauration** : `restore_postgresql()` + contr√¥le checksum.
5. **Import fichiers ‚Üí tables** : `import_file_to_table()` (CSV/JSON/Parquet) avec `table_cible`.
6. **Audit** : Requ√™tes `SELECT * FROM t38_dump_audit ORDER BY date_creation DESC`.
7. **Rotation automatique** : Suppression / archivage selon `date_expiration`.

---

## ‚úÖ **R√©sum√© de l'impl√©mentation**

1. ‚úÖ **Table `t38_dump_audit`** int√©gr√©e au DDL et cr√©√©e par `02_schema_create.ipynb`.
2. ‚úÖ **Module Python** `scripts/dump_restore_utils.py` op√©rationnel.
3. üîÑ **Notebook d√©di√©** `06_dump_and_restore.ipynb` √† compl√©ter (exemples + rotation).
4. ‚úÖ **Documentation** `docs/FICHE_DUMPS_DATASENS.md` et `docs/INTEGRATION_DUMPS_PLAN.md`.

**Prochaines √©tapes imm√©diates** :
- Finaliser le notebook 06 avec cas d‚Äôusage r√©els (import CSV, dump SQL, restore).
- Ajouter une t√¢che Prefect/cron pour lancer `dump_postgresql()` (7 derniers dumps).
- Publier un extrait `SELECT * FROM t38_dump_audit` dans `05_snapshot_and_readme.ipynb`.

