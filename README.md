DataSens ‚Äî Projet
=================

Voir le guide technique: docs/GUIDE_TECHNIQUE_E1.md

![GitHub Release](https://img.shields.io/github/v/release/ALMAGNUS/datasens?include_prereleases)
![GitHub Stars](https://img.shields.io/github/stars/ALMAGNUS/datasens?style=social)
![GitHub Issues](https://img.shields.io/github/issues/ALMAGNUS/datasens)
![Code Size](https://img.shields.io/github/languages/code-size/ALMAGNUS/datasens)
![License](https://img.shields.io/github/license/ALMAGNUS/datasens)
[![MIT License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
![Last Commit](https://img.shields.io/github/last-commit/ALMAGNUS/datasens)
[![Security](https://img.shields.io/badge/Security-Policy-blue.svg)](SECURITY.md)
[![Contributing](https://img.shields.io/badge/Contributing-Guide-orange.svg)](CONTRIBUTING.md)

Liens rapides
-------------

- üì¶ **D√©p√¥t** : https://github.com/ALMAGNUS/datasens
- üêõ **Issues** : https://github.com/ALMAGNUS/datasens/issues
- üöÄ **Releases** : https://github.com/ALMAGNUS/datasens/releases
- üí¨ **Discussions** : https://github.com/ALMAGNUS/datasens/discussions
- üîÄ **Pull Requests** : https://github.com/ALMAGNUS/datasens/pulls
- üîí **S√©curit√©** : [SECURITY.md](SECURITY.md)
- ü§ù **Contribuer** : [CONTRIBUTING.md](CONTRIBUTING.md)

Description
-----------

**DataSens** est un **pipeline ETL complet d'annotation et de structuration de dataset pour l'IA**.

### üéØ Vision du Projet

DataSens est un **bouffeur de donn√©es** et un **classifieur performant** qui :

1. **Collecte** depuis **6 types de sources** :
   - üìÑ **Fichier plat** (CSV Kaggle)
   - üóÑÔ∏è **Base de donn√©es** (SQLite/PostgreSQL)
   - üåê **API REST** (OpenWeatherMap, NewsAPI)
   - üï∑Ô∏è **Web Scraping** (Reddit, YouTube, Vie-publique, data.gouv)
   - üåç **Big Data** (GDELT GKG - Global Knowledge Graph)
   - üìä **Barom√®tres d'opinion** (INSEE, IFOP, ADEME - en pr√©paration E2)

2. **Transforme et Structure** :
   - **D√©duplication** SHA256 pour √©viter les doublons
   - **Nettoyage** et normalisation (texte, dates, langues)
   - **Classification professionnelle** selon standards M√©diam√©trie (Nomenclature, Donn√©es Ma√Ætres, Op√©rationnelles, D√©cisionnelles, M√©tadonn√©es)
   - **Annotation simple** (E1_v3) : nettoyage, d√©duplication, QA de base
   - **Annotation IA avanc√©e** (E2) : CamemBERT, FlauBERT pour sentiment, NER, keywords

3. **Charge et Stocke** :
   - **PostgreSQL** : Donn√©es structur√©es (36/37 tables Merise) pour requ√™tes SQL
   - **MinIO DataLake** : Donn√©es brutes (S3-like) pour analyses Big Data
   - **Stockage hybride** : 50/50 split selon type de source

4. **CRUD Complet** :
   - **CREATE** : Insertion multi-sources avec tra√ßabilit√©
   - **READ** : Requ√™tes jointes complexes avec visualisations
   - **UPDATE** : Mise √† jour contr√¥l√©e
   - **DELETE** : Suppression avec int√©grit√© r√©f√©rentielle

### üöÄ Pipeline ETL Multi-Sources

**Extract ‚Üí Transform ‚Üí Load** avec :
- ‚úÖ Logging structur√© (fichiers + traceback)
- ‚úÖ Gestion d'erreurs robuste (try/except + fallback)
- ‚úÖ D√©duplication automatique (SHA256 fingerprinting)
- ‚úÖ Tra√ßabilit√© compl√®te (flux, manifests JSON, versioning Git)
- ‚úÖ Visualisations √† chaque √©tape (pandas + matplotlib)
- ‚úÖ Contr√¥les qualit√© (doublons, NULL, int√©grit√© FK)

### üìä Export Dataset Structur√© pour IA

Le pipeline g√©n√®re un **dataset nettoy√© et annot√©**, pr√™t pour enrichissement IA :
- Format **Parquet** (optimis√© pour ML/IA)
- Format **CSV** (compatibilit√©)
- Export dans `data/gold/dataset_ia/`
- M√©tadonn√©es compl√®tes (source, type, th√®mes, territoire)

Structure acad√©mique DataSens. Pilotage par notebooks Jupyter, versionn√© sur GitHub.

Arborescence du d√©p√¥t
---------------------

- `data/`
  - `raw/` Zone Bronze (sources brutes)
  - `silver/` Jeux de donn√©es nettoy√©s
  - `gold/` Jeux finaux enrichis
  - `tmp/` Zone tampon
- `notebooks/` Notebooks d‚Äôex√©cution par blocs/versions
- `docs/` Mod√®les de donn√©es, dictionnaires, READMEs
- `logs/` Manifests et journaux d‚Äôex√©cution
- `scripts/` Scripts (ingestion, contr√¥les, CRUD, pipeline)

Versionnage
-----------

Utiliser des tags pour les jalons:

- `E1_v1`: Sch√©ma prototype (10 tables) donn√©es simul√©es
- `E1_v2`: Ingestion r√©elle (18 tables, 6 sources)
- `E1_v3`: Pipeline complet (36/37 tables, MinIO + PostgreSQL) - **Dataset structur√© pour IA**

D√©marrage rapide (Docker - recommand√©)
---------------------------------------

**Pr√©requis s√©curit√© (√† faire une seule fois) :**

```bash
cp .env.example .env  # le fichier doit rester √† la racine du d√©p√¥t
# Renseigner les secrets (PostgreSQL, MinIO, API). Ne jamais copier .env dans notebooks/
```

**Pour le jury / d√©monstration (2 minutes):**

```bash
# 1. Cloner le d√©p√¥t
git clone https://github.com/ALMAGNUS/datasens.git
cd datasens

# 2. Lancer le stack complet (Jupyter + Postgres + MinIO)
docker compose up -d --build

# 3. Acc√©der √† Jupyter Lab
# Ouvrir: http://localhost:8888
# Token affich√© dans les logs: docker compose logs app | grep token
```

**Stack Docker:**
- Jupyter Lab: http://localhost:8888
- PostgreSQL: localhost:5433 (user: `postgres`, pwd: `postgres`, db: `postgres`)
- MinIO Console: http://localhost:9003 (user: `minioadmin`, pwd: `minioadmin`)
- MinIO API: http://localhost:9002

**Arr√™ter le stack:**
```bash
docker compose down
```

D√©marrage local (sans Docker)
-----------------------------

1. Cr√©er un environnement Python 3.10+
2. Installer les d√©pendances: `pip install -r requirements.txt`
3. D√©marrer PostgreSQL et MinIO localement (ou utiliser les services Docker)
4. Ouvrir les notebooks sous `notebooks/datasens_E1_v3/`

Mode D√©mo (10‚Äì15 min)
---------------------

Objectif: ex√©cuter un notebook unique (sans `.py` externe) montrant ingestion multi-sources ‚Üí normalisation ‚Üí QA ‚Üí stockage ‚Üí CRUD ‚Üí visualisations.

- Notebook conseill√©: `notebooks/datasens_E1_v3/03_ingest_sources.ipynb` (ou la derni√®re version E1 disponible)

√âtapes:
1. Ouvrir le notebook et ex√©cuter toutes les cellules.
2. Suivre le logging inline (console) et v√©rifier les fichiers `logs/` (`collecte_*.log`, `errors_*.log`).
3. V√©rifier le sch√©ma normalis√©: `titre, texte, source_site, url, date_publication, langue` + `hash_fingerprint` (SHA256) et d√©doublonnage.
4. V√©rifier l'insertion PostgreSQL et un CRUD minimal (SELECT/UPDATE/DELETE).
5. Afficher 1‚Äì2 graphiques (documents par source/date/langue).

Configuration Flexible des Sources
-----------------------------------

**üéØ Toutes les sources sont configur√©es dans `config/sources_config.json`**

- **Pour ajouter/modifier une source** : √âditez simplement le JSON selon `config/README_SOURCES.md`, puis impl√©mentez la collecte dans `03_ingest_sources.ipynb`
- **Structure acad√©mique** : Le notebook reste organis√© source par source pour la clart√© p√©dagogique
- **Types de collecteurs** : CSV, API REST, RSS, Web Scraping, GDELT Big Data, PDF

Voir `docs/AUDIT_E1_V3.md` pour l'√©tat complet des sources impl√©ment√©es.

Variables d'environnement (s√©curit√© renforc√©e)
----------------------------------------------

**üìÑ Template officiel** : `cp .env.example .env` (existe seulement √† la racine).
- Le notebook `01_setup_env.ipynb` d√©tecte d√©sormais **strictement** la racine Git avant de charger les secrets.
- Si `.env` est absent, un rappel explique comment r√©g√©n√©rer le template et renseigner les clefs.
- **Interdit** : aucun `.env` ne doit √™tre stock√© dans `notebooks/` ou sous-dossiers.

Variables principales :
- **PostgreSQL** : `POSTGRES_HOST`, `POSTGRES_PORT`, `POSTGRES_DB`, `POSTGRES_USER`, `POSTGRES_PASS`
- **MinIO DataLake** : `MINIO_ENDPOINT`, `MINIO_ACCESS_KEY`, `MINIO_SECRET_KEY`, `MINIO_BUCKET`
- **API Keys** (optionnelles) : `OWM_API_KEY`, `NEWSAPI_KEY`, `KAGGLE_USERNAME`, `KAGGLE_KEY`, `REDDIT_CLIENT_ID`, `YOUTUBE_API_KEY`

### Mode production (.env)

- **S√©parer les fichiers** : conservez un `.env.demo` (cl√©s gratuites) pour les tests et un `.env.prod`/`.env.staging` charg√© via `docker compose --env-file`, GitHub Actions secrets, Vault, Doppler, etc.
- **Ne jamais committer les vraies cl√©s** : stockez-les dans un secret manager ou des variables d'environnement CI/CD.
- **Nouveaux flags utiles** :
  - `ENABLE_ANNOTATION_PIPELINE` (d√©faut `1`) : active/d√©sactive l'√©tape spaCy/YAKE dans le notebook 05.
  - `ANNOTATION_WRITE_DB` (d√©faut `0`) : pousse les polarit√©s/intensit√©s dans `t05_annotation` lorsqu'on lance le pipeline.
- **Rotation & audit** : changez r√©guli√®rement OWM/NewsAPI/Kaggle/Reddit, et logguez toute diffusion vers des environnements partag√©s.
- **Rappel** : `.env.example` reste la seule version tracked ; chaque √©quipe duplique en `.env.local`, `.env.prod`, etc.

**üîí S√©curit√©** : 
- Le fichier `.env` est ignor√© par Git
- Les notebooks refusent de cr√©er un `.env` hors racine (message explicite dans `01_setup_env.ipynb`)
- Requ√™tes SQL param√©tr√©es pour pr√©venir injection SQL
- Fonctions de validation impl√©ment√©es (`assert_valid_identifier`, `load_whitelist_tables`)
- Voir [SECURITY.md](SECURITY.md) pour les d√©tails complets

Annotation Simple (E1_v3) - Pr√©paration Dataset pour E2
--------------------------------------------------------

**E1_v3** : Annotation simple pour pr√©parer le dataset √† l'enrichissement IA (E2)

- **Nettoyage** : Normalisation de base (texte, encodage)
- **D√©tection de langue** : Identification automatique (priorit√© FR)
- **D√©duplication** : SHA256 fingerprint pour √©viter doublons
- **QA de base** : Validation format, longueur minimale, champs requis

**E2** (√† venir) : Annotation IA avanc√©e avec CamemBERT et FlauBERT
- Sentiment/polarit√© (mod√®les FR)
- NER (spaCy mod√®le FR)
- Mots-cl√©s (YAKE FR)

### ü§ñ Annotation IA automatique (spaCy + YAKE)

La premi√®re brique E2 est automatis√©e via `scripts/apply_annotation_pipeline.py` :

```bash
# Annoter le dernier dataset GOLD disponible
python scripts/apply_annotation_pipeline.py \
    --spacy-model fr_core_news_md \
    --output-dir data/dataset/annotated

# Exemple avanc√© : dataset cibl√© + √©criture Postgres
python scripts/apply_annotation_pipeline.py \
    --input data/gold/dataset_ia/datasens_dataset_ia_20251118.parquet \
    --write-db
```

Sorties :
- Dataset annot√© (colonnes `annotation_keywords_yake`, `annotation_entities_spacy`, `annotation_polarity`, `annotation_intensity`, `annotation_summary`)
- Manifest JSON d√©crivant les exports g√©n√©r√©s
- Option `--write-db` pour consigner la polarit√©/intensit√© dans `t05_annotation`

Classification Types de Donn√©es (M√©diam√©trie)
---------------------------------------------

Les types de donn√©es suivent la classification professionnelle :
- **Nomenclature** : Donn√©es de classification (mensuelle)
- **Donn√©es Ma√Ætres** : Donn√©es de r√©f√©rence (quotidienne)
- **Donn√©es Op√©rationnelles** : Donn√©es d'activit√© (secondes)
- **Donn√©es D√©cisionnelles** : Donn√©es d'analyse (quotidienne)
- **M√©tadonn√©es** : Donn√©es sur les donn√©es (variable)

Export Dataset Pr√©par√© (E1_v3)
-------------------------------

- **E1_v3** : Dataset nettoy√© et annot√© simplement, pr√™t pour enrichissement IA (E2)
- Parquet partitionn√© (date/langue/source), ex:
  - `data/gold/annotated/date=YYYY-MM-DD/langue=fr/source=reddit/part.parquet`
- **E2** : Enrichissement IA avec CamemBERT et FlauBERT sur le dataset E1_v3

Tol√©rance aux manques (d√©mo)
----------------------------

- Cl√©s API absentes: la source est saut√©e (log) et l‚Äôex√©cution continue.
- MinIO absent: √©criture locale sous `data/raw/`.
- PostgreSQL absent: d√©finir `DATASENS_PG_DSN` ou d√©marrer une DB locale (erreurs explicites sinon).

S√©curit√© & Secrets
------------------

- Ne jamais committer de secrets: `.env` est ignor√©.
- SQL param√©tr√© (`text("... :param")` + dict). √âviter concat√©nations/f-strings pour les valeurs/identifiants.
- Si noms de tables dynamiques, valider via liste blanche.

Publication & Image conteneur
-----------------------------

- Tag s√©mantique: `git tag -a vX.Y.Z -m "release" && git push --tags`
- Lancer le stack: `docker compose up -d --build`

Workflow de release GitHub (recommand√©)
---------------------------------------

1. Cr√©er une branche de release: `git checkout -b release/vX.Y.Z`
2. Mettre √† jour version/changelog si besoin
3. Ouvrir une PR ‚Üí squash & merge
4. Cr√©er la Release GitHub: https://github.com/ALMAGNUS/datasens/releases/new
   - Tag: `vX.Y.Z`
   - Titre: `DataSens vX.Y.Z`
   - Notes: changements cl√©s, compat, run (`docker compose up -d --build`)

Image GHCR (publication auto sur tag)
-------------------------------------
- Pull `latest`: `docker pull ghcr.io/almagnus/datasens:latest`
- Pull version: `docker pull ghcr.io/almagnus/datasens:${TAG}` (ex: `v0.2.0`)
- Run via compose: `IMAGE=ghcr.io/almagnus/datasens:latest docker compose up -d`

Automatisation quotidienne
--------------------------

- Workflow GitHub Actions `DataSens - Collecte Quotidienne Automatis√©e` (`.github/workflows/collect-data.yml`)
  - Provisionne Postgres + MinIO via `docker compose`
  - Ex√©cute `02_schema_create.ipynb` puis `03_ingest_sources.ipynb` avec Papermill
  - Convertit le notebook ex√©cut√© en rapport HTML
  - Archive notebooks ex√©cut√©s, datasets et logs comme artefacts d'ex√©cution
  - Cron √† 05h15 UTC + d√©clenchement manuel `workflow_dispatch`

Les journaux GitHub contiennent le d√©tail d‚Äôinitialisation des conteneurs et les statistiques finales (`artifacts/collection_stats.json`).


