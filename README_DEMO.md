# ğŸ¯ GUIDE DÃ‰MO TECHNIQUE - DataSens ETL Pipeline

## âš¡ DÃ©marrage Infrastructure (2 min)

### 1. Lancer l'infrastructure
```powershell
cd "C:\Users\Utilisateur\Desktop\DEV IA 2025\DataSens"
docker-compose up -d
```

### 2. Ouvrir le notebook principal
```
ğŸ“‚ Ouvrir: notebooks/datasens_E1_v3/03_ingest_sources.ipynb
```

## ğŸ¬ ScÃ©nario de DÃ©mo Complet (25 minutes)

### Introduction (2 min)
PrÃ©senter l'architecture refactorisÃ©e:
```
DataSens/
â”œâ”€â”€ datasens/              â† Modules refactorisÃ©s Phase 2
â”‚   â”œâ”€â”€ collectors/        â† 5 collecteurs (Kaggle, RSS, OWM, WebScraping, GDELT)
â”‚   â”œâ”€â”€ annotation/        â† SpaCy NER + YAKE Keywords
â”‚   â”œâ”€â”€ db.py              â† PostgreSQL + Connection Pooling
â”‚   â”œâ”€â”€ storage.py         â† MinIO S3-compatible + Logging
â”‚   â”œâ”€â”€ retry.py           â† Exponential backoff
â”‚   â””â”€â”€ cache.py           â† DÃ©duplication in-memory
â””â”€â”€ notebooks/datasens_E1_v3/  â† 5 Notebooks progression
    â”œâ”€â”€ 01_setup_env.ipynb         â† Infrastructure setup
    â”œâ”€â”€ 02_schema_create.ipynb     â† Schema DDL (36 tables)
    â”œâ”€â”€ 03_ingest_sources.ipynb    â† ETL Pipeline â­
    â”œâ”€â”€ 04_crud_tests.ipynb        â† Unit tests
    â””â”€â”€ 05_snapshot_and_readme.ipynb â† Documentation
```

### Notebook 1: `01_setup_env.ipynb` (2 min)

**Objectif**: Validation environnement technique

**ExÃ©cuter cellules 1-3:**
```
âœ… Python 3.11+
âœ… Dependencies (pandas, sqlalchemy, minio, spacy, yake)
âœ… Environment variables (.env)
âœ… PostgreSQL connection
âœ… MinIO S3 connection
```

**Points techniques:**
- Configuration centralisÃ©e `.env`
- Health checks automatisÃ©s
- Dependency validation

### Notebook 2: `02_schema_create.ipynb` (3 min)

**Objectif**: DDL PostgreSQL avec architecture Medallion

**ExÃ©cuter cellules principales:**
```
âœ… Schema datasens created
âœ… 36 tables (t01-t37) with foreign keys
âœ… Indexes on hash_fingerprint, date_publication
âœ… Medallion architecture (RAW â†’ SILVER â†’ GOLD)
```

**Points techniques:**
- Normalized schema (3NF)
- Referential integrity constraints
- Optimized indexes for queries

### Notebook 3: `03_ingest_sources.ipynb` â­ PRINCIPAL (12 min)

**Objectif**: Pipeline ETL complet avec collecte multi-sources

#### Partie A: Sources Classiques (7 min)

**ExÃ©cuter cellules 1-25:**
- âœ… Configuration chargÃ©e (PostgreSQL + MinIO)
- âœ… Collecte RSS (Franceinfo, 20 Minutes, Le Monde)
- âœ… Collecte mÃ©tÃ©o OpenWeatherMap
- âœ… Upload MinIO + Insertion PostgreSQL

**Points techniques:**
- Modules rÃ©utilisables avec retry mechanism
- Structured logging (taille, durÃ©e, status)
- Exponential backoff (1s â†’ 2s â†’ 4s)

#### Partie B: BigData + ML (5 min)

**ExÃ©cuter nouvelles cellules fin de notebook:**

1. **Collecte GDELT:**
   - Download ZIP from GDELT GKG API
   - Parse 28 colonnes mÃ©tadonnÃ©es
   - Insert PostgreSQL + Upload MinIO

2. **SpaCy NER:**
   - ModÃ¨le fr_core_news_md
   - Extract PER, LOC, ORG entities
   - 3 colonnes ajoutÃ©es au DataFrame

3. **YAKE Keywords:**
   - Unsupervised extraction (n-grams 1-3)
   - Top 5 keywords per document
   - Deduplication threshold 0.9

4. **Visualisations:** â­
   - Bar/Pie chart: Distribution entitÃ©s NER
   - Horizontal bar: Top 15 keywords YAKE
   - Stacked bar + Table: Matrice annotations complÃ¨te

5. **Upload MinIO:**
   - Dataset annotÃ© (CSV format)
   - URI s3://datasens-raw/demo/

**Points techniques:**
- GDELT = Global Event Database (15min frequency)
- SpaCy = Production-ready NER (French model)
- YAKE = Statistical keyword extraction
- Dataset AI-ready pour ML/DL downstream tasks

### Notebook 4: `04_crud_tests.ipynb` (3 min)

**Objectif**: Unit tests opÃ©rations CRUD

**ExÃ©cuter cellules clÃ©s:**
- âœ… CREATE: Insert test data
- âœ… READ: SELECT queries validation
- âœ… UPDATE: Modify records
- âœ… DELETE: Remove records

**Points techniques:**
- Data integrity validation
- Schema constraints verification
- Referential integrity tests

### Notebook 5: `05_snapshot_and_readme.ipynb` (3 min)

**Objectif**: Auto-documentation et versioning

**ExÃ©cuter cellules:**
- âœ… README.md auto-generated
- âœ… Database snapshot (tables + row counts)
- âœ… Global statistics export
- âœ… Metadata manifest (JSON)

**Points techniques:**
- Automated documentation pipeline
- State versioning (git-compatible)
- Metadata traceability

## ğŸ“ Messages ClÃ©s pour le Professeur

### 1. Architecture RefactorisÃ©e âœ…
"Nous avons refactorisÃ© le code monolithique en modules rÃ©utilisables. Les notebooks appellent maintenant `datasens.collectors`, `datasens.annotation`, etc."

### 2. Optimisations Phase 3 âš¡
"Nous avons implÃ©mentÃ© 3 optimisations majeures:
- **Retry** avec exponential backoff (rÃ©silience rÃ©seau)
- **Connection Pooling** PostgreSQL (5+10 connexions)
- **Cache** dÃ©duplication in-memory (10k hash)"

### 3. BigData GDELT ğŸŒ
"GDELT publie des fichiers toutes les 15 minutes avec des millions d'Ã©vÃ©nements mondiaux. Notre collecteur tÃ©lÃ©charge, parse et stocke ces donnÃ©es massives."

### 4. Annotations ML ğŸ”
"Nous prÃ©parons des datasets AI-ready avec:
- **SpaCy**: Extraction d'entitÃ©s (NER franÃ§ais)
- **YAKE**: Extraction de mots-clÃ©s
Le dataset annotÃ© est prÃªt pour du ML/DL (sentiment, classification, etc.)"

### 5. Production Ready ğŸš€
"Notre pipeline est production-ready:
- Logging structurÃ© (taille, durÃ©e, erreurs)
- Test de connexion MinIO
- Gestion d'erreurs avec retry
- Storage hybride (PostgreSQL + MinIO S3)"

## ğŸ“‹ Si Questions Techniques

**Q: "Pourquoi refactoriser en modules?"**
â†’ "Code reusability, testability, maintainability. Single Responsibility Principle: 1 collector = 1 module. Easier debugging and horizontal scaling."

**Q: "DiffÃ©rence avec code monolithique?"**
â†’ "Avant: 5000 lignes notebook monolithique. Maintenant: notebooks courts + Python packages. DRY principle, proper imports, version control friendly."

**Q: "Pourquoi GDELT?"**
â†’ "Production-scale BigData validation: millions de records, 15min frequency, 28 metadata columns. Demonstrates pipeline capacity for real-world volumes."

**Q: "Purpose des annotations?"**
â†’ "Generate AI-ready datasets for downstream ML tasks: sentiment analysis, topic classification, entity linking. SpaCy NER + YAKE keywords = feature engineering automation."

**Q: "Scalability approach?"**
â†’ "Retry mechanism = network resilience. Connection pooling = concurrency. MinIO S3 = horizontal storage scaling. In-memory cache = performance optimization. Architecture supports distributed processing (future: Spark/Dask)."

## âœ… Checklist Avant DÃ©mo

- [ ] Docker Compose lancÃ©: `docker ps` (voir postgres + minio)
- [ ] Virtual env activÃ©: `(.venv)` visible dans terminal
- [ ] SpaCy installÃ©: `python -m spacy download fr_core_news_md`
- [ ] Tous les notebooks dans `datasens_E1_v3/` ouverts en onglets
- [ ] Ordre des onglets: 01 â†’ 02 â†’ 03 â†’ 04 â†’ 05
- [ ] Notebook 01 cellules 1-3 prÃ©-exÃ©cutÃ©es (gain de temps)
- [ ] Notebook 02 cellules schema prÃ©-exÃ©cutÃ©es (gain de temps)

## ğŸ¬ Script de DÃ©mo DÃ©taillÃ©

### Minute 0-2: Notebook 01 (Setup)
```
"Je vais vous montrer notre pipeline DataSens en 5 Ã©tapes.
D'abord, on vÃ©rifie que l'infrastructure Docker est opÃ©rationnelle."

[ExÃ©cuter cellules 1-3]

"âœ… PostgreSQL connectÃ©, MinIO connectÃ©, toutes les dÃ©pendances sont OK.
Notre configuration est centralisÃ©e dans .env."
```

### Minute 2-5: Notebook 02 (Schema)
```
"Maintenant, crÃ©ons le schÃ©ma PostgreSQL complet."

[ExÃ©cuter cellules crÃ©ation tables]

"âœ… 36 tables crÃ©Ã©es selon notre MCD/MPD documentÃ©.
Architecture Medallion: RAW â†’ SILVER â†’ GOLD."
```

### Minute 5-17: Notebook 03 (Ingest) â­ STAR
```
"C'est le cÅ“ur du pipeline: la collecte multi-sources."

[ExÃ©cuter cellules configuration]
"Configuration chargÃ©e depuis nos modules refactorisÃ©s."

[ExÃ©cuter cellules RSS]
"âœ… Collecte RSS: Franceinfo, 20 Minutes, Le Monde.
DonnÃ©es uploadÃ©es vers MinIO, insÃ©rÃ©es dans PostgreSQL.
Remarquez le retry automatique et le logging structurÃ©."

[ExÃ©cuter cellules GDELT]
"ğŸŒ GDELT BigData: fichiers mondiaux toutes les 15 minutes.
50 Ã©vÃ©nements tÃ©lÃ©chargÃ©s, parsÃ©s, stockÃ©s."

[ExÃ©cuter cellules SpaCy]
"ğŸ” Annotations SpaCy NER: extraction automatique des entitÃ©s.
Personnes, lieux, organisations identifiÃ©s."

[ExÃ©cuter cellules YAKE]
"ğŸ”‘ YAKE: extraction mots-clÃ©s unsupervised.
Dataset maintenant AI-ready pour ML/DL."

[ExÃ©cuter cellules statistiques]
"ğŸ“Š RÃ©sultats: X Ã©vÃ©nements, Y entitÃ©s, Z mots-clÃ©s.
Tout est dans PostgreSQL + MinIO."
```

### Minute 17-20: Notebook 04 (CRUD)
```
"Tests unitaires pour valider l'intÃ©gritÃ©."

[ExÃ©cuter 2-3 cellules CRUD]

"âœ… CREATE, READ, UPDATE, DELETE fonctionnent.
Contraintes respectÃ©es."
```

### Minute 20-23: Notebook 05 (Snapshot)
```
"Documentation automatique et snapshot final."

[ExÃ©cuter cellules gÃ©nÃ©ration]

"âœ… README gÃ©nÃ©rÃ©, mÃ©tadonnÃ©es exportÃ©es.
TraÃ§abilitÃ© complÃ¨te pour versioning."
```

### Minute 23-25: Questions
```
"VoilÃ  le pipeline complet de bout en bout.
Questions?"
```

## ğŸ¯ Timing IdÃ©al (25 min)

| Notebook | DurÃ©e | Contenu |
|----------|-------|---------|
| **01_setup_env** | 2 min | VÃ©rification environnement |
| **02_schema_create** | 3 min | CrÃ©ation 36 tables PostgreSQL |
| **03_ingest_sources** | 12 min | Pipeline ETL + GDELT + Annotations â­ |
| **04_crud_tests** | 3 min | Tests CRUD + validations |
| **05_snapshot_readme** | 3 min | Documentation auto + snapshot |
| **Questions** | 2 min | RÃ©ponses + discussion |
| **TOTAL** | **25 min** | |

## ğŸ“‹ Ordre d'ExÃ©cution DÃ©mo

### SÃ©quence RecommandÃ©e

1. **01_setup_env.ipynb** â†’ Montrer que tout est prÃªt
2. **02_schema_create.ipynb** â†’ Montrer l'architecture DB
3. **03_ingest_sources.ipynb** â†’ â­ DÃ‰MO PRINCIPALE
   - Partie A: Collecte RSS + OWM (sources classiques)
   - Partie B: GDELT + SpaCy + YAKE (BigData + ML)
4. **04_crud_tests.ipynb** â†’ Tests rapides (optionnel si temps limitÃ©)
5. **05_snapshot_readme.ipynb** â†’ Documentation finale

### Si Temps LimitÃ© (15 min)

Concentrez-vous sur **03_ingest_sources.ipynb**:
- Cellules 1-3: Configuration
- Cellules RSS: Collecte multi-sources
- Cellules GDELT: BigData
- Cellules Annotations: SpaCy + YAKE
- Cellules Statistiques: RÃ©sultats finaux

## ğŸ“ Technical Highlights par Notebook

### 01 - Setup
"Dockerized infrastructure: PostgreSQL 14, MinIO S3-compatible, PgAdmin. Environment variables managed via .env."

### 02 - Schema
"Medallion architecture with 36 normalized tables (3NF). Entity relationships documented in MCD/MPD. Indexes optimized for hash_fingerprint lookups."

### 03 - Ingest â­
"Multi-source ETL pipeline with exponential backoff retry, PostgreSQL connection pooling (5+10), in-memory deduplication cache. GDELT BigData + SpaCy/YAKE annotations generate AI-ready datasets."

### 04 - CRUD
"Unit tests validate schema integrity: CREATE, READ, UPDATE, DELETE operations. Foreign key constraints and referential integrity verified."

### 05 - Snapshot
"Automated documentation generation. Database state versioning with JSON manifests. Git-compatible traceability."

## ğŸ’¡ Presentation Best Practices

### Pre-Demo Setup
1. **Pre-execute notebooks 01-02** (saves 5 minutes)
2. **Keep 5 notebooks as browser tabs** (ordered 01â†’05)
3. **Dry-run once** to validate timings

### During Demo
1. **Navigate between tabs** to show pipeline progression
2. **Comment while executing** (explain what's happening)
3. **Highlight logs** (structured output: size, duration, status)
4. **Display DataFrames** (visual proof with .head())
5. **Show visualizations** (graphs = concrete evidence)

### High-Impact Moments
- ğŸŒ **GDELT BigData** â†’ Demonstrates volume capacity
- ğŸ” **SpaCy NER** â†’ Shows intelligent processing
- ğŸ“Š **Visualizations** â†’ Concrete proof of annotations
- â˜ï¸ **MinIO** â†’ Proves scalable architecture
- ï¿½ **Statistics** â†’ Quantifiable results

### Technical Issue Handling
- **MinIO disconnected**: "Local fallback active, pipeline continues gracefully"
- **GDELT timeout**: "Worldwide BigData source, adjustable limits for demo"
- **SpaCy slow**: "NER model analyzes every token, expected latency"
- **Visualization error**: "Matplotlib backend issue, data remains valid"

---

ğŸ¬ **Demo Ready!** 5 notebooks tell complete story: Infrastructure â†’ Schema â†’ ETL â†’ Tests â†’ Documentation
